
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Model calibration part I - reliability assessment &#8212; The Practical Man&#39;s Guide to Binary Imbalanced Classification</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="How imbalanced learning affects the interpretability of a model" href="Interpretability.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/DataLab-logo-white.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">The Practical Man's Guide to Binary Imbalanced Classification</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="Introduction.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Loss%20functions.html">
   Loss functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%201%20-%20Intro%20%26%20ROC%20AUC.html">
   Classification metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%202%20-%20Lift%20curve.html">
   A complement to the ROC: the lift curve (aka CAP curve)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%203%20-%20KS%20score.html">
   The KS score and Youden’s J
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%204%20-%20Precision%20and%20Recall.html">
   Precision, recall, and
   <span class="math notranslate nohighlight">
    \(F\)
   </span>
   -scores
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pablo-baseline-experiment.html">
   Choosing a baseline model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Feature%20selection%20methods.html">
   Feature selection via the Boruta algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Interpretability.html">
   How imbalanced learning affects the interpretability of a model
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Model calibration part I - reliability assessment
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/notebooks/Calibration.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/notebooks/Calibration.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Model calibration part I - reliability assessment
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#when-scores-are-not-probabilities">
     When scores are not probabilities
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-would-you-want-probabilities-anyway">
     Why would you want probabilities anyway?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-first-glance-at-model-reliability">
     A first glance at model reliability
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calibration-curves-aka-reliability-diagrams">
     Calibration curves (aka reliability diagrams)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-to-interpret-a-reliability-plot">
       How to interpret a reliability plot?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-to-choose-the-number-of-bins">
       How to choose the number of bins?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metrics-for-assessing-model-reliability">
     Metrics for assessing model reliability
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-brier-skill-score">
       The Brier (skill) score
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#is-the-brier-score-sufficient">
         Is the Brier score sufficient?
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#calibration-spread-measures">
       Calibration spread measures
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary-running-reliability-analysis">
     Summary: running reliability analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-calibration-part-ii-calibrating-outputs">
   Model calibration part II - calibrating outputs
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#but-first-should-you-be-calibrating">
     But first: should you be calibrating?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calibrating-pre-trained-models">
     Calibrating pre-trained models
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#digression-what-is-an-isotonic-regression">
     Digression: what is an isotonic regression?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#best-practices-for-calibration">
     Best practices for calibration
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#useful-functions">
     Useful functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-about-class-imbalance">
   What about class imbalance?
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-good-calibration-metric-in-imbalanced-cases">
     A good calibration metric in imbalanced cases
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-two-step-approach-to-calibrating-in-imbalanced-cases">
     A two-step approach to calibrating in imbalanced cases
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-naive-approach-model-not-optimized-for-imbalanced-data">
       a) Naive approach - model NOT optimized for imbalanced data
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#trying-the-wallace-dahabreh-underbagging-approach">
       Trying the Wallace-Dahabreh underbagging approach
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#b-better-approach-model-suited-for-imbalanced-data">
       b) Better approach - model suited for imbalanced data
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#same-exercise-different-model">
     Same exercise, different model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#is-the-underbagging-approach-always-the-best">
       Is the underbagging approach always the best?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recommendations">
     Recommendations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix">
   Appendix
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#roc-auc-and-the-ks-score-are-invariant-under-calibration-average-precision-almost">
     ROC AUC and the KS score are invariant under calibration; average precision almost
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#references">
     References:
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Model calibration part I - reliability assessment</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Model calibration part I - reliability assessment
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#when-scores-are-not-probabilities">
     When scores are not probabilities
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#why-would-you-want-probabilities-anyway">
     Why would you want probabilities anyway?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-first-glance-at-model-reliability">
     A first glance at model reliability
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calibration-curves-aka-reliability-diagrams">
     Calibration curves (aka reliability diagrams)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-to-interpret-a-reliability-plot">
       How to interpret a reliability plot?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-to-choose-the-number-of-bins">
       How to choose the number of bins?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metrics-for-assessing-model-reliability">
     Metrics for assessing model reliability
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-brier-skill-score">
       The Brier (skill) score
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#is-the-brier-score-sufficient">
         Is the Brier score sufficient?
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#calibration-spread-measures">
       Calibration spread measures
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#summary-running-reliability-analysis">
     Summary: running reliability analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-calibration-part-ii-calibrating-outputs">
   Model calibration part II - calibrating outputs
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#but-first-should-you-be-calibrating">
     But first: should you be calibrating?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calibrating-pre-trained-models">
     Calibrating pre-trained models
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#digression-what-is-an-isotonic-regression">
     Digression: what is an isotonic regression?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#best-practices-for-calibration">
     Best practices for calibration
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#useful-functions">
     Useful functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-about-class-imbalance">
   What about class imbalance?
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-good-calibration-metric-in-imbalanced-cases">
     A good calibration metric in imbalanced cases
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-two-step-approach-to-calibrating-in-imbalanced-cases">
     A two-step approach to calibrating in imbalanced cases
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-naive-approach-model-not-optimized-for-imbalanced-data">
       a) Naive approach - model NOT optimized for imbalanced data
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#trying-the-wallace-dahabreh-underbagging-approach">
       Trying the Wallace-Dahabreh underbagging approach
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#b-better-approach-model-suited-for-imbalanced-data">
       b) Better approach - model suited for imbalanced data
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#same-exercise-different-model">
     Same exercise, different model
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#is-the-underbagging-approach-always-the-best">
       Is the underbagging approach always the best?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recommendations">
     Recommendations
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix">
   Appendix
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#roc-auc-and-the-ks-score-are-invariant-under-calibration-average-precision-almost">
     ROC AUC and the KS score are invariant under calibration; average precision almost
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#references">
     References:
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="model-calibration-part-i-reliability-assessment">
<h1>Model calibration part I - reliability assessment<a class="headerlink" href="#model-calibration-part-i-reliability-assessment" title="Permalink to this headline">#</a></h1>
<p>This section is all about estimating the <strong>score function</strong>
$<span class="math notranslate nohighlight">\(\boxed{f(x) = \mathbb P (Y=1|X=x)}\)</span>$</p>
<p>which, as we know, really generates everything in binary classification. This is our main object of interest.</p>
<p>Recall that one can obtain binary predictions from a score <span class="math notranslate nohighlight">\(f\)</span> by choosing a threshold <span class="math notranslate nohighlight">\(\lambda\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat{y}(x) = \begin{cases}
1 &amp; \mbox{if } f(x) \geq \lambda \\
0 &amp; \mbox{if } f(x) &lt; \lambda
\end{cases}
\end{split}\]</div>
<p>Now, forget that we know that <span class="math notranslate nohighlight">\(f\)</span> is a probability and let us take the definition of <span class="math notranslate nohighlight">\(\hat y(x)\)</span> above as-is, for any real-valued function <span class="math notranslate nohighlight">\(f\)</span>. It still makes sense: for big values of <span class="math notranslate nohighlight">\(f(x)\)</span> it spits out a 1, and for small values it returns a 0.</p>
<p>In fact, <strong>most machine learning algorithms don’t estimate the score</strong> <span class="math notranslate nohighlight">\(\mathbb P (Y=1|X=x)\)</span>. They just calculate a new function, say <span class="math notranslate nohighlight">\(g\)</span>, which approximates <span class="math notranslate nohighlight">\(\mathbb P\)</span>, in the sense that</p>
<div class="math notranslate nohighlight">
\[g(x) = \mathbb{\widehat P}(Y=1|X=x)\]</div>
<p>from which one can create rules exactly like before:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\hat y(x) = \begin{cases}
1 &amp; \mbox{if } \hat g(x) \geq \lambda \\
0 &amp; \mbox{if } \hat g(x) &lt; \lambda
\end{cases}
\end{split}\]</div>
<p>In fact, <span class="math notranslate nohighlight">\(g\)</span> does not even need to be bound to <span class="math notranslate nohighlight">\([0,1]\)</span> (but we can suppose it is, with no loss of generality; were it not, we would just need to normalize it).</p>
<p><strong>Is <span class="math notranslate nohighlight">\(g\)</span> a probability?</strong> No, not necessarily. But why?</p>
<section id="when-scores-are-not-probabilities">
<h2>When scores are not probabilities<a class="headerlink" href="#when-scores-are-not-probabilities" title="Permalink to this headline">#</a></h2>
<p>For some arbitrary random variable <span class="math notranslate nohighlight">\(Z\)</span>, What does <span class="math notranslate nohighlight">\(\mathbb P (Z&lt;40) = 0.2\)</span> mean?</p>
<p>An interesting way to characterize this is via simulation. Let <span class="math notranslate nohighlight">\(Z_1, Z_2, \ldots, Z_n\)</span> be <span class="math notranslate nohighlight">\(n\)</span> iid copies of <span class="math notranslate nohighlight">\(Z\)</span>. If we measure all the <span class="math notranslate nohighlight">\(Z_i\)</span>’s, and build a histogram, what the statement <span class="math notranslate nohighlight">\(\mathbb P (Z&lt;40) = 0.2\)</span> really means is that <em>about 20% of the samples will have values lesser than 40</em>.</p>
<p>This same reasoning can be applied to the score function.</p>
</section>
<section id="why-would-you-want-probabilities-anyway">
<h2>Why would you want probabilities anyway?<a class="headerlink" href="#why-would-you-want-probabilities-anyway" title="Permalink to this headline">#</a></h2>
<p>In some cases, you don’t. If all you care about is getting a model and predicting 1’s and 0’s, it doesn’t need to be calibrated.</p>
<p>But in some cases you do. A classical example is in credit. Say you are a bank, and John wishes to get a loan from you. You need to define an interest rate <span class="math notranslate nohighlight">\(r\)</span> to be applied to the loan.</p>
<p>Suppose John’s actual probability of defaulting on the payment is <span class="math notranslate nohighlight">\(p\)</span>. If you lend him <span class="math notranslate nohighlight">\(L\)</span> dollars, this means that:</p>
<ul class="simple">
<li><p>With probability <span class="math notranslate nohighlight">\(p\)</span>, you lose all your money; your profit is <span class="math notranslate nohighlight">\(-L\)</span>;</p></li>
<li><p>With probability <span class="math notranslate nohighlight">\(1-p\)</span>, he pays it all back with interest; your profit is <span class="math notranslate nohighlight">\(+rL\)</span></p></li>
</ul>
<p>Your expected percent return is</p>
<div class="math notranslate nohighlight">
\[\frac{\mathbb E[\mathrm{profit}]}{L} = \frac{- p L + (1-p) r L}{L} = -p + r(1-p)\]</div>
<p>Further assume your bank wants to have an expected return on this loan of <span class="math notranslate nohighlight">\(\alpha\)</span> (eg. 5%), that is</p>
<div class="math notranslate nohighlight">
\[-p+r(1-p) \geq \alpha \quad \Rightarrow \quad r \geq \frac{\alpha+p}{1-p}\]</div>
<p>Suppose <span class="math notranslate nohighlight">\(p = 0.01\)</span> and <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>. This implies <span class="math notranslate nohighlight">\(r \geq 6\%\)</span> is an adequate interest to apply to the loan.</p>
<p><strong>But suppose our model is badly calibrated</strong>. Then <span class="math notranslate nohighlight">\(p\)</span> could have been taken to be 0.05, for example, and the interest would be more than 10%!</p>
<blockquote>
<div><p><strong>Conclusion: models need to be well-calibrated when the actual probabilities will be used, and not only the model binary predictions or their ranking!</strong></p>
</div></blockquote>
</section>
<hr class="docutils" />
<section id="a-first-glance-at-model-reliability">
<h2>A first glance at model reliability<a class="headerlink" href="#a-first-glance-at-model-reliability" title="Permalink to this headline">#</a></h2>
<p>Create a sample dataset, in which we will train 3 different models: gradient boosted trees, random forest, and a good old logistic regression.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">,</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_repeated</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>

<span class="n">gb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>
<span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>For each of the models, let’s plot their test set performances:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">precision_recall_curve</span><span class="p">,</span> <span class="n">average_precision_score</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compare_models</span><span class="p">(</span><span class="n">model_list</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">),</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">model_list</span><span class="p">:</span>

        <span class="n">model_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>

        <span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

        <span class="c1"># ROC</span>
        <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>
        <span class="n">auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>

        <span class="c1"># PR</span>
        <span class="n">prec</span><span class="p">,</span> <span class="n">rec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">precision_recall_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>
        <span class="n">ap</span> <span class="o">=</span> <span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>

        <span class="c1"># plot</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">model_name</span> <span class="o">+</span> <span class="s2">&quot; (AUC=</span><span class="si">{0:.2f}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">auc</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rec</span><span class="p">,</span> <span class="n">prec</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">model_name</span> <span class="o">+</span> <span class="s2">&quot; (AP=</span><span class="si">{0:.2f}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ap</span><span class="p">))</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;ROC curves&quot;</span><span class="p">);</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Precision-Recall curves&quot;</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">compare_models</span><span class="p">(</span><span class="n">model_list</span><span class="o">=</span><span class="p">[</span><span class="n">lr</span><span class="p">,</span> <span class="n">rf</span><span class="p">,</span> <span class="n">gb</span><span class="p">],</span> <span class="n">X_test</span><span class="o">=</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="o">=</span><span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_25_0.png" src="../_images/Calibration_25_0.png" />
</div>
</div>
<p>We can also just write the results into a table:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Model</p></th>
<th class="head"><p>AUC</p></th>
<th class="head"><p>AP</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Logistic regression</p></td>
<td><p>.80</p></td>
<td><p>.79</p></td>
</tr>
<tr class="row-odd"><td><p>Random forest</p></td>
<td><p>.95</p></td>
<td><p>.95</p></td>
</tr>
<tr class="row-even"><td><p>Gradient boosting</p></td>
<td><p>.98</p></td>
<td><p>.98</p></td>
</tr>
</tbody>
</table>
<p>We see that the gradient boosted model outperforms the other two in both metrics.</p>
<p>The question is, then, <strong>how do these models compare regarding how well they predict actual probabilities?</strong></p>
</section>
<section id="calibration-curves-aka-reliability-diagrams">
<h2>Calibration curves (aka reliability diagrams)<a class="headerlink" href="#calibration-curves-aka-reliability-diagrams" title="Permalink to this headline">#</a></h2>
<p>Let’s take the logistic regression model as an example. We predict its scores for all elements in the test set, and sort our data from low to high scores:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_probs</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s1">&#39;label&#39;</span><span class="p">:</span> <span class="n">y_test</span><span class="p">,</span>
    <span class="s1">&#39;scores&#39;</span><span class="p">:</span> <span class="n">y_probs</span><span class="p">,</span>
<span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>scores</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0.740110</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0.648457</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0.524043</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>0.939656</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0.164257</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now, there are two strategies we can follow in order to compare predicted vs. actual probabilities:</p>
<ol class="simple">
<li><p><strong>Quantile strategy</strong>: take your data (the DataFrame above) and split it into <span class="math notranslate nohighlight">\(n\)</span> equally sized chunks based on the list of scores. In each chunk, you can calculate the average of the predicted scores (say, 80%) as well as an average of how many times one had <span class="math notranslate nohighlight">\(y=1\)</span> inside that chunk. A well-calibrated model would have this latter frequency equal to 80% as well</p></li>
<li><p><strong>Uniform strategy</strong>: instead of splitting into equally sized chunks, we consider a sequence of fixed scores (eg, 10%, 20%, 30%…) and break the data into bins between each of these scores. Again, in each chunk, you’ll have an average of the predicted scores as well as an average of how many times one had <span class="math notranslate nohighlight">\(y=1\)</span> inside that chunk.</p></li>
</ol>
<p>From this reasoning, we could build a curve where, one one axis, we plot the average score inside each chunk, and on the other axis, we plot the average number of defaults observed.</p>
<p>We will run the analysis of the quantile strategy above step-by-step; both of them can be seen in the function <code class="docutils literal notranslate"><span class="pre">calib_curve</span></code> below in more detail. Let’s use 5 bins.</p>
<p>First, we sort our data from low to high scores, then split in equal-sized chunks; the Pandas <code class="docutils literal notranslate"><span class="pre">cut</span></code> function is useful for this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;scores&#39;</span><span class="p">)</span>

<span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;bins&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;scores&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">predictions</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>scores</th>
      <th>bins</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1273</th>
      <td>0</td>
      <td>0.008941</td>
      <td>(0.0, 0.2]</td>
    </tr>
    <tr>
      <th>52</th>
      <td>0</td>
      <td>0.009827</td>
      <td>(0.0, 0.2]</td>
    </tr>
    <tr>
      <th>4586</th>
      <td>0</td>
      <td>0.011319</td>
      <td>(0.0, 0.2]</td>
    </tr>
    <tr>
      <th>4442</th>
      <td>0</td>
      <td>0.017242</td>
      <td>(0.0, 0.2]</td>
    </tr>
    <tr>
      <th>5826</th>
      <td>0</td>
      <td>0.021725</td>
      <td>(0.0, 0.2]</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>3793</th>
      <td>1</td>
      <td>0.981494</td>
      <td>(0.8, 1.0]</td>
    </tr>
    <tr>
      <th>1161</th>
      <td>1</td>
      <td>0.983320</td>
      <td>(0.8, 1.0]</td>
    </tr>
    <tr>
      <th>3822</th>
      <td>1</td>
      <td>0.984830</td>
      <td>(0.8, 1.0]</td>
    </tr>
    <tr>
      <th>789</th>
      <td>1</td>
      <td>0.985066</td>
      <td>(0.8, 1.0]</td>
    </tr>
    <tr>
      <th>3112</th>
      <td>1</td>
      <td>0.990047</td>
      <td>(0.8, 1.0]</td>
    </tr>
  </tbody>
</table>
<p>6000 rows × 3 columns</p>
</div></div></div>
</div>
<p>Now we find the bin-wise average of scores: <code class="docutils literal notranslate"><span class="pre">predictions.groupby('bins').mean()</span></code> works. We also change the names of the columns to make sense:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">calibration</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;bins&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">calibration</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Fraction of positives&#39;</span><span class="p">,</span> <span class="s1">&#39;Average score&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">calibration</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Fraction of positives</th>
      <th>Average score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.138</td>
      <td>0.129</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.289</td>
      <td>0.299</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.493</td>
      <td>0.498</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.713</td>
      <td>0.701</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.884</td>
      <td>0.883</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>These are already our calibration curve axes. Plotting one against the other yields the calibration curve:m</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">calibration</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;Average score&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Fraction of positives&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Calibration with 5 bins&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Full calibration&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_43_0.png" src="../_images/Calibration_43_0.png" />
</div>
</div>
<p>We see that the logistic regression model is very well calibrated (at least at the 5-bin scale we are looking at). The curve is relatively close to the 45 degree line.</p>
<p>The function below does both methods:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">calib_curve</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;quantile&#39;</span><span class="p">):</span>
    
    <span class="k">assert</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;quantile&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">],</span> <span class="s2">&quot;Unrecognized method&quot;</span>
    
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
        <span class="s1">&#39;label&#39;</span><span class="p">:</span> <span class="n">y_true</span><span class="p">,</span>
        <span class="s1">&#39;scores&#39;</span><span class="p">:</span> <span class="n">y_probs</span><span class="p">,</span>
        <span class="p">})</span>
    
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;scores&#39;</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;quantile&#39;</span><span class="p">:</span>
        <span class="n">quantiles</span> <span class="o">=</span> <span class="p">[</span><span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;scores&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="p">)</span> <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> 
        <span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;bins&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;scores&#39;</span><span class="p">],</span> <span class="n">quantiles</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> 
        <span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;bins&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;scores&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="p">)</span>
        
    <span class="c1"># we can now aggregate: average y=1 per chunk and average score per chunk</span>
    <span class="n">calibration</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;bins&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">calibration</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Fraction of positives&#39;</span><span class="p">,</span> <span class="s1">&#39;Average score&#39;</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">calibration</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s use both methods and compare to the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> native method:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_bins</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">quantile_calibration</span> <span class="o">=</span> <span class="n">calib_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="n">n_bins</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;quantile&#39;</span><span class="p">)</span>
<span class="n">uniform_calibration</span> <span class="o">=</span> <span class="n">calib_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="n">n_bins</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.calibration</span> <span class="kn">import</span> <span class="n">CalibrationDisplay</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="c1"># Quantile calibration</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">quantile_calibration</span><span class="p">[</span><span class="s2">&quot;Average score&quot;</span><span class="p">],</span> <span class="n">quantile_calibration</span><span class="p">[</span><span class="s2">&quot;Fraction of positives&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ours&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">CalibrationDisplay</span><span class="o">.</span><span class="n">from_predictions</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span>
                                   <span class="n">y_probs</span><span class="p">,</span>
                                   <span class="n">n_bins</span><span class="o">=</span><span class="n">n_bins</span><span class="p">,</span>
                                   <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                   <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Sklearn&#39;</span><span class="p">,</span>
                                   <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;quantile&#39;</span><span class="p">)</span>

<span class="c1"># Uniform calibration</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">uniform_calibration</span><span class="p">[</span><span class="s2">&quot;Average score&quot;</span><span class="p">],</span> <span class="n">uniform_calibration</span><span class="p">[</span><span class="s2">&quot;Fraction of positives&quot;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ours&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="n">CalibrationDisplay</span><span class="o">.</span><span class="n">from_predictions</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span>
                                   <span class="n">y_probs</span><span class="p">,</span>
                                   <span class="n">n_bins</span><span class="o">=</span><span class="n">n_bins</span><span class="p">,</span>
                                   <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                   <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Sklearn&#39;</span><span class="p">,</span>
                                   <span class="n">strategy</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Quantile&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Uniform&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_49_0.png" src="../_images/Calibration_49_0.png" />
</div>
</div>
<p>As we can see, our curves are identical. Scikit-learn uses the same logic to build their curves. We can happily use their library. However, sometimes we will need to access the proportion of points in each bin; for that, we also create the function below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.calibration</span> <span class="kn">import</span> <span class="n">calibration_curve</span>

<span class="k">def</span> <span class="nf">calib_curve_probs</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">):</span>
    
    <span class="k">assert</span> <span class="n">method</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;quantile&#39;</span><span class="p">,</span> <span class="s1">&#39;uniform&#39;</span><span class="p">],</span> <span class="s2">&quot;Unrecognized method&quot;</span>
    
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
        <span class="s1">&#39;label&#39;</span><span class="p">:</span> <span class="n">y_true</span><span class="p">,</span>
        <span class="s1">&#39;scores&#39;</span><span class="p">:</span> <span class="n">y_probs</span><span class="p">,</span>
        <span class="p">})</span>
    
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;scores&#39;</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">method</span> <span class="o">==</span> <span class="s1">&#39;quantile&#39;</span><span class="p">:</span>
        <span class="n">quantiles</span> <span class="o">=</span> <span class="p">[</span><span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;scores&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">q</span><span class="p">)</span> <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> 
        <span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;bins&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;scores&#39;</span><span class="p">],</span> <span class="n">quantiles</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">bins</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> 
        <span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;bins&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="s1">&#39;scores&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="p">)</span>
        
    <span class="c1"># we can now aggregate: average y=1 per chunk and average score per chunk</span>
    <span class="n">calibration</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;bins&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">calibration</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Fraction of positives&#39;</span><span class="p">,</span> <span class="s1">&#39;Average score&#39;</span><span class="p">]</span>
    
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">calibration</span><span class="p">[</span><span class="s1">&#39;Average score&#39;</span><span class="p">],</span> <span class="n">calibration</span><span class="p">[</span><span class="s1">&#39;Fraction of positives&#39;</span><span class="p">]</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">predictions</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;bins&#39;</span><span class="p">)</span><span class="o">.</span>\
                   <span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span>\
                   <span class="n">values</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">p</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_calibration_curve</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">calib_curve_probs</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="n">n_bins</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>

    <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">/</span><span class="n">p</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="n">p</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
    
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="o">*</span><span class="n">p</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">legend</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Mean predicted probability&quot;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Fraction of positives&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">legend</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">100</span><span class="o">*</span><span class="n">p</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">legend</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Mean predicted probability&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Fraction of positives&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">legend</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_calibration_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="s1">&#39;Logistic regression&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_53_0.png" src="../_images/Calibration_53_0.png" />
</div>
</div>
<p>This plot includes the amount of points in each bin in the marker size.</p>
<section id="how-to-interpret-a-reliability-plot">
<h3>How to interpret a reliability plot?<a class="headerlink" href="#how-to-interpret-a-reliability-plot" title="Permalink to this headline">#</a></h3>
<p>Consider the plot on the right, for simplicity, and look at the y axis.</p>
<ul class="simple">
<li><p>For small probabilities (until around 0.25), our model is too “optimistic”: it outputs higher scores than it should</p></li>
<li><p>This behavior is the same for high probabilities (above 0.60)</p></li>
<li><p>It is opposite for intermediate probabilities (between 0.3 and 0.6), for which the model is actually pessimistic: the predicted scores are smaller than the corresponding probabilities</p></li>
</ul>
</section>
<section id="how-to-choose-the-number-of-bins">
<h3>How to choose the number of bins?<a class="headerlink" href="#how-to-choose-the-number-of-bins" title="Permalink to this headline">#</a></h3>
<p>There is a trade-off here:</p>
<ul class="simple">
<li><p>If there are too many bins, each bin will contain few points. The mean score inside will be noisy</p></li>
<li><p>If there are too little bins, each will comprehend a large range of scores; the mean will be precise but the variance. will be high. We also won’t have enough points to assess the calibration of the curve</p></li>
</ul>
<p>Below we display examples with increasing number of bins:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">axx</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">],</span> <span class="n">ax</span><span class="p">):</span>
    <span class="n">plot_calibration_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axx</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="s1">&#39;Logistic regression&#39;</span><span class="p">)</span>
    <span class="n">axx</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2"> bins&quot;</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_60_0.png" src="../_images/Calibration_60_0.png" />
</div>
</div>
</section>
</section>
<section id="metrics-for-assessing-model-reliability">
<h2>Metrics for assessing model reliability<a class="headerlink" href="#metrics-for-assessing-model-reliability" title="Permalink to this headline">#</a></h2>
<p>In what follows, we will use our three pre-trained models to study model calibration metrics:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">model</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">lr</span><span class="p">,</span> <span class="n">rf</span><span class="p">,</span> <span class="n">gb</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Logistic regression&quot;</span><span class="p">,</span> <span class="s2">&quot;Random forest&quot;</span><span class="p">,</span> <span class="s2">&quot;Gradient boosting&quot;</span><span class="p">]):</span>
    <span class="n">plot_calibration_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">legend</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
                                       
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Reliability curves for our models&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_63_0.png" src="../_images/Calibration_63_0.png" />
</div>
</div>
<section id="the-brier-skill-score">
<h3>The Brier (skill) score<a class="headerlink" href="#the-brier-skill-score" title="Permalink to this headline">#</a></h3>
<p>A commonly used metric to analyze model reliability is the <strong>Brier loss</strong>: given a dataset <span class="math notranslate nohighlight">\(\{x_i, y_i\}_{i\in 1:N}\)</span> and predicted scores <span class="math notranslate nohighlight">\(\{s_i = f(x_i)\}_{i\in 1:N}\)</span>, the Brier score is</p>
<div class="math notranslate nohighlight">
\[\boxed{\mathrm{BS} = \frac{1}{N}\sum_{i=1}^N (s_i - y_i)^2}\]</div>
<p>ie. the mean squared error applied directly to probabilities. A well calibrated-model will have low Brier score.</p>
<blockquote>
<div><p>This is an estimator of the theoretical quantity <span class="math notranslate nohighlight">\(\displaystyle \mathbb E[(f(X) - Y)^2]\)</span>.</p>
</div></blockquote>
<p>How much is low, though? Especially in imbalanced classification, one might argue that predicting all probabilities to be zero could yield a low benchmark to the Brier score. Because of that, we rather propose to use the following:</p>
<p><strong>Brier Skill Score</strong>. Let</p>
<div class="math notranslate nohighlight">
\[\bar y = \frac{1}{N} \sum_{i=1}^N y_i,\qquad y_i \in \{0,1\}\]</div>
<p>be the average observed frequency of the positive class (which we assume to be the minority class as well). Define a baseline Brier score as</p>
<div class="math notranslate nohighlight">
\[\mathrm{BS_{baseline}} = \frac{1}{N}\sum_{i=1}^N (\hat y - y_i)^2.\]</div>
<p>Then, the Brier Skill Score (BSS) is defined as</p>
<div class="math notranslate nohighlight">
\[\boxed{\mathrm{BSS} = 1 - \mathrm{\frac{BS}{BS_{baseline}}}}\]</div>
<p>A <strong>higher BSS</strong> indicates a better model. In fact, we can think of BSS as analogous to <span class="math notranslate nohighlight">\(R\)</span> squared in regression analysis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">brier_skill_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    
    <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">brier_score_loss</span>
    
    <span class="n">y_bar</span> <span class="o">=</span> <span class="n">y_true</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    
    <span class="n">bs_baseline</span> <span class="o">=</span> <span class="n">brier_score_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_bar</span><span class="p">)</span>
    <span class="n">bs</span> <span class="o">=</span> <span class="n">brier_score_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">bs</span><span class="o">/</span><span class="n">bs_baseline</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Brier skill score comparison (the higher, the better): &quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="p">[</span><span class="n">lr</span><span class="p">,</span> <span class="n">rf</span><span class="p">,</span> <span class="n">gb</span><span class="p">]:</span>

    <span class="n">model_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
    
    <span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">bss</span><span class="o">=</span> <span class="n">brier_skill_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">bss</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Brier skill score comparison (the higher, the better): 
LogisticRegression	 26.6%
RandomForestClassifier	 52.6%
HistGradientBoostingClassifier	 79.0%
</pre></div>
</div>
</div>
</div>
<p>This shows that not only do our gradient boosting model perform better, but it is better calibrated from a Brier score point of view.</p>
<p><strong>However!</strong> In this view, Random Forest seems to be much better calibrated than Logistic Regression; from a visual inspection of the calibration curves, that doesn’t seem to be the case. What is happening?</p>
<section id="is-the-brier-score-sufficient">
<h4>Is the Brier score sufficient?<a class="headerlink" href="#is-the-brier-score-sufficient" title="Permalink to this headline">#</a></h4>
<p>Not really. It is a good indication of overall calibration, but it is also “contaminated” by raw model performance: a better model (from a ROC AUC point a view) will, on average, have a higher Brier score by default.</p>
<p>A schematic view of this fact is:</p>
<div class="math notranslate nohighlight">
\[\boxed{\mathrm{Brier \sim Calibration + Performance}}\]</div>
<blockquote>
<div><p>Proof (optional): this decomposition supposes that the scores can only take values on a finite set <span class="math notranslate nohighlight">\(\{S_1,\ldots,S_K\}\)</span>, ie. in bins. We can then write
$<span class="math notranslate nohighlight">\(\mathrm{BS} = \frac 1N \sum_{i=1}^N(s_i - y_i)^2 = \frac 1N \sum_{k=1}^K \sum_{i: s_i = S_K} (S_k - y_i)^2\)</span><span class="math notranslate nohighlight">\(
For a fixed value \)</span>k<span class="math notranslate nohighlight">\(, there are \)</span>n_k<span class="math notranslate nohighlight">\( instances, out of which there are \)</span>o_k<span class="math notranslate nohighlight">\( where \)</span>y_i=1<span class="math notranslate nohighlight">\(. We can expand and then complete the square:
\)</span><span class="math notranslate nohighlight">\(
\begin{align}
\\
\sum_{i: s_i = S_k} (S_k - y_i)^2  &amp;= \sum_{i: s_i = S_k} (S_k^2 - 2 S_k y_i + y_i^2)\\
&amp;= n_k S_k^2 - 2 S_k \sum_{i: s_i = S_k} y_i + \sum_{i: s_i = S_k} y_i^2\\
&amp;= n_k S_k^2 - 2 S_k o_k + o_k \\
&amp;= n_k \left(S_k^2 - 2 S_k \frac{o_k}{n_k} \right) + o_k\\
&amp;= n_k \left[S_k^2 - 2 S_k \frac{o_k}{n_k} + \left( \frac{o_k}{n_k} \right)^2\right] + o_k - \frac{o_k^2}{n_k}\\
&amp;= n_k \left(S_k - \frac{o_k}{n_k} \right)^2 + n_k \frac{o_k}{n_k} \left(1- \frac{o_k}{n_k} \right)\\
&amp;\equiv n_k (S_k - \bar o_k)^2 + n_k \bar o_k (1-\bar o_k)\\
\end{align}
\)</span><span class="math notranslate nohighlight">\( 
where we defined \)</span>\bar o_k = o_k/n_k<span class="math notranslate nohighlight">\( as the bin-wise event frequency. Then, we can write the Brier score as
\)</span><span class="math notranslate nohighlight">\(\mathrm{BS} = \sum_{k=1}^K \frac{n_k}{N} (S_k - \bar o_k)^2 + \sum_{k=1}^K \frac{n_k}{N} \bar o_k (1-\bar o_k)\)</span><span class="math notranslate nohighlight">\( 
The first term measures how much the empirical frequency of events \)</span>\bar o_k<span class="math notranslate nohighlight">\( compares to the score bin \)</span>S_k$. For a perfectly calibrated model, they will be equal and the first term is zero - this is the calibration term. The second one is a Gini impurity-like quantity.</p>
</div></blockquote>
<p>Because of this, we need to be carefully with using the Brier score alone to measure calibration. A high ROC AUC will affect this assessment.</p>
</section>
</section>
<section id="calibration-spread-measures">
<h3>Calibration spread measures<a class="headerlink" href="#calibration-spread-measures" title="Permalink to this headline">#</a></h3>
<p>Looking at the calibration curves, one might be tempted to use them directly to measure calibration quality. This can be achieved by looking at the overall spread around the optimal calibration line, the 45 degree diagonal.</p>
<p>Given a point <span class="math notranslate nohighlight">\((x, y)\)</span> (where <span class="math notranslate nohighlight">\(x\)</span> here is a shorthand for the predicted probability of a bin, and <span class="math notranslate nohighlight">\(y\)</span> is a shorthand for the actual probability of the bin), a common metric seen in the literature (eg. <a class="reference external" href="https://people.cs.pitt.edu/~milos/research/AAAI_Calibration.pdf">here</a>) is the ECE and its cousin, the Maximum Calibration Error (MCE):</p>
<div class="math notranslate nohighlight">
\[\boxed{\mathrm{ECE} = \sum_{b=1}^B p_b |x_b - y_b|}\]</div>
<div class="math notranslate nohighlight">
\[\boxed{\mathrm{MCE} = \max_{b \in \{1,\ldots,B\}} |x_b-y_b|}\]</div>
<p>where <span class="math notranslate nohighlight">\(p_b\)</span> is the fraction of points falling in bin <span class="math notranslate nohighlight">\(b\)</span>. Note that the total bin count <span class="math notranslate nohighlight">\(B\)</span> is fixed here, and taken to be <span class="math notranslate nohighlight">\(B=10\)</span>. Note that the ECE calculation using the <code class="docutils literal notranslate"><span class="pre">quantile</span></code> strategy simply yields the <span class="math notranslate nohighlight">\(L1\)</span> error, analogous to our root-mean-squared one from the previous section.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ece</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">calib_curve_probs</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="n">n_bins</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">p</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">mce</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;uniform&#39;</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">calib_curve_probs</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="n">n_bins</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">method</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ECE comparison (the lower, the better): &quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="p">[</span><span class="n">lr</span><span class="p">,</span> <span class="n">rf</span><span class="p">,</span> <span class="n">gb</span><span class="p">]:</span>

    <span class="n">model_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
    
    <span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">ece</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">error</span><span class="p">,</span><span class="mi">1</span> <span class="p">)</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;-----&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MCE comparison (the lower, the better): &quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="p">[</span><span class="n">lr</span><span class="p">,</span> <span class="n">rf</span><span class="p">,</span> <span class="n">gb</span><span class="p">]:</span>

    <span class="n">model_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>
    
    <span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">mce</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="se">\t</span><span class="s2"> </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">error</span><span class="p">,</span><span class="mi">1</span> <span class="p">)</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ECE comparison (the lower, the better): 
LogisticRegression	 1.1%
RandomForestClassifier	 17.5%
HistGradientBoostingClassifier	 5.7%
-----
MCE comparison (the lower, the better): 
LogisticRegression	 3.3%
RandomForestClassifier	 25.7%
HistGradientBoostingClassifier	 16.7%
</pre></div>
</div>
</div>
</div>
<p>Both give qualitative results which are similar to the spread analysis we did before.</p>
</section>
</section>
<section id="summary-running-reliability-analysis">
<h2>Summary: running reliability analysis<a class="headerlink" href="#summary-running-reliability-analysis" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>First, decide if reliability / calibration is important to you or not</p>
<ul>
<li><p>If you just care about ordering points correctly, then you do not need calibrated models</p></li>
<li><p>If you do care about the actual probabilities, then you need calibration</p></li>
</ul>
</li>
<li><p>For each model you want to test:</p>
<ul>
<li><p>Calculate their calibration curves - run a visual analysis</p></li>
<li><p>Calculate Brier Skill Score</p></li>
<li><p>Calculate the ECE</p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="model-calibration-part-ii-calibrating-outputs">
<h1>Model calibration part II - calibrating outputs<a class="headerlink" href="#model-calibration-part-ii-calibrating-outputs" title="Permalink to this headline">#</a></h1>
<p>So far, we have studied how to assess and measure reliability in pre-trained models. In this section, we go one step further and discuss how to improve their reliability via a <strong>calibration procedure</strong>.</p>
<p>There are essentially two ways to calibrate a model:</p>
<ol class="simple">
<li><p>Take a pre-trained model and use a 1-1 function to “scale up and down” its outputs, so that it better reflects the inherent probabilities.</p></li>
<li><p>Train both model and the “calibrator” together from the beginning.</p></li>
</ol>
<p>We will discuss these two sequentially.</p>
<section id="but-first-should-you-be-calibrating">
<h2>But first: should you be calibrating?<a class="headerlink" href="#but-first-should-you-be-calibrating" title="Permalink to this headline">#</a></h2>
<p>Some guys at Google <a class="reference external" href="https://towardsdatascience.com/why-calibrators-part-1-of-the-series-on-probability-calibration-9110831c6bde">say you shouldn’t</a>. Their points:</p>
<ul class="simple">
<li><p>Calibration sweeps structural issues with yoour data/model under the rug</p></li>
<li><p>Calibration adds a new layer of complexity to your model, which needs to be maintained</p></li>
</ul>
<p>They are not wrong. It is true that bad model reliability might just be a downstream symptom of other issues, such as bad or incomplete data/features, bias, and overly strong regularization. These need to be dealt with separately.</p>
<p>But it is also true that in real-life applications there might be time/money/availability constraints which hinder you from improving your data. In that case, being able to count on a model having interpretable probabilistic outputs is crucial, and we cannot avoid calibration.</p>
</section>
<section id="calibrating-pre-trained-models">
<h2>Calibrating pre-trained models<a class="headerlink" href="#calibrating-pre-trained-models" title="Permalink to this headline">#</a></h2>
<p>The basic approach to calibrating pre-trained models is to apply a <strong>post-processing function</strong> with nice properties which maps predicted scores to newer scores which are closer to actual probabilities. More specifically, if <span class="math notranslate nohighlight">\(s \in [0,1]\)</span> is a predicted score, we want to find a function <span class="math notranslate nohighlight">\(\phi:  [0,1] \to [0,1]\)</span> such that <span class="math notranslate nohighlight">\(\phi(s)\)</span> is better calibrated than <span class="math notranslate nohighlight">\(s\)</span> itself.</p>
<p>There are two main categories of this “post-processing” calibration:</p>
<ul class="simple">
<li><p>Parametric approach (Platt, 1999) - usually with a <strong>logistic curve</strong></p></li>
<li><p>Non-parametric:</p>
<ul>
<li><p>Binning (quantile method)</p></li>
<li><p><strong>Isotonic-regression</strong>, via the PAV algorithm.</p></li>
<li><p>Others, such as BBQ (Bayesian Binning into Quantiles)</p></li>
</ul>
</li>
</ul>
<p>Methods in boldface are available in <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> via the <code class="docutils literal notranslate"><span class="pre">CalibratedClassifierCV</span></code> method.</p>
</section>
<section id="digression-what-is-an-isotonic-regression">
<h2>Digression: what is an isotonic regression?<a class="headerlink" href="#digression-what-is-an-isotonic-regression" title="Permalink to this headline">#</a></h2>
<p>Consider simple regression: we have a data set <span class="math notranslate nohighlight">\(\{x_i, y_i\}_{i\in 1:N}\)</span> where both <span class="math notranslate nohighlight">\(x_i\)</span>’s and <span class="math notranslate nohighlight">\(y_i\)</span>’s are real numbers. Our goal is to find a simple function <span class="math notranslate nohighlight">\(f(x)\)</span> which fits the data well, <strong>which is also monotonic</strong>.</p>
<blockquote>
<div><p>A function <span class="math notranslate nohighlight">\(f\)</span> is said to be monotonic if it is either non-increasing or non-decreasing. For instance, the function <span class="math notranslate nohighlight">\(f(x) = x^3\)</span> is monotonic over <span class="math notranslate nohighlight">\(\mathbb R\)</span> since it is non-decreasing (being in fact purely increasing: <span class="math notranslate nohighlight">\(y &gt; x \Rightarrow f(y) &gt; f(x)\)</span>.</p>
</div></blockquote>
<p>This can be written as a quadratic optimization problem. First, with no loss of generality assume that <span class="math notranslate nohighlight">\(x_i\)</span>’s are ordered, such that <span class="math notranslate nohighlight">\(x_{i+1} \geq x_i\)</span>.</p>
<p>The problem is then to find <span class="math notranslate nohighlight">\(f\)</span> such that $<span class="math notranslate nohighlight">\(\sum_{i=1}^N (y_i - f(x_i))^2\)</span><span class="math notranslate nohighlight">\( is minimal, subject to \)</span>f(x_{i+1}) \geq f(x_i)<span class="math notranslate nohighlight">\( for all \)</span>i$.</p>
<p>This can be solved via the so-called PAV algorithm. Below, we show an example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># some point scatter</span>
<span class="n">x0</span> <span class="o">=</span> <span class="mi">5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">200</span><span class="p">))</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We now fit an isotonic regression:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.isotonic</span> <span class="kn">import</span> <span class="n">IsotonicRegression</span>

<span class="n">iso</span> <span class="o">=</span> <span class="n">IsotonicRegression</span><span class="p">(</span><span class="n">increasing</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">iso</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span><span class="n">x1</span><span class="p">)</span>

<span class="n">x0_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x0</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x0</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
<span class="n">x1_</span> <span class="o">=</span> <span class="n">iso</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x0_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Original scatter&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x0_</span><span class="p">,</span> <span class="n">x1_</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Isotonic regression&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_100_0.png" src="../_images/Calibration_100_0.png" />
</div>
</div>
<p>As you can see, the isotonic regression fits a piecewise monotonic function over the data!</p>
</section>
<section id="best-practices-for-calibration">
<h2>Best practices for calibration<a class="headerlink" href="#best-practices-for-calibration" title="Permalink to this headline">#</a></h2>
<p>The points below were adapted from [1]:</p>
<ul class="simple">
<li><p>Always <strong>separate a (stratified) calibration dataset</strong> (if using <code class="docutils literal notranslate"><span class="pre">CalibratedClassifierCV</span></code> this is already done for you)</p></li>
<li><p><strong>Ensure your calibration dataset / CV folds are large enough</strong>; ideally with a 1000-2000 or more entries</p></li>
<li><p>If using neural networks or logistic regression, most likely calibration won’t help much. Reliability metrics will either stay the same or even get worse.</p></li>
<li><p><strong>Tree-based models and SVM will benefit from calibration</strong>. Their reliability curves will most likely have an S-shape; naive Bayes will also benefit, with a reliability curve initially looking like an inverted S-shape.</p></li>
<li><p>Use <strong>isotonic regression</strong>.</p></li>
</ul>
<p>Below, we train a few different models and calibrate them using <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_calib_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">calib_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>

    <span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
    
    <span class="n">X_train_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train_val</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span>
                                                                <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>
    
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_train_val</span><span class="p">,</span> <span class="n">y_train_val</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">calib_size</span><span class="o">/</span><span class="p">(</span><span class="n">train_size</span><span class="o">+</span><span class="n">calib_size</span><span class="p">),</span> 
                                                      <span class="n">stratify</span><span class="o">=</span><span class="n">y_train_val</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="o">+</span><span class="mi">12</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">y_test</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Making data - same as before</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_repeated</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_calib</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_calib_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                                                     <span class="n">train_size</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
                                                                     <span class="n">calib_size</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span>
                                                                     <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
                                                                     <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Train models on TRAINING set</span>

<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.calibration</span> <span class="kn">import</span> <span class="n">CalibratedClassifierCV</span>

<span class="n">models</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span>
    <span class="n">LogisticRegression</span><span class="p">(),</span>
    <span class="n">GaussianNB</span><span class="p">()</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calibrate models on VALIDATION set</span>
<span class="n">calib_models</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">models</span><span class="p">:</span>
    <span class="n">calibrator</span> <span class="o">=</span> <span class="n">CalibratedClassifierCV</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="s1">&#39;prefit&#39;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;isotonic&#39;</span><span class="p">)</span>
    <span class="n">calibrator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">)</span>
    <span class="n">calib_models</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calibrator</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assess performance on TEST set</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">nrows</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="k">for</span> <span class="n">axx</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">calibrator</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">calib_models</span><span class="p">):</span>
    
    <span class="n">model_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="c1"># base model</span>
    <span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">error_base</span> <span class="o">=</span> <span class="n">ece</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>
    <span class="n">plot_calibration_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axx</span><span class="p">,</span> 
                           <span class="n">legend</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Pre-calibration  (ECE=</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">error_base</span><span class="p">)</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">)</span>
    
    <span class="c1"># calibrated model</span>
    <span class="n">y_probs</span> <span class="o">=</span> <span class="n">calibrator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">error_calib</span> <span class="o">=</span> <span class="n">ece</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>
    <span class="n">plot_calibration_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axx</span><span class="p">,</span> 
                           <span class="n">legend</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Post-calibration (ECE=</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">error_calib</span><span class="p">)</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">)</span>
        
    <span class="n">axx</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span> 

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_110_0.png" src="../_images/Calibration_110_0.png" />
</div>
</div>
<p>In all cases except Logistic Regression, the calibrated models performed better in terms of ECE (and even in the logistic case, the ECE values are close).</p>
<p>Also notice the characteristic S-shape in the Random Forest / Gradient Boosting reliability plots, and the inverse S-shape for Gaussian Naive Bayes.</p>
<p>Finally, looking at the marker sizes, we see that</p>
</section>
<section id="useful-functions">
<h2>Useful functions<a class="headerlink" href="#useful-functions" title="Permalink to this headline">#</a></h2>
<p>Below, we make available a function to get a pre-trained model and display its <strong>performance &amp; calibration</strong> metrics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">classification_model_report</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    
    <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">average_precision_score</span><span class="p">,</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">precision_recall_curve</span>
    <span class="kn">from</span> <span class="nn">sklearn.calibration</span> <span class="kn">import</span> <span class="n">CalibratedClassifierCV</span>
    
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="nb">type</span><span class="p">(</span><span class="n">model</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
    
    <span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># ROC</span>
    <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>
    <span class="n">auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>
    
    <span class="c1"># PR</span>
    <span class="n">prec</span><span class="p">,</span> <span class="n">rec</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">precision_recall_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>
    <span class="n">ap</span> <span class="o">=</span> <span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>
    
    
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ROC base (AUC=</span><span class="si">{0:.2f}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">auc</span><span class="p">));</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;ROC curve&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;FPR&quot;</span><span class="p">);</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;TPR&quot;</span><span class="p">)</span>
    
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rec</span><span class="p">,</span> <span class="n">prec</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;PR base (AP=</span><span class="si">{0:.2f}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">ap</span><span class="p">));</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Precision-Recall curve&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Recall&quot;</span><span class="p">);</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Precision&quot;</span><span class="p">)</span>
    
    <span class="c1"># Calibration</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">ece</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span> 
    <span class="n">plot_calibration_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> 
                           <span class="n">legend</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Pre-calibration  (ECE=</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">error</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">)</span>


    <span class="n">calibrator</span> <span class="o">=</span> <span class="n">CalibratedClassifierCV</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="s1">&#39;prefit&#39;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;isotonic&#39;</span><span class="p">)</span>
    <span class="n">calibrator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">)</span>
    <span class="n">y_probs</span> <span class="o">=</span> <span class="n">calibrator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">ece</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>
    
    <span class="n">plot_calibration_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> 
                                        <span class="n">legend</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Post-calibration  (ECE=</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="mi">100</span><span class="o">*</span><span class="n">error</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="si">}</span><span class="s1">%)&#39;</span><span class="p">)</span>

    <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span> <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Reliability curve&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="n">calibrator</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example: on gradient boosting model vs. random forest</span>
<span class="n">calib_model</span> <span class="o">=</span> <span class="n">classification_model_report</span><span class="p">(</span><span class="n">models</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_115_0.png" src="../_images/Calibration_115_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">calib_model</span> <span class="o">=</span> <span class="n">classification_model_report</span><span class="p">(</span><span class="n">models</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_116_0.png" src="../_images/Calibration_116_0.png" />
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="what-about-class-imbalance">
<h1>What about class imbalance?<a class="headerlink" href="#what-about-class-imbalance" title="Permalink to this headline">#</a></h1>
<p>Intuitively, higher class imbalance means</p>
<ol class="simple">
<li><p>Less trustworthy binning of points, with less points of the minority class per bin</p></li>
<li><p>More concentration of scores in very high or very low values</p></li>
<li><p>A need to use metrics which are sensitive to class imbalance</p></li>
</ol>
<p>As [3] points out,</p>
<blockquote>
<div><p>Class probability estimates attained via supervised learning in imbalanced scenarios <strong>systematically underestimate the probabilities for minority class instances</strong>, despite ostensibly good overall calibration.</p>
</div></blockquote>
<p>The key point here is that we are still getting bad estimates for minority class probabilities, <strong>regardless of overall good calibration</strong>. Again, [3] reiterates that:</p>
<blockquote>
<div><p>Probability estimates for the minority instances in imbalanced datasets are unreliable, and methods for mitigating <em>class imbalance</em> for classification do not automatically fix <em>calibration</em>.</p>
</div></blockquote>
<p>Starting from the conclusion:</p>
<ul class="simple">
<li><p><strong>Train the best model you can</strong>. In many cases, using best practices for training models in imbalanced situations will be enough (we will see this below).</p></li>
<li><p><strong>Try using regular calibration, but don’t expect much of it</strong>. This is a consequence of the argument above. Most likely, it will improve calibration for the majority class only.</p></li>
<li><p><strong>Choose calibration approaches which are adapted to imbalanced scenarios</strong>. We describe one of these below via bagging.</p></li>
</ul>
<section id="a-good-calibration-metric-in-imbalanced-cases">
<h2>A good calibration metric in imbalanced cases<a class="headerlink" href="#a-good-calibration-metric-in-imbalanced-cases" title="Permalink to this headline">#</a></h2>
<p>Wallace &amp; Dahabreh (2012) in [3] propose to study two instead of just one Brier score, stratifying by label:</p>
<div class="math notranslate nohighlight">
\[\mathrm{BS}_1 = \frac{1}{N_1} \sum_{i: y_i = 1} (s_i - 1)^2\]</div>
<div class="math notranslate nohighlight">
\[\mathrm{BS}_0 = \frac{1}{N_0} \sum_{i: y_i = 0} (s_i - 0)^2\]</div>
<p>where <span class="math notranslate nohighlight">\(N_i\)</span> is the total of entries in each class. Arguably, in the imbalanced case we care more about <span class="math notranslate nohighlight">\(\mathrm{BS}_1\)</span>, and it is the metric that should be included in analyses.</p>
<p>The function below calculates both <strong>stratified Brier scores</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">stratified_brier_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">):</span>
    <span class="n">dic</span><span class="o">=</span> <span class="p">{</span>
        <span class="mi">1</span><span class="p">:</span> <span class="p">((</span><span class="n">y_probs</span><span class="p">[</span><span class="n">y_true</span><span class="o">==</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
        <span class="mi">0</span><span class="p">:</span> <span class="p">((</span><span class="n">y_probs</span><span class="p">[</span><span class="n">y_true</span><span class="o">==</span><span class="mi">0</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dic</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Brier&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We will test this function in the next section.</p>
</section>
<section id="a-two-step-approach-to-calibrating-in-imbalanced-cases">
<h2>A two-step approach to calibrating in imbalanced cases<a class="headerlink" href="#a-two-step-approach-to-calibrating-in-imbalanced-cases" title="Permalink to this headline">#</a></h2>
<p>Reference [3] suggests we use two sequential approaches to solve for class imbalance:</p>
<blockquote>
<div><p>Obs: here, we assume a <strong>pre-trained model</strong>! Therefore, techniques to solve for class imbalance in training can be used at will; this discussion applies to the calibration process only.</p>
</div></blockquote>
<ol class="simple">
<li><p><strong>Random undersampling</strong> of the majority class will equilibrate class balance; however, it will introduce a bias due to the random nature of the undersampling</p></li>
<li><p><strong>Bagging</strong> solves the bias issue by creating various boostrap samples of the undersampled data and calibrating to each one, then averaging the results.</p></li>
</ol>
<p>We will call this the <strong>underbagging approach</strong> (Wallace-Dahabreh approach) for simplicity.</p>
<p>An extra step that can be done with <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> is to change <code class="docutils literal notranslate"><span class="pre">class_weights</span></code> (when available); this is similar to undersampling in which positive samples will have a higher weight.</p>
<p>Let’s run this procedure for a toy dataset. Consider the same data we’ve been using so far, but with a high imbalance rate:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_repeated</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                           <span class="n">weights</span><span class="o">=</span><span class="p">(</span><span class="mf">0.99</span><span class="p">,))</span> <span class="c1">## 99 to 1 proportion between classes</span>
</pre></div>
</div>
</div>
</div>
<p>We split into a stratified train/calibration/test base. The train data will be used to train a model (however we like); we will then modify the calibration data to solve for imbalance. All results will be tested in the test data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_calib</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_calib_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                                                     <span class="n">train_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                                                                     <span class="n">calib_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span>
                                                                     <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
                                                                     <span class="n">random_state</span><span class="o">=</span><span class="mi">21</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="a-naive-approach-model-not-optimized-for-imbalanced-data">
<h3>a) Naive approach - model NOT optimized for imbalanced data<a class="headerlink" href="#a-naive-approach-model-not-optimized-for-imbalanced-data" title="Permalink to this headline">#</a></h3>
<p>For a second, ignore the recommended underbagging procedure and see what we get with a naive usage of the methods so far:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                               <span class="c1"># class_weight=&quot;balanced_subsample&quot;) # we will turn this on later!</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">model_calib</span> <span class="o">=</span> <span class="n">classification_model_report</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_137_0.png" src="../_images/Calibration_137_0.png" />
</div>
</div>
<p>Something is weird. Why are the <strong>ECEs so low</strong> when the curves (both pre and post calibration) look so bad? The answer lies in the definition of ECE. Recall that it is the <em>weighted</em> sum of <span class="math notranslate nohighlight">\(|x-y|\)</span> for each bin. Look at the bubble size: basically all points are thrown to zero! We can see that more clearly via a histogram:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">calib_curve_probs</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">n_bins</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span> <span class="n">p</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yscale</span><span class="p">(</span><span class="s1">&#39;log&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Fraction of points in bin&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">));</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Bin upper range&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mf">0.90</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_140_0.png" src="../_images/Calibration_140_0.png" />
</div>
</div>
<p>More than 99% of points have a score between 0 and 10%; that makes sense, since the data is heavily imbalanced, so most points have very little chance of being in the positive class.</p>
<p>Because of that, using ECE won’t help much, and we need other metrics.</p>
<p><strong>But does naive calibration help with the stratified Brier score?</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Original model&quot;</span><span class="p">)</span>
<span class="n">stratified_brier_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> 
    <span class="n">y_probs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Original model
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Brier</th>
      <td>0.879947</td>
      <td>0.000352</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Calibrated model&quot;</span><span class="p">)</span>
<span class="n">stratified_brier_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> 
    <span class="n">y_probs</span><span class="o">=</span><span class="n">model_calib</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Calibrated model
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Brier</th>
      <td>0.785159</td>
      <td>0.000754</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>There was indeed an improvement on the stratified Brier score for class 1 (10 points), despite a worsening for class 0.</p>
</section>
<section id="trying-the-wallace-dahabreh-underbagging-approach">
<h3>Trying the Wallace-Dahabreh underbagging approach<a class="headerlink" href="#trying-the-wallace-dahabreh-underbagging-approach" title="Permalink to this headline">#</a></h3>
<p>Let us then use the undersampling+bagging procedure. We build a function to get boostrap samples of the calibration database:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">undersample_boostrap</span><span class="p">(</span><span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">,</span> <span class="n">target_balance</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    
    <span class="c1"># separate positive and negative classes</span>
    <span class="n">X1</span> <span class="o">=</span> <span class="n">X_calib</span><span class="p">[</span><span class="n">y_calib</span><span class="o">==</span><span class="mi">1</span><span class="p">];</span> <span class="n">n1</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X1</span><span class="p">)</span>
    <span class="n">X0</span> <span class="o">=</span> <span class="n">X_calib</span><span class="p">[</span><span class="n">y_calib</span><span class="o">==</span><span class="mi">0</span><span class="p">];</span> <span class="n">n0</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X0</span><span class="p">)</span>

    <span class="c1"># undersample X0</span>
    <span class="n">bs_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">n0</span><span class="p">),</span> 
                            <span class="n">size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">n1</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">target_balance</span><span class="p">)</span><span class="o">/</span><span class="n">target_balance</span><span class="p">),</span>
                            <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">X0_u</span> <span class="o">=</span> <span class="n">X0</span><span class="p">[</span><span class="n">bs_indices</span><span class="p">];</span> <span class="n">n0_u</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X0_u</span><span class="p">)</span>

    <span class="c1"># rebuild</span>
    <span class="n">X_calib_u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">X1</span><span class="p">,</span> <span class="n">X0_u</span><span class="p">])</span>
    <span class="n">y_calib_u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n0_u</span><span class="p">)])</span>

    <span class="c1"># shuffle - or should it be bootstrap ?</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_calib_u</span><span class="p">))</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>

    <span class="n">X_calib_u</span> <span class="o">=</span> <span class="n">X_calib_u</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
    <span class="n">y_calib_u</span> <span class="o">=</span> <span class="n">y_calib_u</span><span class="p">[</span><span class="n">indices</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">X_calib_u</span><span class="p">,</span> <span class="n">y_calib_u</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span>

<span class="k">class</span> <span class="nc">BaggedCalibratedPredictor</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">pretrained_model</span><span class="p">,</span>
                 <span class="n">target_balance</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">bootstrap_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">pretrained_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">target_balance</span> <span class="o">=</span> <span class="n">target_balance</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bootstrap_samples</span> <span class="o">=</span> <span class="n">bootstrap_samples</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="kc">False</span>
        
    <span class="k">def</span> <span class="nf">calibrate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">):</span>
        
        <span class="n">calibrator_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bootstrap_samples</span><span class="p">):</span>
            <span class="n">calibrator</span> <span class="o">=</span> <span class="n">CalibratedClassifierCV</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span>
                                                <span class="n">cv</span><span class="o">=</span><span class="s1">&#39;prefit&#39;</span><span class="p">,</span> 
                                                <span class="n">method</span><span class="o">=</span><span class="s1">&#39;isotonic&#39;</span><span class="p">)</span>

            <span class="n">X_boot</span><span class="p">,</span> <span class="n">y_boot</span> <span class="o">=</span> <span class="n">undersample_boostrap</span><span class="p">(</span><span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">,</span> 
                                                  <span class="n">target_balance</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">target_balance</span><span class="p">)</span>
            <span class="n">calibrator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_boot</span><span class="p">,</span> <span class="n">y_boot</span><span class="p">)</span>
            <span class="n">calibrator_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calibrator</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">calibrator_list</span> <span class="o">=</span> <span class="n">calibrator_list</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trained</span> <span class="o">=</span> <span class="kc">True</span>
    
    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">trained</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Calibrator not calibrated yet&quot;</span><span class="p">)</span>
        
        <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">calibrator</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">calibrator_list</span><span class="p">:</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">calibrator</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        
        <span class="n">mean_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mean_pred</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="c1"># Not implemented</span>
        <span class="k">return</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bagged_calib</span> <span class="o">=</span> <span class="n">BaggedCalibratedPredictor</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> 
                                  <span class="n">target_balance</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">bootstrap_samples</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
<span class="n">bagged_calib</span><span class="o">.</span><span class="n">calibrate</span><span class="p">(</span><span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stratified_brier_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> 
    <span class="n">y_probs</span><span class="o">=</span><span class="n">bagged_calib</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Brier</th>
      <td>0.334018</td>
      <td>0.071806</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>So we see our strategy worked. We did improve significantly the calibration for the positive class by almost three times; however, we did so by significantly killing the calibration for class 0.</p>
<p>Summarizing results:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># base model</span>
<span class="n">df_base</span> <span class="o">=</span> <span class="n">stratified_brier_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> 
    <span class="n">y_probs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># simply calibrated model</span>
<span class="n">df_cal</span> <span class="o">=</span> <span class="n">stratified_brier_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> 
    <span class="n">y_probs</span><span class="o">=</span><span class="n">model_calib</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># undersample+bagged calibrated model</span>
<span class="n">bagged_calib</span> <span class="o">=</span> <span class="n">BaggedCalibratedPredictor</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> 
                                  <span class="n">target_balance</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">bootstrap_samples</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
<span class="n">bagged_calib</span><span class="o">.</span><span class="n">calibrate</span><span class="p">(</span><span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">)</span>

<span class="n">df_bagcal</span> <span class="o">=</span> <span class="n">stratified_brier_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> 
    <span class="n">y_probs</span><span class="o">=</span><span class="n">bagged_calib</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># join everything together</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_base</span><span class="p">,</span> <span class="n">df_cal</span><span class="p">,</span> <span class="n">df_bagcal</span><span class="p">])</span>
<span class="n">res</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Original&#39;</span><span class="p">,</span> <span class="s1">&#39;Simple&#39;</span><span class="p">,</span> <span class="s1">&#39;Underbagged&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stratified Brier score comparison [bad base model]&#39;</span><span class="p">)</span>
<span class="n">res</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Stratified Brier score comparison [bad base model]
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Original</th>
      <td>0.879947</td>
      <td>0.000352</td>
    </tr>
    <tr>
      <th>Simple</th>
      <td>0.785159</td>
      <td>0.000754</td>
    </tr>
    <tr>
      <th>Underbagged</th>
      <td>0.333115</td>
      <td>0.072168</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="b-better-approach-model-suited-for-imbalanced-data">
<h3>b) Better approach - model suited for imbalanced data<a class="headerlink" href="#b-better-approach-model-suited-for-imbalanced-data" title="Permalink to this headline">#</a></h3>
<p>Let us repeat the procedure above, but with a better random forest model (including class weights).</p>
<p>Specifically, we will compare:</p>
<ul class="simple">
<li><p>Original model</p></li>
<li><p>Simple calibration of original model</p></li>
<li><p>Underbagging calibration of model</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                               <span class="n">class_weight</span><span class="o">=</span><span class="s2">&quot;balanced_subsample&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">model_calib</span> <span class="o">=</span> <span class="n">classification_model_report</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_159_0.png" src="../_images/Calibration_159_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># base model</span>
<span class="n">df_base</span> <span class="o">=</span> <span class="n">stratified_brier_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> 
    <span class="n">y_probs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># simply calibrated model</span>
<span class="n">df_cal</span> <span class="o">=</span> <span class="n">stratified_brier_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> 
    <span class="n">y_probs</span><span class="o">=</span><span class="n">model_calib</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># undersampling+bagging calibrated model</span>
<span class="n">bagged_calib</span> <span class="o">=</span> <span class="n">BaggedCalibratedPredictor</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> 
                                  <span class="n">target_balance</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">bootstrap_samples</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
<span class="n">bagged_calib</span><span class="o">.</span><span class="n">calibrate</span><span class="p">(</span><span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">)</span>

<span class="n">df_bagcal</span> <span class="o">=</span> <span class="n">stratified_brier_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> 
    <span class="n">y_probs</span><span class="o">=</span><span class="n">bagged_calib</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># join everything together</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_base</span><span class="p">,</span> <span class="n">df_cal</span><span class="p">,</span> <span class="n">df_bagcal</span><span class="p">])</span>
<span class="n">res</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Original&#39;</span><span class="p">,</span> <span class="s1">&#39;Simple&#39;</span><span class="p">,</span> <span class="s1">&#39;Underbagged&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stratified Brier score comparison [improved model]&#39;</span><span class="p">)</span>
<span class="n">res</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Stratified Brier score comparison [improved model]
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Original</th>
      <td>0.283600</td>
      <td>0.095445</td>
    </tr>
    <tr>
      <th>Simple</th>
      <td>0.781644</td>
      <td>0.000783</td>
    </tr>
    <tr>
      <th>Underbagged</th>
      <td>0.283162</td>
      <td>0.069271</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>What do we see here? The original model is already pretty well-calibrated for both classes (although better for 0 than 1). Upon trying to run simple calibration, it destroys that - class 0 gets extremely well-calibrated, whereas class 1 gets much worse. Indeed, these new Brier scores are similar to those of the calibrated “naive” model with no weights. Finally, underbagging calibration yields similar results to those of the original model, showing that in this particular case there isn’t much value in calibrating the model, but it doesn’t hurt either.</p>
<p>Had we not used a better random forest model (with weights), the underbagging calibration process would have been our best shot (and indeed, its Brier score for both classes is similar to the final one we got here as well). Thus, it is a good idea to use this method especially if the trained model is not “smart” against imbalance.</p>
</section>
</section>
<section id="same-exercise-different-model">
<h2>Same exercise, different model<a class="headerlink" href="#same-exercise-different-model" title="Permalink to this headline">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code> model does not have an option to pass class weights, but we can specify sample weights during training.</p>
<p>We will choose sample weights to balance the odds:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">positives</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">negatives</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span> <span class="o">-</span> <span class="n">positives</span>
<span class="n">sample_weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="n">negatives</span><span class="o">/</span><span class="n">positives</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weights</span><span class="p">)</span>

<span class="c1"># calibrate model</span>
<span class="n">model_calib</span> <span class="o">=</span> <span class="n">CalibratedClassifierCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="s1">&#39;prefit&#39;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;isotonic&#39;</span><span class="p">)</span>
<span class="n">model_calib</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># base model</span>
<span class="n">df_base</span> <span class="o">=</span> <span class="n">stratified_brier_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> 
    <span class="n">y_probs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># simply calibrated model</span>
<span class="n">df_cal</span> <span class="o">=</span> <span class="n">stratified_brier_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> 
    <span class="n">y_probs</span><span class="o">=</span><span class="n">model_calib</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># bagged calibrated model</span>
<span class="n">bagged_calib</span> <span class="o">=</span> <span class="n">BaggedCalibratedPredictor</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> 
                                  <span class="n">target_balance</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">bootstrap_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">bagged_calib</span><span class="o">.</span><span class="n">calibrate</span><span class="p">(</span><span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">)</span>

<span class="n">df_bagcal</span> <span class="o">=</span> <span class="n">stratified_brier_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> 
    <span class="n">y_probs</span><span class="o">=</span><span class="n">bagged_calib</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># join everything together</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_base</span><span class="p">,</span> <span class="n">df_cal</span><span class="p">,</span> <span class="n">df_bagcal</span><span class="p">])</span>
<span class="n">res</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Original&#39;</span><span class="p">,</span> <span class="s1">&#39;Simple&#39;</span><span class="p">,</span> <span class="s1">&#39;Underbagged&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stratified Brier score comparison&#39;</span><span class="p">)</span>
<span class="n">res</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Stratified Brier score comparison
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Original</th>
      <td>0.337104</td>
      <td>0.026831</td>
    </tr>
    <tr>
      <th>Simple</th>
      <td>0.618758</td>
      <td>0.001429</td>
    </tr>
    <tr>
      <th>Underbagged</th>
      <td>0.247081</td>
      <td>0.054970</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>This case exemplifies well how useful the underbagging calibration method can be. The original model had OK calibration for class 1, and a good one for class 0. Upon standard calibration, we get the usual trend: class 0 gets much better calibrated, whereas class 1 worsens. Finally, using bagged calibration, we improve the original calibration of class 1 while not too strongly worsening the calibration of class 0.</p>
<p>Finally, a good sanity check is to see if our metrics are still preserved:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">compare_models</span><span class="p">([</span><span class="n">model</span><span class="p">,</span> <span class="n">model_calib</span><span class="p">,</span> <span class="n">bagged_calib</span><span class="p">],</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_170_0.png" src="../_images/Calibration_170_0.png" />
</div>
</div>
<section id="is-the-underbagging-approach-always-the-best">
<h3>Is the underbagging approach always the best?<a class="headerlink" href="#is-the-underbagging-approach-always-the-best" title="Permalink to this headline">#</a></h3>
<p>No. The example below with a <strong>Logistic Regression</strong> proves it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># calibrate model</span>
<span class="n">model_calib</span> <span class="o">=</span> <span class="n">CalibratedClassifierCV</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="s1">&#39;prefit&#39;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;isotonic&#39;</span><span class="p">)</span>
<span class="n">model_calib</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">);</span>

<span class="c1"># base model</span>
<span class="n">df_base</span> <span class="o">=</span> <span class="n">stratified_brier_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> 
    <span class="n">y_probs</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># simply calibrated model</span>
<span class="n">df_cal</span> <span class="o">=</span> <span class="n">stratified_brier_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> 
    <span class="n">y_probs</span><span class="o">=</span><span class="n">model_calib</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># bagged calibrated model</span>
<span class="n">bagged_calib</span> <span class="o">=</span> <span class="n">BaggedCalibratedPredictor</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> 
                                  <span class="n">target_balance</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">bootstrap_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">bagged_calib</span><span class="o">.</span><span class="n">calibrate</span><span class="p">(</span><span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">)</span>

<span class="n">df_bagcal</span> <span class="o">=</span> <span class="n">stratified_brier_score</span><span class="p">(</span>
    <span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> 
    <span class="n">y_probs</span><span class="o">=</span><span class="n">bagged_calib</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="p">)</span>

<span class="c1"># join everything together</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df_base</span><span class="p">,</span> <span class="n">df_cal</span><span class="p">,</span> <span class="n">df_bagcal</span><span class="p">])</span>
<span class="n">res</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Original&#39;</span><span class="p">,</span> <span class="s1">&#39;Simple&#39;</span><span class="p">,</span> <span class="s1">&#39;Underbagged&#39;</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Stratified Brier score comparison&#39;</span><span class="p">)</span>
<span class="n">res</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Stratified Brier score comparison
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>1</th>
      <th>0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Original</th>
      <td>0.197709</td>
      <td>0.207775</td>
    </tr>
    <tr>
      <th>Simple</th>
      <td>0.868663</td>
      <td>0.001230</td>
    </tr>
    <tr>
      <th>Underbagged</th>
      <td>0.353928</td>
      <td>0.083891</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The original model had a better calibration for the minority class than the underbagged one! This shows that one must use this procedure carefully. Particularly for logistic regression, where we already are fitting probabilities, calibration might not yield great results.</p>
</section>
</section>
<section id="recommendations">
<h2>Recommendations<a class="headerlink" href="#recommendations" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>First, decide if reliability / calibration is important to you or not</p>
<ul>
<li><p>If you just care about ordering points correctly, then you do not need calibrated models</p></li>
<li><p>If you do care about the actual probabilities, then you need calibration</p></li>
</ul>
</li>
<li><p>If you have a <em>balanced</em> classification problem:</p>
<ul>
<li><p>Reliability curves and ECE are the main tools to assess reliability</p></li>
<li><p>Standard model calibration might be enough, especially for tree-based models</p></li>
<li><p>Pre-fit <code class="docutils literal notranslate"><span class="pre">CalibratedClassifierCV</span></code> with isotonic regression is the way to go</p></li>
</ul>
</li>
<li><p>If you have an <em>imbalanced</em> problem:</p>
<ul>
<li><p>Smartly training a model which deals well with imbalance (via its algorithm, <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> or <code class="docutils literal notranslate"><span class="pre">sample_weights</span></code> parameters, etc) is crucial</p></li>
<li><p>Stratified Brier scores are your best choice of reliability metric</p></li>
<li><p>Undersampling+bagging calibration (also with isotonic regression) might be your best choice, especially for tree-based models.</p></li>
</ul>
</li>
<li><p>General best practices to run a calibration procedure:</p>
<ul>
<li><p>Always separate a (stratified) calibration dataset (or, if using  this is already done for you)</p></li>
<li><p>Ensure your calibration dataset / CV folds are large enough; ideally with a 1000-2000 or more entries</p></li>
<li><p>If using neural networks or logistic regression, most likely calibration won’t help much. Reliability metrics will either stay the same or even get worse. Still, doesn’t hurt to test</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="appendix">
<h1>Appendix<a class="headerlink" href="#appendix" title="Permalink to this headline">#</a></h1>
<section id="roc-auc-and-the-ks-score-are-invariant-under-calibration-average-precision-almost">
<h2>ROC AUC and the KS score are invariant under calibration; average precision almost<a class="headerlink" href="#roc-auc-and-the-ks-score-are-invariant-under-calibration-average-precision-almost" title="Permalink to this headline">#</a></h2>
<blockquote>
<div><p>This section was initially written for our <a class="reference external" href="https://medium.com/datalab-log/voc%C3%AA-deve-calibrar-seu-modelo-desbalanceado-ou-n%C3%A3o-3f111160653a">Medium post on model calibration</a>; we translate and present it here.</p>
</div></blockquote>
<p>Calibration is a procedure that changes the predicted scores monotonically, so that if we have two scores <span class="math notranslate nohighlight">\(q_1 \geq q_2\)</span> initially, then the calibration <span class="math notranslate nohighlight">\(\phi\)</span> must satisfy <span class="math notranslate nohighlight">\(\phi(q_1) \geq \phi(q_2 )\)</span>: the order does not change.</p>
<p>This means that metrics like the ROC AUC or the KS score (which only depend on the relative <em>order</em> of the scores) are not changed, and a calibrated model should give the same ROC AUC / KS as the original model.</p>
<blockquote>
<div><p>To see this, just notice that the ROC AUC (also known as the C statistic) can be defined as <span class="math notranslate nohighlight">\(\mathbb P(Z_1 \geq Z_0)\)</span> where <span class="math notranslate nohighlight">\(Z_1\)</span> are the predicted scores for members of class 1, and <span class="math notranslate nohighlight">\(Z_0\)</span> are the class 0 scores. Similarly, the KS score depends only on the ROC curve, which also does not change under calibration.</p>
</div></blockquote>
<p>What about <strong>average precision</strong> (ie. the area under the precision/recall curve)? If we naively calculate it using Scikit-Learn, it seems like it changes:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">average_precision_score</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_repeated</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_calib</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> \
    <span class="n">train_calib_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                           <span class="n">train_size</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
                           <span class="n">calib_size</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span>
                           <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">mod</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">mod_calib</span> <span class="o">=</span> <span class="n">CalibratedClassifierCV</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">mod</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="s1">&#39;prefit&#39;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;isotonic&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_calib</span><span class="p">,</span> <span class="n">y_calib</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AP base model:&quot;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]),</span><span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AP calibrated model  :&quot;</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">mod_calib</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]),</span><span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AP base model: 0.803
AP calibrated model  : 0.785
</pre></div>
</div>
</div>
</div>
<p>If we use the <code class="docutils literal notranslate"><span class="pre">PrecisionRecallDisplay</span></code> function, which shows the precision-recall curve and Avg. Precision, we see why:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">PrecisionRecallDisplay</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">PrecisionRecallDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">PrecisionRecallDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">mod_calib</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_187_0.png" src="../_images/Calibration_187_0.png" />
</div>
</div>
<p>For some reason it looks like we have a ladder function.</p>
<p>In fact, this is a graphic quirk. What happens is that the process of calibrating a model effectively ends up taking its convex hull into the ROC curve (see [4] for a proof). The convex clasp connects points on the ROC curve with line segments, in order to make it convex.</p>
<p>Scikit-Learn doesn’t save the whole curve; it just keeps the vertices connected by these line segments. When mapping these points to the PR (Precision/Recall) plane, we only have a few points, and due to the particular geometry of the PR plane, we cannot simply connect them with lines.</p>
<blockquote>
<div><p>It is possible to prove, via Bayes’ theorem, the following fact. Let <span class="math notranslate nohighlight">\((x,y)\)</span> be the coordinates of the ROC plane (ie, <span class="math notranslate nohighlight">\(x\)</span> is the false positive rate and <span class="math notranslate nohighlight">\(y\)</span> is the true positive rate). Also let <span class="math notranslate nohighlight">\(d\)</span> be the imbalance rate (for example, if we have a 1:2 ratio between class 1 and class 0, <span class="math notranslate nohighlight">\(d = 1/2\)</span>). If we call the coordinates in the PR plane <span class="math notranslate nohighlight">\((x', y')\)</span>, with <span class="math notranslate nohighlight">\(x'\)</span> being the recall and <span class="math notranslate nohighlight">\(y'\)</span> being the precision, then
\begin{align*}
x’ &amp;= y\
y’ &amp;= \frac{y}{y+ x/r}.
\end{align*}
Straight lines in the <span class="math notranslate nohighlight">\((x,y)\)</span> plane are not mapped to straight lines in the <span class="math notranslate nohighlight">\((x', y')\)</span> plane. Therefore, we cannot simply say that the precision/recall curve of the calibrated model can be obtained by linear interpolation of the vertices.</p>
</div></blockquote>
<p>Below, we solve this problem as follows:</p>
<ul class="simple">
<li><p>We calculate the ROC curve of the calibrated model;</p></li>
<li><p>We use <code class="docutils literal notranslate"><span class="pre">scipy.interpolate</span></code> to create a linear interpolation;</p></li>
<li><p>We sampled several points;</p></li>
<li><p>We switched to the PR plan;</p></li>
<li><p>We use numerical integration to calculate the area under the curve, ie. the Average Precision of the calibrated model.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">average_precision_improved</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">auc</span><span class="p">,</span> <span class="n">roc_curve</span>
    <span class="kn">from</span> <span class="nn">scipy.interpolate</span> <span class="kn">import</span> <span class="n">interp1d</span>
    
    <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>
    
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">1e-8</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="mf">1e-8</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">interp1d</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">y_true</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y_true</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

    <span class="n">rec</span>  <span class="o">=</span> <span class="n">y</span>
    <span class="n">prec</span> <span class="o">=</span> <span class="n">y</span><span class="o">/</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">x</span><span class="o">/</span><span class="n">d</span><span class="p">)</span>
    
    <span class="n">rec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rec</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">prec</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">prec</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">rec</span><span class="p">,</span> <span class="n">prec</span><span class="p">,</span> <span class="n">auc</span><span class="p">(</span><span class="n">rec</span><span class="p">,</span> <span class="n">prec</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rec</span><span class="p">,</span> <span class="n">prec</span><span class="p">,</span> <span class="n">ap</span> <span class="o">=</span> <span class="n">average_precision_improved</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span>
                                              <span class="n">y_probs</span><span class="o">=</span><span class="n">mod_calib</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">PrecisionRecallDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rec</span><span class="p">,</span> <span class="n">prec</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Calibrated (AP = </span><span class="si">{</span><span class="nb">round</span><span class="p">(</span><span class="n">ap</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Calibration_191_0.png" src="../_images/Calibration_191_0.png" />
</div>
</div>
<p>As we can see, the area is basically the same. So, there are no problems using Avg. Precision of the original model as a proxy.</p>
</section>
<section id="references">
<h2>References:<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>[1] Seminal paper: <a class="reference external" href="https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf">https://www.cs.cornell.edu/~alexn/papers/calibration.icml05.crc.rev3.pdf</a></p></li>
<li><p>[2] Bayesian binning (BBQ): <a class="reference external" href="https://people.cs.pitt.edu/~milos/research/AAAI_Calibration.pdf">https://people.cs.pitt.edu/~milos/research/AAAI_Calibration.pdf</a></p></li>
<li><p>[3] Imbalanced calibration: <a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/6413859">https://ieeexplore.ieee.org/abstract/document/6413859</a></p></li>
<li><p>[4] Fawcett, T., Niculescu-Mizil, A. PAV and the ROC convex hull. Mach Learn 68, 97–106 (2007). <a class="reference external" href="https://doi.org/10.1007/s10994-007-5011-0">https://doi.org/10.1007/s10994-007-5011-0</a></p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="Interpretability.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">How imbalanced learning affects the interpretability of a model</p>
        </div>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Experian LatAm DataLab<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>