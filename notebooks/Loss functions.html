
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Loss functions &#8212; Imbalanced Binary Classification - A survey with code</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Classification metrics" href="Metrics%201%20-%20Intro%20%26%20ROC%20AUC.html" />
    <link rel="prev" title="Introduction" href="Introduction.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/DataLab-logo-white.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Imbalanced Binary Classification - A survey with code</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="Introduction.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Loss functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%201%20-%20Intro%20%26%20ROC%20AUC.html">
   Classification metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%202%20-%20Lift%20curve.html">
   The lift curve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%203%20-%20KS%20score.html">
   The KS score and Youden’s J
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%204%20-%20Precision%20and%20Recall.html">
   Precision, recall, and
   <span class="math notranslate nohighlight">
    \(F\)
   </span>
   -scores
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pablo-baseline-experiment.html">
   Choosing a baseline model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Feature%20selection%20methods.html">
   Feature selection via the Boruta algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Calibration.html">
   Probability calibration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Interpretability.html">
   Model interpretability: how is it affected by imbalance?
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/notebooks/Loss functions.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/notebooks/Loss functions.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Loss functions
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-functions-executive-summary">
     Loss functions: executive summary
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#back-to-basics-the-log-loss">
     Back to basics: the log-loss
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#where-does-the-log-loss-come-from-the-bernoulli-formalism">
       Where does the log-loss come from? The Bernoulli formalism
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#why-do-we-use-the-log-loss">
       Why do we use the log-loss?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#are-there-other-losses">
       Are there other losses?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#possible-issues-with-our-loss-functions">
     Possible issues with our loss functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#losses-for-imbalanced-classification">
   Losses for imbalanced classification
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#doing-nothing">
     Doing nothing
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weighting">
     Weighting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#weights-change-the-threshold-distribution">
       Weights change the threshold distribution
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#focal-loss">
     Focal loss
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lightgbm-implementation-of-focal-loss">
       LightGBM implementation of focal loss
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#xgboost-implementation-of-focal-loss">
       XGBoost implementation of focal loss
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusions">
   Conclusions
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix-proper-scoring-rules">
   Appendix: proper scoring rules
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expected-losses">
     Expected losses
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#proper-scoring-rules">
       Proper scoring rules
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-weight-function-the-unifier-of-proper-scoring-rules">
       The weight function: the unifier of proper scoring rules
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-beta-family-of-proper-scoring-rules">
       The Beta family of proper scoring rules
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Loss functions</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Loss functions
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#loss-functions-executive-summary">
     Loss functions: executive summary
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#back-to-basics-the-log-loss">
     Back to basics: the log-loss
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#where-does-the-log-loss-come-from-the-bernoulli-formalism">
       Where does the log-loss come from? The Bernoulli formalism
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#why-do-we-use-the-log-loss">
       Why do we use the log-loss?
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#are-there-other-losses">
       Are there other losses?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#possible-issues-with-our-loss-functions">
     Possible issues with our loss functions
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#losses-for-imbalanced-classification">
   Losses for imbalanced classification
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#doing-nothing">
     Doing nothing
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weighting">
     Weighting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#weights-change-the-threshold-distribution">
       Weights change the threshold distribution
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#focal-loss">
     Focal loss
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#lightgbm-implementation-of-focal-loss">
       LightGBM implementation of focal loss
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#xgboost-implementation-of-focal-loss">
       XGBoost implementation of focal loss
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusions">
   Conclusions
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#appendix-proper-scoring-rules">
   Appendix: proper scoring rules
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expected-losses">
     Expected losses
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#proper-scoring-rules">
       Proper scoring rules
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-weight-function-the-unifier-of-proper-scoring-rules">
       The weight function: the unifier of proper scoring rules
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#the-beta-family-of-proper-scoring-rules">
       The Beta family of proper scoring rules
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="loss-functions">
<h1>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">#</a></h1>
<p>Every supervised learning algorithm is constructed as a ways to minimize some kind of <strong>loss function</strong>. Intuitively, the lower the loss, the better the algorithm has learned.</p>
<p>Data scientists are usually aware of the issue of overfitting: if you give your algorithm enough degrees of freedom, it will “memorize” the training data and achieve zero loss, but without being able to generalize to new data. Techniques such as regularization and dropoout are means to avoid overfitting; we won’t discuss them in this session, and assume they are being taken care of.</p>
<p>Instead, we will focus on a more fundamental question: what is a good choice for a  loss function?</p>
<section id="loss-functions-executive-summary">
<h2>Loss functions: executive summary<a class="headerlink" href="#loss-functions-executive-summary" title="Permalink to this headline">#</a></h2>
<p>We start with the conclusions for the lazy reader:</p>
<ul class="simple">
<li><p>Simply using <strong>log-loss with weights</strong> is usually more than enough. This can be achieved via <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> parameters in many standard algorithms, and worst-case scenario via <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> in the <code class="docutils literal notranslate"><span class="pre">.fit</span></code> method of all scikit-learn models.</p></li>
<li><p>Given the off-the-shelf LightGBM and XGBoost implementations, we recommend trying to use <strong>focal loss</strong> as an alternative loss function in the imbalanced case</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="back-to-basics-the-log-loss">
<h2>Back to basics: the log-loss<a class="headerlink" href="#back-to-basics-the-log-loss" title="Permalink to this headline">#</a></h2>
<p>In binary classification, the standard loss function is the <strong>log-loss</strong> (aka binary cross-entropy)</p>
<div class="math notranslate nohighlight">
\[\boxed{L( y| q) = - y \log q - (1-y) \log(1-q)}\]</div>
<p>where <span class="math notranslate nohighlight">\(y\)</span> is the real label taking values 0 or 1, and <span class="math notranslate nohighlight">\(q \in [0,1]\)</span> is a predicted score, <span class="math notranslate nohighlight">\(q = q(x) = \widehat{\mathbb P}(Y=1|X=x)\)</span>. The logarithm is usually taken to be the natural log (base <span class="math notranslate nohighlight">\(e\)</span>), although any other base would be equally valid. We take <span class="math notranslate nohighlight">\(0 \log 0 \equiv 0\)</span>, as would be the natural extension from the limit.</p>
<blockquote>
<div><p>For example: if we predict a score of <span class="math notranslate nohighlight">\(q=0.8\)</span> for a point with <span class="math notranslate nohighlight">\(y=1\)</span>, this yields a log-loss of <span class="math notranslate nohighlight">\(-\log 0.8 = 0.22\)</span>.</p>
</div></blockquote>
<p>This function has a few key properties:</p>
<ul class="simple">
<li><p>It is zero when we yield a perfectly sure predictor: if <span class="math notranslate nohighlight">\(y=1\)</span> and we predict <span class="math notranslate nohighlight">\(q=1\)</span>, the error is zero. Analogously for <span class="math notranslate nohighlight">\(y=0\)</span> and <span class="math notranslate nohighlight">\(q=0\)</span>.</p></li>
<li><p>It is non-negative: thus, a prediction can be more or less penalized, but there is always some level of penalty.</p></li>
<li><p><strong>It diverges</strong>: for <span class="math notranslate nohighlight">\(y=1\)</span>, it diverges at <span class="math notranslate nohighlight">\(q \to 0\)</span> and analogously for <span class="math notranslate nohighlight">\(y=0\)</span> it diverges at <span class="math notranslate nohighlight">\(q \to 1\)</span>. This is will be an important point of discussion in the following sections.</p></li>
</ul>
<section id="where-does-the-log-loss-come-from-the-bernoulli-formalism">
<h3>Where does the log-loss come from? The Bernoulli formalism<a class="headerlink" href="#where-does-the-log-loss-come-from-the-bernoulli-formalism" title="Permalink to this headline">#</a></h3>
<p>Recall our probabilistic setup: we consider a pair of jointly distributed variables <span class="math notranslate nohighlight">\((X,Y)\)</span> where <span class="math notranslate nohighlight">\(Y\)</span> takes values in <span class="math notranslate nohighlight">\(\{0,1\}\)</span>. This means that <span class="math notranslate nohighlight">\(\mathbb P(Y=1|X=x) \equiv q(x)\)</span> is some function of <span class="math notranslate nohighlight">\(x\)</span>, and similarly <span class="math notranslate nohighlight">\(\mathbb P(Y=0|X=x) = 1 - \mathbb P(Y=1|X=x) = 1-q(x)\)</span>.</p>
<p>We then say that the variable <span class="math notranslate nohighlight">\(Y|X=x\)</span> is a <strong>Bernoulli variable</strong> with parameter <span class="math notranslate nohighlight">\(q(x)\)</span>, and write</p>
<div class="math notranslate nohighlight">
\[\boxed{Y|X=x \; \sim \; \mathrm{Bernoulli}(q(x))}\]</div>
<p>The log-loss comes from the following result:</p>
<p><strong>Proposition</strong>. Let there be <span class="math notranslate nohighlight">\(N\)</span> iid random variables <span class="math notranslate nohighlight">\(Y_i \sim \mathrm{Bernoulli}(q)\)</span>, whose values we observe as <span class="math notranslate nohighlight">\(y_1,\ldots, y_N\)</span>. The maximum likelihood estimator <span class="math notranslate nohighlight">\(\hat q\)</span> of <span class="math notranslate nohighlight">\(q\)</span> is the one that maximizes the empirical likelihood function</p>
<div class="math notranslate nohighlight">
\[L(q) =  \sum_{i=1}^N [y_i \log q + (1-y_i) \log (1-q)],\]</div>
<p>namely</p>
<div class="math notranslate nohighlight">
\[\hat q = \frac{1}{N} \sum_{i=1}^N y_i\]</div>
<p><em>Proof</em>: notice that we can write the two expressions <span class="math notranslate nohighlight">\(\mathbb P(Y=1) =q \)</span> and <span class="math notranslate nohighlight">\(\mathbb P(Y=0) = 1 - q\)</span> concisely as</p>
<div class="math notranslate nohighlight">
\[\mathbb P(Y=y) = q^y (1-q)^{1-y}.\]</div>
<p>The joint probability of obtaining the observed values <span class="math notranslate nohighlight">\(y_1,\ldots, y_n\)</span> is then</p>
<div class="math notranslate nohighlight">
\[\mathbb P(Y_1 = y_1,\ldots, Y_N = y_N) = \prod_{i=1}^N q^{y_i} (1-q)^{1-y_i}.\]</div>
<p>Taking the log of both sides yields the log-likelihood function,</p>
<div class="math notranslate nohighlight">
\[L(q) = \log \prod_{i=1}^N q^{y_i} (1-q)^{1-y_i} = \sum_{i=1}^N [y_i \log q + (1-y_i) \log (1-q)]\]</div>
<p>Maximum likelihood estimation states that we want to find the <span class="math notranslate nohighlight">\(q\)</span> value that maximizes this quantity. This is equivalent to minimizing its negative, which is exactly the log-loss, as we wanted to show. <span class="math notranslate nohighlight">\(\blacksquare\)</span></p>
</section>
<section id="why-do-we-use-the-log-loss">
<h3>Why do we use the log-loss?<a class="headerlink" href="#why-do-we-use-the-log-loss" title="Permalink to this headline">#</a></h3>
<p>In most cases in binary classification, we use the log-loss as our loss function. Why?</p>
<p>It is a very natural choice. <em>Any</em> binary classification problem can be written as a (<span class="math notranslate nohighlight">\(x\)</span>-dependent) Bernoulli variable, ie. a Bernoulli problem where the parameter <span class="math notranslate nohighlight">\(q\)</span> is a function <span class="math notranslate nohighlight">\(q(x)\)</span>.</p>
<p>Logistic regression, in particular, <em>explicitly</em> writes <span class="math notranslate nohighlight">\(q(x) \equiv q_\beta(x)\)</span> as a sigmoid,</p>
<div class="math notranslate nohighlight">
\[q_\beta(x) = \frac{1}{\displaystyle 1 + e^{-\beta \cdot x}}\]</div>
<p>and so allows us to run gradient descent on <span class="math notranslate nohighlight">\(\beta\)</span> via explicit minimization of the log-loss.</p>
</section>
<section id="are-there-other-losses">
<h3>Are there other losses?<a class="headerlink" href="#are-there-other-losses" title="Permalink to this headline">#</a></h3>
<p>Absolutely. Any function which gives a positive number when the predicted score is “far” from the label and a small one otherwise is a valid loss. If this function is furthermore a differentiable function of the parameters, everything gets even better.</p>
<ul>
<li><p><strong>SVM example</strong>: in linear support vector machines we minimize the <strong>hinge loss</strong></p>
<div class="math notranslate nohighlight">
\[L_\mathrm{hinge} = \sum_{i=1}^N \max\{0, 1 - y_i (w\cdot x_i+b)\}\]</div>
<p>where <span class="math notranslate nohighlight">\(w\in\mathbb R^n\)</span>, <span class="math notranslate nohighlight">\(b\in\mathbb R\)</span> (and, here, <span class="math notranslate nohighlight">\(y_i \in \{-1, 1\}\)</span>). Notice that this is a completely different setup: instead of predicting a score <span class="math notranslate nohighlight">\(q\)</span>, we predict <span class="math notranslate nohighlight">\( F(x_i) \equiv w \cdot x_i + b\)</span> directly; if this quantity has the same sign as <span class="math notranslate nohighlight">\(y_i\)</span>, then their product is positive and the loss goes down, whereas if the signs are opposite this increases the loss.</p>
</li>
</ul>
<ul>
<li><p><strong>AdaBoost</strong>: in the AdaBoost algorithm, one often talks about the <strong>exponential loss</strong></p>
<div class="math notranslate nohighlight">
\[L_\mathrm{exp} = \frac{1}{N}\sum_{i=1}^N [y_i e^{-F(x_i)} + (1-y_i) e^{+F(x_i)}]\]</div>
<p>where <span class="math notranslate nohighlight">\(F(x)\)</span> is a transformed quantity, related to the predicted probability score (usually, we only take <span class="math notranslate nohighlight">\(\mathrm{sign}\;F(x)\)</span> in the end as the predicted variable).</p>
<p>Under the choice [1] of link function</p>
<div class="math notranslate nohighlight">
\[q(x) = \frac{1}{\displaystyle 1+e^{-2F(x)}} \quad \Leftrightarrow \quad F(x) = \frac 12 \log \frac{q}{1-q}\]</div>
<p>we can rewrite the loss function as a function of <span class="math notranslate nohighlight">\(y\)</span> and the score <span class="math notranslate nohighlight">\(q\)</span>, in an expression similar to the log-loss:</p>
<div class="math notranslate nohighlight">
\[\boxed{L_\mathrm{exp}(y|q) = y \sqrt{\frac{1-q}{q}} + (1-y) \sqrt{\frac{q}{1-q}}.}\]</div>
</li>
<li><p><strong>Brier score</strong>: as we will see in the Calibration section, we can define the so-called Brier loss (or Brier score)</p>
<div class="math notranslate nohighlight">
\[\boxed{L_\mathrm{Brier}(y|q) = (y-q)^2 = y(1-q)^2 + (1-y)q^2}\]</div>
</li>
<li><p><strong>Focal loss</strong>: for completeness, we write out here the focal loss, which we will discuss in the section about imbalanced learning:</p>
<div class="math notranslate nohighlight">
\[\boxed{L_\mathrm{focal}(y|q) = -y (1-q)^\gamma \log q - (1-\eta) q^\gamma \log(1-q)}\]</div>
</li>
</ul>
</section>
</section>
<section id="possible-issues-with-our-loss-functions">
<h2>Possible issues with our loss functions<a class="headerlink" href="#possible-issues-with-our-loss-functions" title="Permalink to this headline">#</a></h2>
<p>As we have mentioned before, the log-loss has divergent behavior for <span class="math notranslate nohighlight">\(q = 0\)</span> and <span class="math notranslate nohighlight">\(q=1\)</span>. Let us study that further.</p>
<p>To make things simpler, we will assume <span class="math notranslate nohighlight">\(y=1\)</span>; the discussion is completely analogous for <span class="math notranslate nohighlight">\(y=0\)</span>. Then, our log-loss is simply</p>
<div class="math notranslate nohighlight">
\[L(y|q) = - \log q.\]</div>
<p>This function diverges <em>exponentially</em> for <span class="math notranslate nohighlight">\(q \to 0\)</span> and <em>linearly</em> goes to 0 at <span class="math notranslate nohighlight">\(q=1\)</span>. See the figure below, where we also plot other loss functions that we have seen above:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.cm</span> <span class="k">as</span> <span class="nn">cm</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Log-loss: $-\log (q)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">q</span><span class="p">)</span><span class="o">/</span><span class="n">q</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Exp. loss: $\sqrt{(1-q)/q)}$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">q</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Brier loss: $(1-q)^2$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;q&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Loss functions_23_0.png" src="../_images/Loss functions_23_0.png" />
</div>
</div>
<p>The behavior we see for the log-loss is that it heavily penalizes misclassified points, due to the exponential behavior - which is good - but only decreases linearly as we classify correctly - which isn’t great.</p>
<p>Ideally, in many applications, you would like points which are “easier to classify” (meaning that the score is close to the actual label 0 or 1) to be “taken care of” first, and then deal with points which are harder to classify. For a classification problem, these are the points close to 0.5 (assuming good model calibration).</p>
<p>Notice that for the exponential loss, these characteristics (diverging at 0, slowly going to 0 at 1) are even more pronounced. The Brier loss has the issue that it doesn’t penalize enough points with low <span class="math notranslate nohighlight">\(q\)</span> here.</p>
<p>In other words, for all cases here, both misclassified and well-classified points count significantly towards the loss, with a higher contribution from misclassifications (and a particularly higher contribution in the case of log-loss and exponential loss).</p>
<p><strong>For those who have read the Appendix on proper scoring rules</strong></p>
<p>The issue is that all losses in the Beta family have weights of the form <span class="math notranslate nohighlight">\(\omega(q) = q^{\alpha-1} (1-q)^{\beta-1}\)</span>; as we can see in the plot below, for all values of <span class="math notranslate nohighlight">\(\alpha &lt;1\)</span>, we have the same behavior: convex curves which diverge at <span class="math notranslate nohighlight">\(q=0\)</span> and <span class="math notranslate nohighlight">\(q=1\)</span>. Therefore, qualitatively, they all have similar behaviors as log-loss. For <span class="math notranslate nohighlight">\(\alpha=1\)</span> (which corresponds to the Brier score), all values are equally weighted and the loss function doesn’t have enough discriminative power.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="k">for</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Exp. loss&#39;</span><span class="p">,</span> <span class="s1">&#39;Log-loss&#39;</span><span class="p">,</span> <span class="s1">&#39;Arcsin&#39;</span><span class="p">,</span> <span class="s1">&#39;Brier&#39;</span><span class="p">]):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="p">(</span><span class="n">q</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">q</span><span class="p">))</span><span class="o">**</span><span class="p">(</span><span class="n">alpha</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;alpha=</span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1"> (</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;q&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$\omega(q)$&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Weight factors for losses in the Beta family&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Loss functions_27_0.png" src="../_images/Loss functions_27_0.png" />
</div>
</div>
<p>In fact, we can explicitly use the expression</p>
<div class="math notranslate nohighlight">
\[L(y|q) = y B(1-q; \beta+1, \alpha) + (1-y) B(q; \alpha+1,\beta)\]</div>
<p>for the losses, and compute them explicitly. As before, take <span class="math notranslate nohighlight">\(y=1\)</span> and study that case particularly:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">incomplete_beta</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">beta</span><span class="p">,</span> <span class="n">betainc</span>
    
    <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-6</span> <span class="c1"># for numerical stability</span>
    <span class="k">return</span> <span class="n">beta</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">eps</span><span class="p">,</span> <span class="n">b</span><span class="o">+</span><span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">betainc</span><span class="p">(</span><span class="n">a</span><span class="o">+</span><span class="n">eps</span><span class="p">,</span> <span class="n">b</span><span class="o">+</span><span class="n">eps</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">incomplete_beta</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">q</span><span class="p">,</span> <span class="n">a</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">a</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">plasma</span><span class="p">((</span><span class="n">a</span><span class="o">+</span><span class="mf">0.5</span><span class="p">)</span><span class="o">/</span><span class="mf">2.5</span><span class="p">),</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Loss functions (for y=1) at different parameters alpha&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;q&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Loss functions_30_0.png" src="../_images/Loss functions_30_0.png" />
</div>
</div>
<p>We can the study all these losses’ behavior close to <span class="math notranslate nohighlight">\(q=1\)</span>. Taylor series are easy to calculate there, since our function <span class="math notranslate nohighlight">\(q \mapsto B(1-q;\beta+1,\alpha)\)</span> is described as an integral of polynomial-like quantities. Hence, we should not expect eg. exponential weighings close to 1, only polynomial behavior.</p>
<p>In conclusion, the Beta family provides us with a class of functions which diverge at the origin <span class="math notranslate nohighlight">\(q=0\)</span>, and are polynomially behaved at <span class="math notranslate nohighlight">\(q=1\)</span> (of course, the opposite is true if we study the loss function for <span class="math notranslate nohighlight">\(y=0\)</span>). This will affect how they perform in imbalanced cases, as we will see below.</p>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="losses-for-imbalanced-classification">
<h1>Losses for imbalanced classification<a class="headerlink" href="#losses-for-imbalanced-classification" title="Permalink to this headline">#</a></h1>
<p>In the imbalanced classification case, as usual, one needs to be careful. Below, we show some common strategies applied to an artificial dataset with 99:1 imbalance between classes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">HistGradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span><span class="p">,</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">average_precision_score</span><span class="p">,</span> <span class="n">make_scorer</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_repeated</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                           <span class="n">weights</span><span class="o">=</span><span class="p">(</span><span class="mf">0.99</span><span class="p">,))</span> <span class="c1">## 99 to 1 proportion between classes</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
                                                    <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="c1"># don&#39;t forget to stratify</span>
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="doing-nothing">
<h2>Doing nothing<a class="headerlink" href="#doing-nothing" title="Permalink to this headline">#</a></h2>
<p>What if we simply try to instantiate a model and fit it?</p>
<p>(Instead of cross-validation, we will be using simple train/test splits here to focus on the different methods)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># first model: gradient boosting</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC: </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AP:  </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC: 0.819
AP:  0.424
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># second model: random forest</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC: </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AP:  </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC: 0.820
AP:  0.451
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># third model: logistic regression</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC: </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AP:  </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC: 0.768
AP:  0.213
</pre></div>
</div>
</div>
</div>
<p>These will serve as our baseline models.</p>
</section>
<section id="weighting">
<h2>Weighting<a class="headerlink" href="#weighting" title="Permalink to this headline">#</a></h2>
<p>We can easily add weights to different training instances. Effectively, this means considering a weight vector <span class="math notranslate nohighlight">\(w_i\)</span> (one entry for each training sample) and the weighted loss</p>
<div class="math notranslate nohighlight">
\[L_\mathrm{weighted} = \frac{\sum_{i=1}^N w_i L(y_i|q_i)}{\sum_{i=1}^N w_i}\]</div>
<p>In scikit-learn, there are two ways of doing this. We illustrate them both below:</p>
<p><strong>1)  Direcly pass weights to</strong> <code class="docutils literal notranslate"><span class="pre">.fit</span></code></p>
<p>Let us build a weight vector which will <strong>give more weight to the minority class</strong>. Any choice would work, but we will specifically pick this one: let <span class="math notranslate nohighlight">\(f_1, f_0\)</span> denote the relative frequency of classes 1 and 2 (here, <span class="math notranslate nohighlight">\(f_1 \approx 0.01\)</span> and <span class="math notranslate nohighlight">\(f_0 \approx 0.99\)</span>; the approximation comes from the fact that <code class="docutils literal notranslate"><span class="pre">y_train</span></code> does not exactly keep the proportions perfect when stratified from <code class="docutils literal notranslate"><span class="pre">y</span></code>)</p>
<div class="math notranslate nohighlight">
\[\begin{split}w_i = \begin{cases}
\frac{1}{2 f_1} &amp; \mbox{if } y_i = 1\\
\frac{1}{2 f_0} &amp; \mbox{if } y_i = 0
\end{cases}\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n0</span><span class="p">,</span> <span class="n">n1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_train</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n1</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">n0</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>We can now retrain our gradient boosting classifier as follows:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># first model: gradient boosting</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>

<span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC: </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AP:  </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC: 0.819
AP:  0.374
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># second model: random forest</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>

<span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC: </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AP:  </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC: 0.821
AP:  0.486
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># third model: logistic regression</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">weights</span><span class="p">)</span>

<span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC: </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AP:  </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC: 0.780
AP:  0.118
</pre></div>
</div>
</div>
</div>
<p>We see that adding weights improved the performance of the random forest and logistic regression classifiers in terms of AUC; it didn’t do much for gradient boosting (and in fact we had some loss average-precision wise).</p>
<p><strong>2) Class weights on model instantiation</strong></p>
<p>Some scikit-learn classifiers have a <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> attribute. What it does is that it receives a dictionary `{0: w_0, 1: w_1}$ for weights to give to elements of each class.</p>
<p>Instead of passing this dictionary by hand, we can also use <code class="docutils literal notranslate"><span class="pre">class_weight='balanced'</span></code> to automatically give more weights to the minority class.</p>
<p>In most classifiers, this is equivalent to the weight rule we used above: you will see we get exactly the same results for both <code class="docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code> and <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># second model: random forest</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC: </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AP:  </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC: 0.821
AP:  0.486
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># third model: logistic regression</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC: </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AP:  </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC: 0.780
AP:  0.118
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code> however does not have a <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> attribute; for it, we need to specity <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code>.</p>
<p><strong>Weights can be cross-validated</strong></p>
<p>As we just saw here, the choice of weights can be thought of as yet another hyperparameter to cross-validate.</p>
<section id="weights-change-the-threshold-distribution">
<h3>Weights change the threshold distribution<a class="headerlink" href="#weights-change-the-threshold-distribution" title="Permalink to this headline">#</a></h3>
<p>Let us plot two ROC curves for the logistic regression model: one raw, and one with class weights. In each, we will also plot the different thresholds corresponding to each point.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">complete_plot_roc</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">threshold_step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
   
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run full ROC diagnostic</span>
<span class="sd">    &quot;&quot;&quot;</span>
   
    <span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">roc_auc_score</span>
    <span class="kn">from</span> <span class="nn">matplotlib.cm</span> <span class="kn">import</span> <span class="n">viridis</span> <span class="k">as</span> <span class="n">mycolormap</span>
   
    <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresh</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>
   
    <span class="c1"># x axis, y axis</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">fpr</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">tpr</span>
 
    <span class="n">auc</span> <span class="o">=</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)</span>
 
    <span class="c1"># color</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">thresh</span>
    <span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">c</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="mi">1</span>
    <span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="n">c</span> <span class="o">-</span> <span class="n">c</span><span class="o">.</span><span class="n">min</span><span class="p">())</span><span class="o">/</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">c</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
 
    <span class="n">base</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">threshold_step</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">mycolormap</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
 
        <span class="k">if</span> <span class="n">thresh</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">base</span><span class="p">:</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">mycolormap</span><span class="p">(</span><span class="n">c</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;  </span><span class="si">{0:.1f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">thresh</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mf">1.03</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mf">0.96</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
            <span class="n">base</span> <span class="o">-=</span> <span class="n">threshold_step</span>
   
    <span class="n">ax</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="s2">&quot;AUC = </span><span class="si">{0:.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">auc</span><span class="p">),</span> <span class="p">(</span><span class="mf">0.77</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
               <span class="n">bbox</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">facecolor</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mf">5.0</span><span class="p">))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;FPR&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;TPR&quot;</span><span class="p">)</span>
   
    <span class="k">return</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>

<span class="c1"># without weights</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_probs_raw</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">complete_plot_roc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs_raw</span><span class="p">,</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Without weights&quot;</span><span class="p">)</span>

<span class="c1"># with weights</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_probs_weights</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="n">complete_plot_roc</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs_weights</span><span class="p">,</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;With weights&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Loss functions_64_0.png" src="../_images/Loss functions_64_0.png" />
</div>
</div>
<p>Notice that the raw model heavily skews the threshold range: almost 80% of the curve lies within thresholds 0 and 0.1. The model with weights is much better at spacing the thresholds at uniform intervals.</p>
</section>
</section>
<section id="focal-loss">
<h2>Focal loss<a class="headerlink" href="#focal-loss" title="Permalink to this headline">#</a></h2>
<p>The <strong>focal loss</strong> was introduced by the team at Facebook research in [2], in the context of object detection. It is a loss function specialized in imbalanced classification, which we present below.</p>
<p>Recall that the standard log-loss for a single point is</p>
<div class="math notranslate nohighlight">
\[L(y|q) = - y \log q - (1-y) \log(1-q)\]</div>
<p>The authors in [2] propose a modification to this expression which adds an <strong>exponential decay term</strong> close to <span class="math notranslate nohighlight">\(q=1\)</span>:</p>
<div class="math notranslate nohighlight">
\[\boxed{L_\mathrm{focal}(y|q) = -y (1-q)^\gamma \log q - (1-\eta) q^\gamma \log(1-q)}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gamma</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Log-loss: $-\log (q)$&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="o">-</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">q</span><span class="p">)</span><span class="o">**</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">q</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Focal loss: $-(1-q)^\gamma \log (q)$&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;q&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Loss functions_70_0.png" src="../_images/Loss functions_70_0.png" />
</div>
</div>
<ul class="simple">
<li><p>For <span class="math notranslate nohighlight">\(q \approx 0\)</span>, the focal loss behaves very similarly to the log-loss; this is expected, since for small <span class="math notranslate nohighlight">\(q\)</span>, <span class="math notranslate nohighlight">\(L_\mathrm{focal}/L_\mathrm{log-loss} \approx 1 - \gamma q \to 1\)</span>.</p></li>
<li><p>However, for <span class="math notranslate nohighlight">\(q\)</span> closer to 1, the exponential decay quickly makes the loss go to 0. This means that the focal loss cares much less about “easily classifiable” points, and instead focuses on <strong>learning points which are harder to classify</strong>.</p></li>
</ul>
<section id="lightgbm-implementation-of-focal-loss">
<h3>LightGBM implementation of focal loss<a class="headerlink" href="#lightgbm-implementation-of-focal-loss" title="Permalink to this headline">#</a></h3>
<p>We have implemented in the <code class="docutils literal notranslate"><span class="pre">focal_lgbm.py</span></code> file a version of the standard LightGBM classifier which uses focal loss.
The rationale behind doing this in LightGBM is that it is a library which allows implementation of user-defined losses, as long as we also have the Jacobian/Hessian of that loss.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">lightgbm</span> <span class="k">as</span> <span class="nn">lgb</span>
<span class="kn">from</span> <span class="nn">focal_lgbm</span> <span class="kn">import</span> <span class="n">FocalLossLGBM</span>
</pre></div>
</div>
</div>
</div>
<p>First, we set a benchmark with the standard LightGBM classifier:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">LGBMClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC: </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AP:  </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC: 0.834
AP:  0.424
</pre></div>
</div>
</div>
</div>
<p>Then, we try the model with focal loss. Below, <code class="docutils literal notranslate"><span class="pre">alpha</span></code> is actually a weight similar to <code class="docutils literal notranslate"><span class="pre">w_i</span></code> in the discussions above. All other parameters aside from <code class="docutils literal notranslate"><span class="pre">gamma</span></code> are the same ones in standard LightGBM.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span>  <span class="n">FocalLossLGBM</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> 
                      <span class="n">verbose</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC: </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AP:  </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC: 0.845
AP:  0.485
</pre></div>
</div>
</div>
</div>
<p>The focal loss model performed better! In fact, with a little cheating, we can get it even better:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span>  <span class="n">FocalLossLGBM</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> 
                      <span class="n">learning_rate</span><span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
                      <span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
                      <span class="n">num_leaves</span><span class="o">=</span><span class="mi">63</span><span class="p">,</span>
                      <span class="n">verbose</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC: </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AP:  </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC: 0.863
AP:  0.530
</pre></div>
</div>
</div>
</div>
<p>In this particular case, the model with focal loss performed better than the baseline LightGBM and the other models.</p>
<p><strong>The whole point here is to optimize for hyperparameters.</strong> The choice of loss is also a hyperparameter!</p>
</section>
<section id="xgboost-implementation-of-focal-loss">
<h3>XGBoost implementation of focal loss<a class="headerlink" href="#xgboost-implementation-of-focal-loss" title="Permalink to this headline">#</a></h3>
<p>The team in [3] has made available an implementation of focal loss in XGBoost (<a class="reference external" href="https://github.com/jhwjhw0123/Imbalance-XGBoost">https://github.com/jhwjhw0123/Imbalance-XGBoost</a>). Let us test it here as well:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="ch">#!pip install imbalance-xgboost</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">imxgboost.imbalance_xgb</span> <span class="kn">import</span> <span class="n">imbalance_xgboost</span> <span class="k">as</span> <span class="n">imb_xgb</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/c91852a/.conda/envs/2022/lib/python3.8/site-packages/xgboost/compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.
  from pandas import MultiIndex, Int64Index
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xgboster</span> <span class="o">=</span> <span class="n">imb_xgb</span><span class="p">(</span><span class="n">special_objective</span><span class="o">=</span><span class="s1">&#39;focal&#39;</span><span class="p">,</span> <span class="n">focal_gamma</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">num_round</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">imbalance_alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xgboster</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_probs</span> <span class="o">=</span> <span class="n">xgboster</span><span class="o">.</span><span class="n">predict_sigmoid</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC: </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AP:  </span><span class="si">{0:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_probs</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>AUC: 0.869
AP:  0.490
</pre></div>
</div>
</div>
</div>
<p>This model (with a little tuning) performed even better than our LightGBM implementation!</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="conclusions">
<h1>Conclusions<a class="headerlink" href="#conclusions" title="Permalink to this headline">#</a></h1>
<ul class="simple">
<li><p>We obtained good results simply using <strong>log-loss with weights</strong> (via <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> or <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code>)</p></li>
<li><p>We were able to get even better results from <strong>focal loss</strong> via the LightGBM and XGBoost implementations in the imbalanced case, and we recommend trying it out</p></li>
</ul>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="appendix-proper-scoring-rules">
<h1>Appendix: proper scoring rules<a class="headerlink" href="#appendix-proper-scoring-rules" title="Permalink to this headline">#</a></h1>
<section id="expected-losses">
<h2>Expected losses<a class="headerlink" href="#expected-losses" title="Permalink to this headline">#</a></h2>
<p>All loss functions discussed here can be written in the form</p>
<div class="math notranslate nohighlight">
\[\boxed{L(y|q) = y L_1(1-q) + (1-y)L_0(q)}.\]</div>
<p>Here, we have explicitly written that <span class="math notranslate nohighlight">\(L\)</span> depends on <span class="math notranslate nohighlight">\(y\)</span> <em>conditioned on</em> the predicted score <span class="math notranslate nohighlight">\(q\)</span>. Notice that this is the only dependence that <span class="math notranslate nohighlight">\(y\)</span> has on <span class="math notranslate nohighlight">\(x\)</span>, namely via <span class="math notranslate nohighlight">\(q = q(x)\)</span>.</p>
<p><em>Example</em>: for the log-loss,</p>
<p>\begin{align*}
L_0(q) &amp;= - \log(1-q)\
L_1(1-q) &amp;= -\log q \quad \Rightarrow L_1(u) = - \log(1-u) = L_0(u)
\end{align*}</p>
<p>Notice that both functions are equal, but functions of different variables.</p>
<p><em>Example</em>: for the Brier loss,</p>
<div class="math notranslate nohighlight">
\[L_0(q) = q^2, \quad L_1(1-q) = (1-q)^2.\]</div>
<p>In what follows, <strong>we fix a point <span class="math notranslate nohighlight">\(X=x\)</span></strong>. Write <span class="math notranslate nohighlight">\(Y|X=x\)</span> as simply <span class="math notranslate nohighlight">\(Y\)</span> as an abuse of notation. Then,</p>
<div class="math notranslate nohighlight">
\[Y \sim \mathrm{Bernoulli}(\eta)\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta = \mathbb P(Y=1)\)</span> (really <span class="math notranslate nohighlight">\(\mathbb P(Y=1|X=x)\)</span>).</p>
<p>We define the <strong>expected loss</strong></p>
<div class="math notranslate nohighlight">
\[\boxed{R(\eta|q) = \mathbb E_Y L(Y|q).}\]</div>
<p>It is a function of two variables: the real Bernoulli parameter <span class="math notranslate nohighlight">\(\eta\)</span>, and the predicted score <span class="math notranslate nohighlight">\(q\)</span>.</p>
<p>Intuitively, what do we want? If we predict the parameter perfectly (ie. <span class="math notranslate nohighlight">\(q = \eta\)</span>) we would expect <span class="math notranslate nohighlight">\(R\)</span> to be minimized. This is a logical desideratum, but it is not obvious, and it motivates the following:</p>
<section id="proper-scoring-rules">
<h3>Proper scoring rules<a class="headerlink" href="#proper-scoring-rules" title="Permalink to this headline">#</a></h3>
<p>A loss function <span class="math notranslate nohighlight">\(L(y|q)\)</span> is a <strong>proper scoring rule</strong> if the expected loss <span class="math notranslate nohighlight">\(q \mapsto R(\eta|q)\)</span> is minimized by <span class="math notranslate nohighlight">\(q=\eta\)</span>. It is further described as <strong>strict</strong> if <span class="math notranslate nohighlight">\(\eta\)</span> is the global minimizer.</p>
<p>Notice that we can write, since <span class="math notranslate nohighlight">\(\mathbb E[Y] =\eta\)</span> if <span class="math notranslate nohighlight">\(Y \sim \mathrm{Bernoulli}(\eta)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\boxed{R(\eta|q) = \eta L_1 (1-q) + (1-\eta) L_0(q)}\]</div>
<p>Not everything is a proper scoring rule. The losses we mentioned above definitely are, but ROC AUC or Average Precision, for instance, are not. In fact, proper scoring rules have very interesting properties, which we now lay out:</p>
<p><strong>Theorem</strong> [Shuford-Albert-Massengil]</p>
<p><span class="math notranslate nohighlight">\(L(y|p)\)</span> is a proper scoring rule if and only if there exists a <strong>weight function</strong> <span class="math notranslate nohighlight">\(\omega: [0,1] \to \mathbb R_+\)</span> such that</p>
<div class="math notranslate nohighlight">
\[\frac{d L_1(1-q)}{d(1-q)} = \omega(q) (1-q),\qquad \frac{d L_0}{dq} = \omega(q) q;\]</div>
<p>if these are bounded below, then the equations can be integrated to give</p>
<div class="math notranslate nohighlight">
\[L_1(1-q) = \int_q^1 (1-c) \omega(c) dc,\qquad L_0(q) = \int_0^q c \omega(c) dc,\]</div>
<p>[and in fact, this is the general form regardless of smoothness, due to a theorem by Schervish 1989].</p>
<p>In the case of twice-differentiable <span class="math notranslate nohighlight">\(R\)</span>,</p>
<div class="math notranslate nohighlight">
\[\boxed{\omega(\eta) = \left.\frac{\partial^2 R(\eta|q)}{\partial q^2} \right|_{q = \eta}}\]</div>
<p><em>Example</em>: consider again the log-loss, where <span class="math notranslate nohighlight">\(L(y|q) = - y \log q - (1-y) \log (1-q)\)</span>. The expected loss is</p>
<div class="math notranslate nohighlight">
\[R(\eta|q) = -\mathbb E[y] \log q - (1-\mathbb E[y]) \log(1-q) = - \eta \log q - (1-\eta)\log(1-q).\]</div>
<p>Differentiating, we get</p>
<div class="math notranslate nohighlight">
\[\frac{\partial R}{\partial q} = - \frac{\eta}{q} + \frac{1-\eta}{1-q};\quad \frac{\partial^2 R}{\partial q^2} =  \frac{\eta}{q^2} + \frac{1-\eta}{(1-q)^2}\]</div>
<div class="math notranslate nohighlight">
\[\Rightarrow \omega(\eta) = \left.\frac{\partial^2 R}{\partial q^2}\right|_{q=\eta} = \frac{1}{\eta (1-\eta)}.\]</div>
</section>
<section id="the-weight-function-the-unifier-of-proper-scoring-rules">
<h3>The weight function: the unifier of proper scoring rules<a class="headerlink" href="#the-weight-function-the-unifier-of-proper-scoring-rules" title="Permalink to this headline">#</a></h3>
<p>The calculation in the example above is actually interesting because it shows that this weight function (which we haven’t interpreted yet) diverges at both <span class="math notranslate nohighlight">\(\eta = 0\)</span> and <span class="math notranslate nohighlight">\(\eta = 1\)</span>, similarly to how the log-loss itself does.</p>
<p>To motivate the reasoning of <span class="math notranslate nohighlight">\(\omega\)</span> as a weight, we notice a theorem by Shuford, Albert, Massengil, Savage and Scherviss (see [1]) which states the following:</p>
<p>Fix <span class="math notranslate nohighlight">\(0 &lt; c &lt; 1\)</span>, and let</p>
<div class="math notranslate nohighlight">
\[\boxed{L_c(y|q) = y(1-c) 1_{q \leq c} + (1-y) c 1_{q&gt;c}}\]</div>
<p>be a <strong>cost-weighted misclassification error</strong>: it penalizes false positives with cost <span class="math notranslate nohighlight">\(c\)</span> and false negatives with cost <span class="math notranslate nohighlight">\(1-c\)</span>.</p>
<p>Then, any proper scoring rule can be written as</p>
<div class="math notranslate nohighlight">
\[\boxed{L(y|p) = \int_0^1 L_c(y|p) \,\omega(c) dc}\]</div>
<p><strong>This result is extremely interesting</strong>. It is a <strong>unified vision</strong> of all the different loss functions we saw so far: if we consider the simplest loss function, which penalizes both false positive and negatives, and integrate it against a weight function <span class="math notranslate nohighlight">\(\omega(c)\)</span> for all possible costs <span class="math notranslate nohighlight">\(c\)</span>, we arrive at a loss function which is a proper scoring rule!</p>
<p>In fact, it shows that the degree of freedom in choosing a proper scoring rule <strong>lies in the choice of the weight function</strong>, period.</p>
<blockquote>
<div><p>Exercise: starting from <span class="math notranslate nohighlight">\(\omega(c) = [c(1-c)]^{-1}\)</span>, obtain the log-loss.</p>
</div></blockquote>
</section>
<section id="the-beta-family-of-proper-scoring-rules">
<h3>The Beta family of proper scoring rules<a class="headerlink" href="#the-beta-family-of-proper-scoring-rules" title="Permalink to this headline">#</a></h3>
<p>In the discussion above, we saw a unified approach to creating loss functions which are proper scoring rules. We can go one step beyond, and show that they all (except the focal loss) <em>belong in the the same family of functions!</em></p>
<p>Recall that</p>
<div class="math notranslate nohighlight">
\[\omega_\mathrm{log-loss}(q) = \frac{1}{q(1-q)}\]</div>
<p>For the exponential loss,</p>
<div class="math notranslate nohighlight">
\[L(y|q) = y \sqrt{\frac{1-q}{q}} + (1-y) \sqrt{\frac{q}{1-q}}\]</div>
<p>we have an actually similar weight function</p>
<div class="math notranslate nohighlight">
\[\omega_\mathrm{exp}(q) = \left(\frac{1}{q(1-q)}\right)^{3/2}\]</div>
<p>We can actually introduce a parametric family of functions which include all these as well as the Brier score and misclassification loss: the <strong>Beta family</strong></p>
<div class="math notranslate nohighlight">
\[\boxed{\omega_{\alpha,\beta}(q) := q^{\alpha-1} (1-q)^{\beta-1}}\]</div>
<p>where the name is obviously motivated by the <a class="reference external" href="https://en.wikipedia.org/wiki/Beta_function">Beta function</a>.</p>
<p>Then:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha=\beta=-1/2\)</span>: exponential loss</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha=\beta=0\)</span>: log-loss</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha=\beta=1\)</span>: Brier (squared error) loss</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha=\beta\to\infty\)</span>: misclassification loss (weak convergence to delta mass)</p></li>
</ul>
<p>Also, any values of <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> which are multiples of <span class="math notranslate nohighlight">\(1/2\)</span> yield closed-form formulas for <span class="math notranslate nohighlight">\(\omega\)</span>. For example, if <span class="math notranslate nohighlight">\(\alpha=\beta=1/2\)</span>, we get an intermediate between log-loss and squared error loss:
$<span class="math notranslate nohighlight">\(L(y|q) = y (\arcsin\sqrt{1-q} - \sqrt{q(1-q)}) + (1-y) (\arcsin\sqrt{q} - \sqrt{q(1-q)})\)</span>$</p>
<p>In general, if we define the (non-regularized) <strong>incomplete Beta function</strong></p>
<div class="math notranslate nohighlight">
\[B(x; a, b) := \int_0^x t^{a-1} (1-t)^{b-1} dt\]</div>
<p>then we can explicitly write the loss functions for the Beta family as</p>
<div class="math notranslate nohighlight">
\[\boxed{L(y|q) = y B(1-q; \beta+1, \alpha) + (1-y) B(q; \alpha+1,\beta)}\]</div>
</section>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="references">
<h1>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h1>
<p>[1] Andreas Buja, Werner Stuetzle, Yi Shen. <em>Loss Functions for Binary Class Probability Estimation and Classification: Structure and Applications</em> (2015)</p>
<p>[2] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár. <em>Focal Loss for Dense Object Detection</em>. Available on <a class="reference external" href="https://arxiv.org/abs/1708.02002">https://arxiv.org/abs/1708.02002</a></p>
<p>[3] Chen Wang and Chengyuan Deng and Suzhen Wang, <em>Imbalance-XGBoost: Leveraging Weighted and Focal Losses for Binary Label-Imbalanced Classification with XGBoost</em>. Available in <a class="reference external" href="https://arxiv.org/abs/1908.01672">https://arxiv.org/abs/1908.01672</a> (2019)</p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="Introduction.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Introduction</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Metrics%201%20-%20Intro%20%26%20ROC%20AUC.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Classification metrics</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Experian LatAm DataLab<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>