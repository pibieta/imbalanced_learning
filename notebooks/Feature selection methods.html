
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Feature selection via the Boruta algorithm &#8212; Imbalanced Binary Classification - A survey with code</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="How imbalanced learning affects the interpretability of a model" href="Interpretability.html" />
    <link rel="prev" title="Choosing a baseline model" href="pablo-baseline-experiment.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/DataLab-logo-white.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Imbalanced Binary Classification - A survey with code</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="Introduction.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Loss%20functions.html">
   Loss functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%201%20-%20Intro%20%26%20ROC%20AUC.html">
   Classification metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%202%20-%20Lift%20curve.html">
   A complement to the ROC: the lift curve (aka CAP curve)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%203%20-%20KS%20score.html">
   The KS score and Youden’s J
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%204%20-%20Precision%20and%20Recall.html">
   Precision, recall, and
   <span class="math notranslate nohighlight">
    \(F\)
   </span>
   -scores
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pablo-baseline-experiment.html">
   Choosing a baseline model
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Feature selection via the Boruta algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Interpretability.html">
   How imbalanced learning affects the interpretability of a model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Calibration.html">
   Model calibration part I - reliability assessment
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/notebooks/Feature selection methods.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/notebooks/Feature selection methods.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-much-is-too-much">
   How much is too much?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-are-the-trade-offs-here">
     What are the trade-offs here?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-boruta-algorithm">
   The Boruta algorithm
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tl-dr">
     TL;DR:
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#more-details">
     More details
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-dive-into-boruta-experiments">
   Deep dive into Boruta: experiments
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-analysis-for-perc-80-baseline-randomforest-model">
     Performance analysis for
     <code class="docutils literal notranslate">
      <span class="pre">
       perc=80
      </span>
     </code>
     , baseline RandomForest model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-boruta-on-imbalanced-datasets">
   Using Boruta on imbalanced datasets
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conclusion-1-pure-undersampling-is-consistently-worse-than-using-weights">
     Conclusion 1: pure undersampling is consistently worse than using weights
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conclusion-2-oversampling-is-too-loose-and-slow">
     Conclusion 2: oversampling is too loose (and slow)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-about-extreme-imbalance">
     What about extreme imbalance?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#another-approach-using-shap-values">
   Another approach: using SHAP values
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#some-acknowledgements">
   Some acknowledgements
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusions">
   Conclusions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Feature selection via the Boruta algorithm</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-much-is-too-much">
   How much is too much?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-are-the-trade-offs-here">
     What are the trade-offs here?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-boruta-algorithm">
   The Boruta algorithm
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tl-dr">
     TL;DR:
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#more-details">
     More details
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-dive-into-boruta-experiments">
   Deep dive into Boruta: experiments
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-analysis-for-perc-80-baseline-randomforest-model">
     Performance analysis for
     <code class="docutils literal notranslate">
      <span class="pre">
       perc=80
      </span>
     </code>
     , baseline RandomForest model
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#using-boruta-on-imbalanced-datasets">
   Using Boruta on imbalanced datasets
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conclusion-1-pure-undersampling-is-consistently-worse-than-using-weights">
     Conclusion 1: pure undersampling is consistently worse than using weights
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conclusion-2-oversampling-is-too-loose-and-slow">
     Conclusion 2: oversampling is too loose (and slow)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-about-extreme-imbalance">
     What about extreme imbalance?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#another-approach-using-shap-values">
   Another approach: using SHAP values
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#some-acknowledgements">
   Some acknowledgements
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusions">
   Conclusions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="feature-selection-via-the-boruta-algorithm">
<h1>Feature selection via the Boruta algorithm<a class="headerlink" href="#feature-selection-via-the-boruta-algorithm" title="Permalink to this headline">#</a></h1>
<p>A common theme in machine learning is selecting good features in other to maximize model performance.</p>
<p>For structured datasets, there are usually two steps in order to choose a final set of features:</p>
<ol class="simple">
<li><p><strong>Feature engineering</strong>: creating new features from data (eg. from unit price and total volume, maybe create a total revenue feature, equal to price <span class="math notranslate nohighlight">\(\times\)</span> volume)</p></li>
<li><p><strong>Feature selection</strong>: from a set of <span class="math notranslate nohighlight">\(p\)</span> features, select a subset which keeps (or even improves) performance</p></li>
</ol>
<p>In order to accomplish these, there exist both <strong>technical</strong> and <strong>business</strong> considerations. We won’t cover the latter: these are usually application-specific and, most importantly, heavily dependent on the data you have at hand.</p>
<p>Therefore, we will stick to technical methods to perform feature selection: given a set of features <span class="math notranslate nohighlight">\(x_1, \ldots, x_p\)</span>, can we select a subset <span class="math notranslate nohighlight">\(x_1',\ldots, x_m'\)</span> (where <span class="math notranslate nohighlight">\(m\)</span> here stands for “minimal”) which lead us to an optimal level of model performance?</p>
<blockquote>
<div><p>For the remainder of this section, <span class="math notranslate nohighlight">\(N\)</span> will denote the amount of samples (ie. we are given a dataset <span class="math notranslate nohighlight">\(\{x_i, y_i\}_{i\in 1:N}\)</span>) and <span class="math notranslate nohighlight">\(p\)</span> will denote the original amount of features (that is, each <span class="math notranslate nohighlight">\(x_i\)</span> is a <span class="math notranslate nohighlight">\(p\)</span>-dimensional vector).</p>
</div></blockquote>
<section id="how-much-is-too-much">
<h2>How much is too much?<a class="headerlink" href="#how-much-is-too-much" title="Permalink to this headline">#</a></h2>
<p>One (old) common answer is that <span class="math notranslate nohighlight">\(p &gt; N\)</span> means too many features. This was always the case in traditional statistics, for methods such as linear or logistic regression for which <span class="math notranslate nohighlight">\(p &gt; N\)</span> literally means that the algorithm breaks down (see <a class="reference external" href="https://stats.stackexchange.com/questions/282663/why-is-n-p-a-problem-for-ols-regression">this post for example</a>), since the <span class="math notranslate nohighlight">\(X^T X\)</span> matrix is no longer invertible.</p>
<p>This is no longer a satisfactory answer. Indeed, it is common in genomics, for example, to have datasets where <span class="math notranslate nohighlight">\(p \gg N\)</span>: gene expression data usually considers <span class="math notranslate nohighlight">\(p\)</span> on the order of 10,000 or even 100,000 thousand, whereas <span class="math notranslate nohighlight">\(N\)</span> might be around a few hundred (one for each patient in the study, for example).</p>
<p>The same can be seen in problems like image processing, where each pixel contributes with <span class="math notranslate nohighlight">\(O(1)\)</span> features. As image sizes get larger, total feature count scales quadratically.</p>
<p>For credit, in our experience, it is common to have datasets with <span class="math notranslate nohighlight">\(p\)</span> between 100-10,000 features pertaining to an individual’s financial, behavioral and demographical information. <span class="math notranslate nohighlight">\(N\)</span> here will usually be on the order of 100,000 - 100 million, ie. some percentage of the population (we are considering the Brazilian case with total population around 200 million).</p>
<p>Let us focus on the credit example. Do we <em>need</em> 5000 features to predict an individual’s credit behaviour? There is no reason why that would not be the case; however, experience shows that we can get very good predictions with values between 20-400 features (ie. ~0.5% to 10% of the total features). This is a data-driven insight, since a Pareto-like rule tends to apply to feature importance measures.</p>
<p><strong>There are always useless features</strong>. In the genomics case, not all genes are active at a given moment - only a fraction of them contribute to whatever phenomenon is of interest. In a 800x600 image of a cat, only a small portion of pixels will indeed describe the cat, with the rest being objects which are not of interest. Similarly, as rich information as an individual’s features might be, only some will contribute to their credit behavior.</p>
<p>Our goal is then to find <strong>systematic ways to filter out useless features</strong>.</p>
<section id="what-are-the-trade-offs-here">
<h3>What are the trade-offs here?<a class="headerlink" href="#what-are-the-trade-offs-here" title="Permalink to this headline">#</a></h3>
<p>It is common sense that, if you have too few features, your model might simply not have enough information to perform well.</p>
<p>Less obvious is that too many features can also be problematic. They might cause loss of performance due to a few related reasons:</p>
<ul class="simple">
<li><p>Overfitting: the more features, the more difficult will it be for points to have close neighbors (the so-called curse of dimensionality); you will need exponentially more data to cover the feature space meaningfully. Your algorithm is prone to just overfitting;</p></li>
<li><p>Noise: useless variables introduce noise which can affect training;</p></li>
<li><p>Time/space considerations: the more dimensions, the more memory it takes on your computer, and the longer it will take for training, hyperparameter optimization etc.</p></li>
</ul>
</section>
</section>
<section id="the-boruta-algorithm">
<h2>The Boruta algorithm<a class="headerlink" href="#the-boruta-algorithm" title="Permalink to this headline">#</a></h2>
<p>One of our favorite methods for feature selection is the Boruta algorithm, introduced in 2010 by Kursa and Rudnicki [1]. It has consistently proven itself as a powerful tool for straightforward selection of good features in the case of thousands of features.</p>
<blockquote>
<div><p>A nice pedagogical overview of the method by the original author can be seen <a class="reference external" href="https://cran.r-project.org/web/packages/Boruta/vignettes/inahurry.pdf">here</a>; the Python implementation (BorutaPy) can be seen on <a class="reference external" href="https://github.com/scikit-learn-contrib/boruta_py">GitHub</a></p>
</div></blockquote>
<p>Simply stated, Boruta works as follows. For each feature, say <code class="docutils literal notranslate"><span class="pre">x1</span></code>, Boruta creates a copy <code class="docutils literal notranslate"><span class="pre">x1_copy</span></code> (called a <em>shadow</em> by the authors) and then randomly mixes the values across all points, creating noise.</p>
<p>It then fits a model (usually a random forest) implementing a feature importance method, and analyses how the original feature’s importance compares to the noisy copies. If they are significantly different, then <code class="docutils literal notranslate"><span class="pre">x1</span></code> is deemed valuable, and kept; if they are not, it means <code class="docutils literal notranslate"><span class="pre">x1</span></code> itself is basically noise, and it is removed.</p>
<section id="tl-dr">
<h3>TL;DR:<a class="headerlink" href="#tl-dr" title="Permalink to this headline">#</a></h3>
<blockquote>
<div><p><strong>Use Boruta with a fixed-depth RandomForest base model, with <code class="docutils literal notranslate"><span class="pre">perc</span></code> parameter set to 80</strong></p>
</div></blockquote>
<blockquote>
<div><p>In the case of  class imbalance, <strong>do not use upsampling</strong>; instead, cross-validate an <strong>undersampling</strong> of the majority class during feature selection. For Boruta, use a <strong>Random Forest base classifier with weights</strong>: <code class="docutils literal notranslate"><span class="pre">class_weight='balanced_subsample'</span></code>, and for the final model (trained on the whole training set using the selected features), see whether using <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> gives a better result than not using it.</p>
</div></blockquote>
<p>The <code class="docutils literal notranslate"><span class="pre">perc</span></code> parameter is an extremely important parameter introduced in the Python version. It is basically how “loose” we want to be with our features: <code class="docutils literal notranslate"><span class="pre">perc=100</span></code> is the most rigorous, and the closer to 0 it gets the looser we are with letting less important features get selected.</p>
</section>
<section id="more-details">
<h3>More details<a class="headerlink" href="#more-details" title="Permalink to this headline">#</a></h3>
<p>We will use Boruta with random forests.</p>
<p>As noted by the author itself (in <em>Boruta for those in a hurry</em>, Miron B. Kursa, May 21 2020), it is important that we have enough trees:</p>
<blockquote>
<div><p>For datasets with lots of features, the default configuration of the importance source is likely insufficient; in the particular case of Random Forest the number of trees is often not large enough to allow the importance scores to stabilise, which in turn often leads to false negatives and unstable results.</p>
</div></blockquote>
<p>This can be taken care of by letting Boruta itself identify an optimal number of trees (as we do below).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="kn">from</span> <span class="nn">boruta.boruta_py</span> <span class="kn">import</span> <span class="n">BorutaPy</span>
</pre></div>
</div>
</div>
</div>
<p>Create a balanced classification problem with 30 features, out of which:</p>
<ul class="simple">
<li><p>6 are informative;</p></li>
<li><p>4 are redundant linear combinations of the informative ones;</p></li>
<li><p>5 are just repeated values;</p></li>
<li><p>The remaining 15 are random noise.</p></li>
</ul>
<p>To all features, we add some Gaussian noise to avoid perfect collinearity.</p>
<p>We will name the columns accordingly, and see which are kept by Boruta and which are taken out.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> 
                           <span class="n">n_features</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">n_repeated</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                           <span class="n">weights</span><span class="o">=</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,),</span>
                           <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">X</span> <span class="o">+=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;informative_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">)]</span> <span class="o">+</span> \
          <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;redundant_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">)]</span> <span class="o">+</span> \
          <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;repeated_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">)]</span> <span class="o">+</span> \
          <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;noise_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">15</span><span class="p">)]</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">select_feature_boruta</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> 
                         <span class="n">perc</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                         <span class="n">alpha</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                         <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                         <span class="n">max_depth</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                         <span class="n">n_estimators</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span>
                         <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>

    <span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
    <span class="kn">from</span> <span class="nn">boruta.boruta_py</span> <span class="kn">import</span> <span class="n">BorutaPy</span>
    
    <span class="n">X_is_df</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">)</span>
    <span class="n">y_is_df</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">)</span>
    
    <span class="n">selector</span> <span class="o">=</span> <span class="n">BorutaPy</span><span class="p">(</span>
        <span class="n">estimator</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">),</span>
        <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
        <span class="n">perc</span><span class="o">=</span><span class="n">perc</span><span class="p">,</span>      
        <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>    
        <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># boruta needs a numpy array, not a dataframe</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">values</span> <span class="k">if</span> <span class="n">X_is_df</span> <span class="k">else</span> <span class="n">X</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">values</span> <span class="k">if</span> <span class="n">y_is_df</span> <span class="k">else</span> <span class="n">y</span>

    <span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> 
    
    <span class="k">if</span> <span class="n">X_is_df</span><span class="p">:</span>
        <span class="n">columns</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span>
        <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">columns</span><span class="p">)[</span><span class="n">selector</span><span class="o">.</span><span class="n">support_</span><span class="o">.</span><span class="n">tolist</span><span class="p">()])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">selector</span><span class="o">.</span><span class="n">support_</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>Let us print the features which survive the feature selection process:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">selected_features</span> <span class="o">=</span> <span class="n">select_feature_boruta</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">selected_features</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 34.3 s, sys: 671 ms, total: 34.9 s
Wall time: 6.47 s
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;informative_0&#39;,
 &#39;informative_1&#39;,
 &#39;informative_2&#39;,
 &#39;informative_3&#39;,
 &#39;informative_4&#39;,
 &#39;informative_5&#39;,
 &#39;redundant_0&#39;,
 &#39;redundant_1&#39;,
 &#39;redundant_2&#39;,
 &#39;redundant_3&#39;,
 &#39;repeated_0&#39;,
 &#39;repeated_1&#39;,
 &#39;repeated_2&#39;,
 &#39;repeated_3&#39;,
 &#39;repeated_4&#39;]
</pre></div>
</div>
</div>
</div>
<p><em>All</em> non-random features survived in the end! This is expected: Boruta is <em>not meant</em> to remove repeated or redundant features. Since these features appear as having significant feature importance, it will keep them; it is our job as data scientists to remove collinearity. We can use the code below to do that first, then run Boruta:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">drop_high_correlations</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="mf">0.95</span><span class="p">):</span>
    <span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>

    <span class="n">upper</span> <span class="o">=</span> <span class="n">corr_matrix</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">corr_matrix</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">))</span>
    <span class="n">to_drop</span> <span class="o">=</span> <span class="p">[</span><span class="n">column</span> <span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">upper</span><span class="o">.</span><span class="n">columns</span> <span class="k">if</span> <span class="nb">any</span><span class="p">(</span><span class="n">upper</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">thresh</span><span class="p">)]</span>

    <span class="k">return</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">to_drop</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_uncor</span> <span class="o">=</span> <span class="n">drop_high_correlations</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">thresh</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">selected_features</span> <span class="o">=</span> <span class="n">select_feature_boruta</span><span class="p">(</span><span class="n">X_train_uncor</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">selected_features</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 30.9 s, sys: 719 ms, total: 31.6 s
Wall time: 5.54 s
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;informative_0&#39;,
 &#39;informative_1&#39;,
 &#39;informative_2&#39;,
 &#39;informative_3&#39;,
 &#39;informative_4&#39;,
 &#39;informative_5&#39;,
 &#39;redundant_0&#39;,
 &#39;redundant_1&#39;,
 &#39;redundant_2&#39;,
 &#39;redundant_3&#39;,
 &#39;repeated_1&#39;,
 &#39;repeated_4&#39;]
</pre></div>
</div>
</div>
</div>
<p>With a threshold of 0.9, this approach removes some of the repeated features, as we would have liked.</p>
<p>Boruta has some free parameters which might be useful. For example, the <code class="docutils literal notranslate"><span class="pre">perc</span></code> parameter determines how strict we want to be when comparing real features with shadow features. If we choose a lower <code class="docutils literal notranslate"><span class="pre">perc</span></code>, more features could survive. In practice, try studying <code class="docutils literal notranslate"><span class="pre">perc</span></code> between 90 and 100.</p>
</section>
</section>
<section id="deep-dive-into-boruta-experiments">
<h2>Deep dive into Boruta: experiments<a class="headerlink" href="#deep-dive-into-boruta-experiments" title="Permalink to this headline">#</a></h2>
<p>A common question among Boruta users is:</p>
<ul class="simple">
<li><p>What base model should I use?</p></li>
<li><p>How much should we set the <code class="docutils literal notranslate"><span class="pre">perc</span></code> parameter?</p></li>
<li><p>How much can we trust Boruta’s output?</p></li>
</ul>
<p>Below, we show the results for a series of experiments. The database used was generated via the <code class="docutils literal notranslate"><span class="pre">make_classification</span></code> method in Scikit-Learn, with a fixed number of 5000 datapoints.</p>
<p>For each iteration, we create a data set with a certain number of features. Out of these, only 40% will be meaningful, and the other 60% will be noise.</p>
<p>We then train three models:</p>
<ol class="simple">
<li><p>One only using the “good” features (ie the 40% of features that we know are meaningful);</p></li>
<li><p>One using all the features;</p></li>
<li><p>One using the features selected by the Boruta algorithm</p></li>
</ol>
<p>We repeat this experiment for gradually increasing number of total features, from 20 to 2000 (ie. from 0.4% to 40% of the total number of data points).</p>
<p>Below, you see the results. The first picture shows the value of the AUCs of the models <em>relative to the model described in (1) above</em>, ie the model with the good features. The idea here is to see how much performance models 2 and 3 can achieve compared to an “oracle” which knows which features are good.</p>
<section id="performance-analysis-for-perc-80-baseline-randomforest-model">
<h3>Performance analysis for <code class="docutils literal notranslate"><span class="pre">perc=80</span></code>, baseline RandomForest model<a class="headerlink" href="#performance-analysis-for-perc-80-baseline-randomforest-model" title="Permalink to this headline">#</a></h3>
<p><img alt="image.png" src="notebooks/attachment:image.png" /></p>
<p>We see that, regardless of the total amount of features, the <strong>model trained with the Boruta-selected features outperforms the one trained on all variables</strong>. This is promising: in real life, we have no oracle, and can only hope that our feature selection algorithm chooses features in such a way as to beat the model trained on all features.</p>
<blockquote>
<div><p>There is a slight overfit (where the green line goes higher than the orange one) for some values, but this is fine.</p>
</div></blockquote>
<p>We can also look at how many of the “good” features were captured by Boruta in each case:
<img alt="image.png" src="notebooks/attachment:image.png" /></p>
<p>As we see, the more features in the dataset, the less Boruta is able to capture them all. With these settings, it plateaus at around 300 features. Despite that, it was able to perform better than a model which had all features available to them.</p>
<p>To see how important the <code class="docutils literal notranslate"><span class="pre">perc</span></code> parameter is, compare the results above with the same ones for <code class="docutils literal notranslate"><span class="pre">perc=100</span></code>:</p>
<p><img alt="image.png" src="notebooks/attachment:image.png" /></p>
<p>The situation is now inverted: Boruta is not able to capture enough features, and the model with selected features loses to the one using the full feature set.</p>
<p>Below, we show a consolidated result. In the x-axis we have <code class="docutils literal notranslate"><span class="pre">perc</span></code>; in the y-axis we have the AUC difference between a model with Boruta-selected features and one trained on the full feature set (normalized by the AUC of the model trained only with the good features). A positive value will mean that the Boruta model is better.</p>
<p>We do this across many feature counts. As we see, for a higher number of total features, Boruta consistently performs better than the full model if <code class="docutils literal notranslate"><span class="pre">perc</span></code> is set to values lower than 90, with the overall best between 70 and 80.</p>
<p><img alt="image.png" src="notebooks/attachment:image.png" /></p>
<p>We can run a similar analysis using LightGBM as the base model:
<img alt="image.png" src="notebooks/attachment:image.png" /></p>
<p>The qualitative conclusion is the same: <code class="docutils literal notranslate"><span class="pre">perc</span></code> must be set to a smaller value, and the difference between Boruta and the model trained on the full features grows as the total amount of features increases.</p>
</section>
</section>
<hr class="docutils" />
<section id="using-boruta-on-imbalanced-datasets">
<h2>Using Boruta on imbalanced datasets<a class="headerlink" href="#using-boruta-on-imbalanced-datasets" title="Permalink to this headline">#</a></h2>
<p>The main question we want to answer is: is Boruta trustworthy when we are dealing with imbalanced datasets?</p>
<p>The approaches to this question mostly boil down to two:</p>
<ol class="simple">
<li><p><em>Undersample (1a)</em> or <em>oversample (1b)</em> the set we use to run the Boruta process;</p></li>
<li><p>Smartly use <em>weights</em> on the models we are using. If you have too much data on your original dataset, some undersampling, followed by using weights, also works.</p></li>
</ol>
<p>Our key takeaway is translated into the following receipt:</p>
<ul class="simple">
<li><p><strong>Don’t use Boruta in case of extreme imbalance</strong>. Here, by extreme imbalance, we mean imbalance ratios &lt; 1% and/or where there are less than a few hundred samples from the minority class. There are two reasons for this:</p>
<ol class="simple">
<li><p>As we saw in the section about feature importances &amp; interpretability, feature importance estimations become noisy as the absolute amount of instances of the minority class becomes small. Although not necessarily always true, there is a large correlation between having extreme imbalance and simply having very few data points. Since Boruta depends on the feature importance estimates of its base learner, it will be confused by the noisy estimates and won’t provide a good set of features;</p></li>
<li><p>As the model becomes very imbalance, it also correlates with having a very large dataset. Boruta scales badly with dataset size, and using it becomes infeasible.</p></li>
</ol>
</li>
<li><p>If you are, however, within the parameters where using Boruta makes sense:</p>
<ol class="simple">
<li><p><strong>Use weights</strong> via <code class="docutils literal notranslate"><span class="pre">class_weight='balanced_subsample'</span></code> in the Random Forests used as the base learner in Boruta;</p></li>
<li><p>If you have too much data initially, adding <strong>some undersampling first, then running item (1)</strong>, is both more computationally efficient and effective. In the DataLab, we have verified that undersampling so that the total number of points is around <strong>15 times the total amount of features</strong> tends to give good results;</p></li>
<li><p>With the selected features, train a model - here, we use  a LightGBM model with <code class="docutils literal notranslate"><span class="pre">class_weight='balanced'</span></code>.</p></li>
</ol>
</li>
</ul>
<p>Below, we show a few results of experiments for a dataset with imbalance ratio of 2%. We compare the undersampled, oversampled and  no-sampling, with-weights approaches.</p>
<p><img alt="image.png" src="notebooks/attachment:image.png" /></p>
<section id="conclusion-1-pure-undersampling-is-consistently-worse-than-using-weights">
<h3>Conclusion 1: pure undersampling is consistently worse than using weights<a class="headerlink" href="#conclusion-1-pure-undersampling-is-consistently-worse-than-using-weights" title="Permalink to this headline">#</a></h3>
<p>Dataset 1: <strong>2% imbalance ratio</strong></p>
<ul class="simple">
<li><p>4867 instances of class 0</p></li>
<li><p>133 instances of class 1</p></li>
</ul>
<p>Below, we see the results of undersampling the majority class as we increase the target balance.</p>
<ul class="simple">
<li><p>In the first plot, the Y-axis is ROC AUC. The second and third plots show, respectively, how many of the good features were captured and how many of the bad features were captured by Boruta. The orange line shows the AUC for a model containing all the good features, which we know a priori; the green line shows the performance of a model trained with all features; and the blue line shows the performance of a model trained only using the features selected via Boruta.</p></li>
<li><p>Also, the dashed red line show the performance for a model trained without sampling, but simply using <code class="docutils literal notranslate"><span class="pre">class_weight='balanced_subsample'</span></code> in the Boruta base learner (Random Forest) and then <code class="docutils literal notranslate"><span class="pre">class_weight='balanced'</span></code> in the final LightGBM model. Just as before, notice there is a slight fitting to noise (surpassing the orange model, with all good features).</p></li>
</ul>
<p><img alt="image.png" src="notebooks/attachment:image.png" /></p>
<p>Notice how we capture more of the good features  with a model with <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> than with an undersampled one; similarly, we avoid more features with it.</p>
</section>
<section id="conclusion-2-oversampling-is-too-loose-and-slow">
<h3>Conclusion 2: oversampling is too loose (and slow)<a class="headerlink" href="#conclusion-2-oversampling-is-too-loose-and-slow" title="Permalink to this headline">#</a></h3>
<p><img alt="image.png" src="notebooks/attachment:image.png" /></p>
<p>We can see that, as we increase the target balance, oversampling quickly becomes too loose and lets all features in, hence making models which are equivalent with just training with all features (also, since it increases the total amount of points, it makes Boruta slower).</p>
</section>
<section id="what-about-extreme-imbalance">
<h3>What about extreme imbalance?<a class="headerlink" href="#what-about-extreme-imbalance" title="Permalink to this headline">#</a></h3>
<p>To be fair, we still don’t know. We have run several experiments here, but not conclusive. We are open to feedbacks here!</p>
</section>
</section>
<hr class="docutils" />
<section id="another-approach-using-shap-values">
<h2>Another approach: using SHAP values<a class="headerlink" href="#another-approach-using-shap-values" title="Permalink to this headline">#</a></h2>
<p>A common approach is to calculate SHAP values and then remove the features with total contribution less than a certain threshold. This has some ambiguity on the threshold definition, and also needs to be done iteratively - after removing features, one must retrain and see the new SHAP values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">shap</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingClassifier</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>HistGradientBoostingClassifier(max_depth=5, random_state=2)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">max_display</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Feature selection methods_69_0.png" src="../_images/Feature selection methods_69_0.png" />
</div>
</div>
<p>We see that SHAP does a good job of finding that the noise features don’t add value. Here, clearly a threshold of <code class="docutils literal notranslate"><span class="pre">mean(|SHAP|)</span> <span class="pre">&gt;</span> <span class="pre">0.05</span></code> seems to be a good feature selector, but this is something we have to identify visually.</p>
<p>We can make this process into a function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">shap_importances</span><span class="p">(</span><span class="n">columns</span><span class="p">,</span> <span class="n">shap_values</span><span class="p">):</span>
    <span class="n">importances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">shap_values</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># same calculation as the bar plot </span>
    <span class="n">feats_imps</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;feature&#39;</span><span class="p">:</span> <span class="n">columns</span><span class="p">,</span> 
                               <span class="s1">&#39;shap&#39;</span><span class="p">:</span> <span class="n">importances</span><span class="p">})</span>
    
    <span class="k">return</span> <span class="n">feats_imps</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s1">&#39;shap&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_important_features_from_shap</span><span class="p">(</span><span class="n">importances</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.04</span><span class="p">):</span>
    <span class="n">imps</span> <span class="o">=</span> <span class="n">importances</span><span class="p">[</span><span class="n">importances</span><span class="p">[</span><span class="s1">&#39;shap&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">]</span>
    <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">imps</span><span class="p">[</span><span class="s1">&#39;feature&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">importances</span> <span class="o">=</span> <span class="n">shap_importances</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">shap_values</span><span class="p">)</span>
<span class="n">importances</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature</th>
      <th>shap</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>informative_0</td>
      <td>0.790061</td>
    </tr>
    <tr>
      <th>7</th>
      <td>redundant_1</td>
      <td>0.751999</td>
    </tr>
    <tr>
      <th>13</th>
      <td>repeated_3</td>
      <td>0.627386</td>
    </tr>
    <tr>
      <th>8</th>
      <td>redundant_2</td>
      <td>0.503884</td>
    </tr>
    <tr>
      <th>11</th>
      <td>repeated_1</td>
      <td>0.411533</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">features_selected_shap</span> <span class="o">=</span> <span class="n">get_important_features_from_shap</span><span class="p">(</span><span class="n">importances</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Rinse and repeat:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">features_selected_shap</span><span class="p">],</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">explainer</span> <span class="o">=</span> <span class="n">shap</span><span class="o">.</span><span class="n">TreeExplainer</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">shap_values</span> <span class="o">=</span> <span class="n">explainer</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">features_selected_shap</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">shap</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">shap_values</span><span class="p">,</span> <span class="n">max_display</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">show</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Feature selection methods_77_0.png" src="../_images/Feature selection methods_77_0.png" />
</div>
</div>
<p>There has been some rearrangement, but overall this process works. No noise features are present, but the repeated / redundant features have remained.</p>
</section>
<section id="some-acknowledgements">
<h2>Some acknowledgements<a class="headerlink" href="#some-acknowledgements" title="Permalink to this headline">#</a></h2>
<p>One interesting approach is to use a mix of Boruta and SHAP values: instead of using the feature importances calculated during the model training, we would instead use SHAP values.</p>
<blockquote>
<div><p>In tree-based classification models, splits are calculated based on Gini impurity or infomation gain. A split into new branches happens when a feature is found to decrease the total impurity. Feature importance for a given feature in a decision tree is then how much that feature contributed to the total impurity reduction. For a random forest, the feature importance attribute is the average over all trees in the forest. More details on <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html">the scikit-learn documentation</a>.</p>
</div></blockquote>
<p>The <a class="reference external" href="https://github.com/Ekeany/Boruta-Shap">Boruta-SHAP library</a> [2] does exactly that. Below, we see how to use it.</p>
<blockquote>
<div><p>Because SHAP value calculation is an expensive operation, this process might take much longer than the equivalent with standard feature importances.</p>
</div></blockquote>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">BorutaShap</span> <span class="kn">import</span> <span class="n">BorutaShap</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="c1"># important to pass the specific model to use; otherwise it can take a long time</span>
<span class="n">selector</span> <span class="o">=</span> <span class="n">BorutaShap</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                      <span class="n">percentile</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                      <span class="n">pvalue</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
                      <span class="n">importance_measure</span><span class="o">=</span><span class="s1">&#39;shap&#39;</span><span class="p">,</span>
                      <span class="n">classification</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
             <span class="n">n_trials</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">sample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
             <span class="n">train_or_test</span><span class="o">=</span><span class="s1">&#39;test&#39;</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
             <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "a0b313b2e0f74d2eb957a80a7cec5bc6", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>15 attributes confirmed important: [&#39;informative_3&#39;, &#39;repeated_1&#39;, &#39;repeated_3&#39;, &#39;informative_1&#39;, &#39;informative_5&#39;, &#39;repeated_2&#39;, &#39;informative_4&#39;, &#39;informative_0&#39;, &#39;redundant_3&#39;, &#39;redundant_0&#39;, &#39;redundant_1&#39;, &#39;redundant_2&#39;, &#39;repeated_4&#39;, &#39;repeated_0&#39;, &#39;informative_2&#39;]
15 attributes confirmed unimportant: [&#39;noise_0&#39;, &#39;noise_12&#39;, &#39;noise_8&#39;, &#39;noise_9&#39;, &#39;noise_2&#39;, &#39;noise_7&#39;, &#39;noise_3&#39;, &#39;noise_6&#39;, &#39;noise_13&#39;, &#39;noise_4&#39;, &#39;noise_1&#39;, &#39;noise_11&#39;, &#39;noise_14&#39;, &#39;noise_10&#39;, &#39;noise_5&#39;]
0 tentative attributes remains: []
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">selected_columns</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">selector</span><span class="o">.</span><span class="n">Subset</span><span class="p">()</span><span class="o">.</span><span class="n">columns</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">selected_columns</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;informative_0&#39;,
 &#39;informative_1&#39;,
 &#39;informative_2&#39;,
 &#39;informative_3&#39;,
 &#39;informative_4&#39;,
 &#39;informative_5&#39;,
 &#39;redundant_0&#39;,
 &#39;redundant_1&#39;,
 &#39;redundant_2&#39;,
 &#39;redundant_3&#39;,
 &#39;repeated_0&#39;,
 &#39;repeated_1&#39;,
 &#39;repeated_2&#39;,
 &#39;repeated_3&#39;,
 &#39;repeated_4&#39;]
</pre></div>
</div>
</div>
</div>
<p>It gets the right columns, but this is an extremely time-consuming algorithm. Hopefully it can be accelerated in the near future.</p>
</section>
<section id="conclusions">
<h2>Conclusions<a class="headerlink" href="#conclusions" title="Permalink to this headline">#</a></h2>
<p>We have presented one of our favorite feature selection methods, Boruta, as well as its limitations:</p>
<ul class="simple">
<li><p>It works well for balanced and imbalanced problems, but not for extreme imbalance (0.1% or under)</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">perc</span></code> parameter is key: setting it to around 80 leads to better results</p></li>
<li><p>We use baseline Random Forest models with fixed depth and <code class="docutils literal notranslate"><span class="pre">class_weight='balanced_subsample'</span></code>.</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<p>Virus infection dataset NCBI GSE73072
<a class="reference external" href="https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE73072">https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE73072</a></p>
<p>[1] Kursa, M. B., &amp; Rudnicki, W. R. (2010). Feature Selection with the Boruta Package. Journal of Statistical Software, 36(11), 1–13. <a class="reference external" href="https://doi.org/10.18637/jss.v036.i11">https://doi.org/10.18637/jss.v036.i11</a></p>
<p>[2] <a class="reference external" href="https://github.com/Ekeany/Boruta-Shap">https://github.com/Ekeany/Boruta-Shap</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="pablo-baseline-experiment.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Choosing a baseline model</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Interpretability.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">How imbalanced learning affects the interpretability of a model</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Experian LatAm DataLab<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>