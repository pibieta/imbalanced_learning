
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>What? &#8212; Imbalanced Binary Classification - A survey with code</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/DataLab-logo-white.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Imbalanced Binary Classification - A survey with code</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="Introduction.html">
                    Introduction
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Loss%20functions.html">
   Loss functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%201%20-%20Intro%20%26%20ROC%20AUC.html">
   Classification metrics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%202%20-%20Lift%20curve.html">
   The lift curve
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%203%20-%20KS%20score.html">
   The KS score and Youden’s J
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Metrics%204%20-%20Precision%20and%20Recall.html">
   Precision, recall, and
   <span class="math notranslate nohighlight">
    \(F\)
   </span>
   -scores
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pablo-baseline-experiment.html">
   Choosing a baseline model
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Feature%20selection%20methods.html">
   Feature selection via the Boruta algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Calibration.html">
   Probability calibration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Interpretability.html">
   Model interpretability: how is it affected by imbalance?
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/notebooks/pablo-best-benchmark-model.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/notebooks/pablo-best-benchmark-model.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   What?
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how">
     How?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#auxiliary-functions">
   Auxiliary Functions
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data">
   Data
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-comparison">
   Model comparison
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-impact">
     Performance Impact
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#next-steps">
     Next steps
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sample-size-effect">
     Sample Size effect
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-assessment-and-selection">
   Model Assessment and Selection
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#error-estimation">
     Error estimation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-error-is-a-bad-error-estimate">
       Training error is a bad error estimate!
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bootstrapp-error-estimation">
       Bootstrapp Error estimation
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>What?</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   What?
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how">
     How?
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imports">
   Imports
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#auxiliary-functions">
   Auxiliary Functions
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#data">
   Data
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-comparison">
   Model comparison
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-impact">
     Performance Impact
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#next-steps">
     Next steps
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sample-size-effect">
     Sample Size effect
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-assessment-and-selection">
   Model Assessment and Selection
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#error-estimation">
     Error estimation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-error-is-a-bad-error-estimate">
       Training error is a bad error estimate!
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bootstrapp-error-estimation">
       Bootstrapp Error estimation
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="what">
<h1>What?<a class="headerlink" href="#what" title="Permalink to this headline">#</a></h1>
<p>In this notebook we will discuss about some topics on model selection in the presence of imabalanced datasets. The discussion can be guided by the following driving questions:</p>
<ul class="simple">
<li><p>What is the effect of class imbalance on different classification algorithms?</p></li>
<li><p>Is class imbalance really a problem when there is enough data?</p></li>
<li><p>What is the best benchmark model to start with? What does “best” actually mean in this situation?</p></li>
<li><p>What can be done to mitigate the effect of imbalance on specific algorithms?</p></li>
</ul>
<section id="how">
<h2>How?<a class="headerlink" href="#how" title="Permalink to this headline">#</a></h2>
<p>We intend to analyze the behavior (using some classification metrics) of different classification models under various imbalance scenarios
At this point we won’t pay too much attention on the optimization of the models themselves. Rather, we will work with the model ‘as-is’ and compare them with the objective of finding the <em>best</em> benchmarking model.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="imports">
<h1>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># basics</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Dict</span>
<span class="kn">from</span> <span class="nn">tqdm.auto</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="c1"># from tqdm.notebook import trange, tqdm</span>

<span class="c1"># plotting</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="nn">px</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">matplotlib.animation</span> <span class="kn">import</span> <span class="n">FuncAnimation</span>


<span class="c1"># sklearn</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="auxiliary-functions">
<h1>Auxiliary Functions<a class="headerlink" href="#auxiliary-functions" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bootstrap_samples</span><span class="p">(</span><span class="n">y_true</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span> <span class="n">n_batches</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Get bootstrap samples</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    y_true : array_like, shape = [n_samples]</span>
<span class="sd">        true events labels</span>

<span class="sd">    y_pred : array_like, shape = [n_samples]</span>
<span class="sd">        events predictions</span>

<span class="sd">    n_batches : int, optinal</span>
<span class="sd">        number of bootstrap samples, default = 100</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>

<span class="sd">    preds_samples : array_like, shape = [n_samples, [n_batches, 2]]</span>
<span class="sd">        list of bootstrap samples from y_true and y_preds</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># y_true = pd.Series(y_true, name=&#39;y_true&#39;)</span>
    <span class="c1"># y_pred = pd.Series(y_pred, name=&#39;y_pred&#39;)</span>
    <span class="c1"># df_preds = pd.concat((y_true, y_pred), axis=1)</span>
    <span class="n">df_preds</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;y_true&#39;</span><span class="p">:</span> <span class="n">y_true</span><span class="p">,</span> <span class="s1">&#39;y_pred&#39;</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">})</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="n">preds_samples</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_batches</span><span class="p">):</span>

        <span class="n">df_preds_resample</span> <span class="o">=</span> <span class="n">df_preds</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">y_true_resample</span> <span class="o">=</span> <span class="n">df_preds_resample</span><span class="p">[</span><span class="s1">&#39;y_true&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
        <span class="n">y_pred_resample</span> <span class="o">=</span> <span class="n">df_preds_resample</span><span class="p">[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

        <span class="n">preds_samples</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s1">&#39;y_true&#39;</span><span class="p">:</span> <span class="n">y_true_resample</span><span class="p">,</span>
                              <span class="s1">&#39;y_pred&#39;</span><span class="p">:</span> <span class="n">y_pred_resample</span><span class="p">})</span>

    <span class="n">preds_samples</span> <span class="o">=</span> <span class="n">preds_samples</span>

    <span class="k">return</span> <span class="n">preds_samples</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">delinquency_curve</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span><span class="p">,</span> <span class="n">pointwise</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Delinquency curve.</span>

<span class="sd">    The delinquency curve is curve that shows the default rate in function of</span>
<span class="sd">    the approval rate. With this curve is possible to have a clear view of a</span>
<span class="sd">    credit operation and its characteristics for all possible scenarios.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    y_true : array, shape = [n_samples]</span>
<span class="sd">        Correct labels for given dataset.</span>

<span class="sd">    y_score : array, shape = [n_samples]</span>
<span class="sd">        Predict scores for the given dataset.</span>

<span class="sd">    pointwise : bool, optional</span>
<span class="sd">        boolean indicating whether to compute pointwise delinquency curve.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    approval_rate: array, shape = [n_samples+1] if pointwise=True else [21]</span>
<span class="sd">        Array containing the approval rates used to compute the default_rate</span>
<span class="sd">        curve.</span>
<span class="sd">    default_rate: array, shape = [n_samples+1] if pointwise=True else [21]</span>
<span class="sd">        Default rate values for the approval rates provided in approval_rate.</span>
<span class="sd">        default_rate[i] is the ratio of events registered in the best</span>
<span class="sd">        approval_rate[i]% of scores.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="n">y_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_score</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_score</span><span class="p">),</span> <span class="sa">F</span><span class="s2">&quot;y_true and y_score doesn&#39;t have &quot;</span> \
        <span class="sa">F</span><span class="s2">&quot;the same length. len(y_true) = &quot;</span> \
        <span class="sa">F</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span><span class="si">}</span><span class="s2">, len(y_score) = &quot;</span> \
        <span class="sa">F</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">y_score</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="n">indexsort</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">y_score</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y_approved</span> <span class="o">=</span> <span class="n">y_true</span><span class="p">[</span><span class="n">indexsort</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">pointwise</span><span class="p">:</span>
        <span class="n">list_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_approved</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">approval_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">list_index</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">list_index</span><span class="p">))</span>
        <span class="n">default_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">y_approved</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span><span class="o">/</span><span class="n">list_index</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">approval_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">21</span><span class="p">)</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>

        <span class="n">default_rate</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">y_approved</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">n</span><span class="p">))]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
                                 <span class="k">if</span> <span class="p">(</span><span class="n">y_approved</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">i</span> <span class="o">*</span> <span class="n">n</span><span class="p">))]</span>
                                     <span class="o">.</span><span class="n">size</span><span class="p">)</span>
                                 <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">approval_rate</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">approval_rate</span><span class="p">,</span> <span class="n">default_rate</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_metrics</span><span class="p">(</span><span class="n">y_true</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span> <span class="n">y_proba</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span> <span class="n">n_round</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Computes several classification metrics at once</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    y_true : array_like, shape = [n_samples]</span>
<span class="sd">        true events labels</span>

<span class="sd">    y_pred : array_like, shape = [n_samples]</span>
<span class="sd">        events predictions</span>

<span class="sd">    n_round : int, optional</span>
<span class="sd">        number o decimals to round gini and ks metrics</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    metrics: pd.DataFrame</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># ROC AUC</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_proba</span><span class="p">),</span> <span class="n">n_round</span><span class="p">)</span>
    <span class="c1"># GINI</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;gini&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="c1"># Average precision</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;avg_precision&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_proba</span><span class="p">),</span> <span class="n">n_round</span><span class="p">)</span>
    <span class="c1"># ROC Curve</span>
    <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds_roc</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span> <span class="o">=</span> <span class="n">y_proba</span><span class="p">)</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;roc_curve&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;fpr&#39;</span><span class="p">:</span> <span class="n">fpr</span><span class="p">,</span>
                            <span class="s1">&#39;tpr&#39;</span><span class="p">:</span> <span class="n">tpr</span><span class="p">,</span> 
                            <span class="s1">&#39;thresholds&#39;</span><span class="p">:</span> <span class="n">thresholds_roc</span><span class="p">}</span>
    <span class="c1"># Precision-Recall curve</span>
    <span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">thresholds_pr</span> <span class="o">=</span> <span class="n">precision_recall_curve</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">probas_pred</span> <span class="o">=</span> <span class="n">y_proba</span><span class="p">)</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;pr_curve&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;precision&#39;</span> <span class="p">:</span> <span class="n">precision</span><span class="p">,</span>
                            <span class="s1">&#39;recall&#39;</span><span class="p">:</span> <span class="n">recall</span><span class="p">,</span>
                            <span class="s1">&#39;thresholds&#39;</span><span class="p">:</span> <span class="n">thresholds_pr</span><span class="p">}</span>
    <span class="c1"># Delinquency curve</span>
    <span class="n">d_curve_p</span> <span class="o">=</span> <span class="n">delinquency_curve</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">y_proba</span><span class="p">,</span> <span class="n">pointwise</span><span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">d_curve_agg</span> <span class="o">=</span> <span class="n">delinquency_curve</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">pointwise</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;d_curve_pointwise&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_curve_p</span>
    <span class="n">metrics</span><span class="p">[</span><span class="s1">&#39;d_curve_agg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">d_curve_agg</span>

    <span class="k">return</span> <span class="n">metrics</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_metrics_bootstrap</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">n_batches</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;computes frequently used metrics from bootstramp samples</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>

<span class="sd">    y_true : array_like, shape = [n_samples]</span>
<span class="sd">        true events labels</span>

<span class="sd">    y_pred : array_like, shape = [n_samples]</span>
<span class="sd">        events predictions</span>

<span class="sd">    n_batches : int, optinal</span>
<span class="sd">        number of bootstrap samples, default = 100</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>

<span class="sd">    metrics_summary : dict</span>
<span class="sd">        dictionary containig statistics of gini, ks, d_curve and c_curves</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">preds_samples</span> <span class="o">=</span> <span class="n">bootstrap_samples</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">n_batches</span><span class="p">)</span>

    <span class="n">df_metrics</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">compute_metrics</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="s1">&#39;y_true&#39;</span><span class="p">],</span>
                                                     <span class="n">y</span><span class="p">[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">])</span>
                              <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">preds_samples</span><span class="p">])</span>

    <span class="n">approval_rate_p</span> <span class="o">=</span> <span class="n">df_metrics</span><span class="p">[</span><span class="s1">&#39;d_curve_pointwise&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">default_rate_mean_p</span> <span class="o">=</span> <span class="n">df_metrics</span><span class="p">[</span><span class="s1">&#39;d_curve_pointwise&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">default_rate_std_p</span> <span class="o">=</span> <span class="n">df_metrics</span><span class="p">[</span><span class="s1">&#39;d_curve_pointwise&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
    <span class="n">d_curve_mean_p</span> <span class="o">=</span> <span class="p">(</span><span class="n">approval_rate_p</span><span class="p">,</span> <span class="n">default_rate_mean_p</span><span class="p">)</span>
    <span class="n">d_curve_std_p</span> <span class="o">=</span> <span class="p">(</span><span class="n">approval_rate_p</span><span class="p">,</span> <span class="n">default_rate_std_p</span><span class="p">)</span>

    <span class="n">approval_rate_agg</span> <span class="o">=</span> <span class="n">df_metrics</span><span class="p">[</span><span class="s1">&#39;d_curve_agg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">default_rate_mean_agg</span> <span class="o">=</span> <span class="n">df_metrics</span><span class="p">[</span><span class="s1">&#39;d_curve_agg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">default_rate_std_agg</span> <span class="o">=</span> <span class="n">df_metrics</span><span class="p">[</span><span class="s1">&#39;d_curve_agg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
    <span class="n">d_curve_mean_agg</span> <span class="o">=</span> <span class="p">(</span><span class="n">approval_rate_agg</span><span class="p">,</span> <span class="n">default_rate_mean_agg</span><span class="p">)</span>
    <span class="n">d_curve_std_agg</span> <span class="o">=</span> <span class="p">(</span><span class="n">approval_rate_agg</span><span class="p">,</span> <span class="n">default_rate_std_agg</span><span class="p">)</span>

    <span class="n">metrics_stats</span> <span class="o">=</span> <span class="n">df_metrics</span><span class="p">[[</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;gini&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;avg_precision&#39;</span><span class="p">,</span>
                                <span class="c1"># &#39;ks&#39;</span>
                                <span class="p">]]</span><span class="o">.</span><span class="n">apply</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">])</span>

    <span class="n">metrics_summary</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;d_curve_pointwise&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;d_curve_mean&#39;</span><span class="p">:</span> <span class="n">d_curve_mean_p</span><span class="p">,</span>
                                             <span class="s1">&#39;d_curve_std&#39;</span><span class="p">:</span> <span class="n">d_curve_std_p</span><span class="p">},</span>
                       <span class="s1">&#39;d_curve_agg&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;d_curve_mean&#39;</span><span class="p">:</span> <span class="n">d_curve_mean_agg</span><span class="p">,</span>
                                       <span class="s1">&#39;d_curve_std&#39;</span><span class="p">:</span> <span class="n">d_curve_std_agg</span><span class="p">},</span>
                       <span class="s1">&#39;metrics_stats&#39;</span><span class="p">:</span> <span class="n">metrics_stats</span><span class="p">,</span>
                       <span class="s1">&#39;df_metrics&#39;</span><span class="p">:</span> <span class="n">df_metrics</span><span class="p">}</span>
    <span class="k">return</span> <span class="n">metrics_summary</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">clean_ax</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="nb">all</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Clean spines of a matplotlib axis&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="nb">all</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">&quot;left&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">&quot;bottom&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">&quot;right&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>    
    <span class="n">ax</span><span class="o">.</span><span class="n">spines</span><span class="p">[</span><span class="s2">&quot;top&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">set_visible</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ax</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="data">
<h1>Data<a class="headerlink" href="#data" title="Permalink to this headline">#</a></h1>
<p>We will write a function that generates synthetic binary classification data allowing for various class imbalance levels</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_imbalanced_binary_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> 
                                            <span class="n">n_features</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
                                            <span class="n">n_informative</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                                            <span class="n">imbalance</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
                                            <span class="n">random_state</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span>
                                            <span class="n">class_sep</span> <span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Creates an imbalanced dataset for binary classification</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_samples: int, default = 1000,</span>
<span class="sd">            number of samples to generate</span>
<span class="sd">    n_features: int default = 4,</span>
<span class="sd">            number of features (not all are informative)</span>
<span class="sd">    n_informative: int default = None,</span>
<span class="sd">            number of informative features</span>
<span class="sd">            if none is specified floor(n_features/2) </span>
<span class="sd">            will be taken</span>
<span class="sd">    imbalance: float, default = 0.1</span>
<span class="sd">            proportion of the minority class</span>
<span class="sd">    random_state: int, default = 42</span>
<span class="sd">    class_sep: float, default = 1.0</span>
<span class="sd">        The larger the value the easier the classification task</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    data: pd.DataFrame,</span>
<span class="sd">        dataframe with n_features + 1 columns</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">n_informative</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">n_informative</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_features</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span> <span class="o">-</span> <span class="n">imbalance</span><span class="p">,</span> <span class="n">imbalance</span><span class="p">]</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">=</span> <span class="n">n_samples</span><span class="p">,</span>
                                <span class="n">n_features</span> <span class="o">=</span> <span class="n">n_features</span><span class="p">,</span>
                                <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                <span class="n">n_informative</span> <span class="o">=</span> <span class="n">n_informative</span><span class="p">,</span>
                                <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">,</span>                
                                <span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span><span class="p">,</span>
                                <span class="n">class_sep</span> <span class="o">=</span> <span class="n">class_sep</span><span class="p">)</span>
    <span class="n">column_names</span> <span class="o">=</span> <span class="p">[</span> <span class="sa">f</span><span class="s1">&#39;feature_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_features</span><span class="p">)]</span>      
    <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">column_names</span><span class="p">),</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">])],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">,</span> <span class="n">column_names</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">3</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">imbalance</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">n_informative</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">class_sep</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">data</span><span class="p">,</span> <span class="n">column_names</span> <span class="o">=</span> <span class="n">create_imbalanced_binary_classification</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">=</span> <span class="n">n_samples</span><span class="p">,</span> 
                                        <span class="n">n_features</span> <span class="o">=</span> <span class="n">n_features</span><span class="p">,</span>
                                        <span class="n">n_informative</span> <span class="o">=</span> <span class="n">n_informative</span><span class="p">,</span>
                                        <span class="n">imbalance</span> <span class="o">=</span> <span class="n">imbalance</span><span class="p">,</span>
                                        <span class="n">class_sep</span> <span class="o">=</span> <span class="n">class_sep</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">hue</span> <span class="o">=</span> <span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="n">plot_kws</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span> <span class="p">:</span> <span class="mf">0.5</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.PairGrid at 0x10c3cd7f0&gt;
</pre></div>
</div>
<img alt="../_images/pablo-best-benchmark-model_11_1.png" src="../_images/pablo-best-benchmark-model_11_1.png" />
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="model-comparison">
<h1>Model comparison<a class="headerlink" href="#model-comparison" title="Permalink to this headline">#</a></h1>
<p>In this section we will compare the performance (and other aspects) of different models under different class imbalance scenarios.
We will start by analyzing the performance dependence on the class imbalance index which we define as being the minority class ratio or <em>Prevalence</em>. If we call <span class="math notranslate nohighlight">\(N_{P}\)</span> and <span class="math notranslate nohighlight">\(N_{N}\)</span> the number of positive and negative observations, the prevalance is given by:
$<span class="math notranslate nohighlight">\(\nu := \dfrac{N_{P}}{N_{P}+N_{N}}.\)</span><span class="math notranslate nohighlight">\(
The main parameter to be explored is \)</span>\nu$, going from an equal-class setup to a highly imbalanced scenario where there really few examples of the minority class.</p>
<p>The first simple analysis is to be made upon the performance impact of the imbalance in classes for several algorithms. At this point we are interested in answering a simple practical question:</p>
<center><span style="background-color: blue">What is the best benchmark algorithm in the presence of class imbalance?</span></center>
<p>To answer this question we might be interested in looking at other aspects of the statistical learning process aside of the classification performance metrics, such as:</p>
<ul class="simple">
<li><p>Sample Size</p></li>
<li><p>Fit Time</p></li>
<li><p>Model complexity</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">HistGradientBoostingClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">matthews_corrcoef</span><span class="p">,</span>  <span class="n">precision_recall_curve</span><span class="p">,</span>\
                            <span class="n">average_precision_score</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">auc</span>
</pre></div>
</div>
</div>
</div>
<section id="performance-impact">
<h2>Performance Impact<a class="headerlink" href="#performance-impact" title="Permalink to this headline">#</a></h2>
<p>Let us see how the performance changes for different imbalance ratios:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">5</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">n_informative</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">class_sep</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">metrics</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">fit_time</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">imbalances</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">5e-3</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mf">5e-4</span><span class="p">]</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="mi">420</span>
<span class="k">for</span> <span class="n">imbalance</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">imbalances</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Minority class proportion: </span><span class="si">{</span><span class="n">imbalance</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">metrics</span><span class="p">[</span><span class="n">imbalance</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="c1"># Create dataset</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">column_names</span> <span class="o">=</span> <span class="n">create_imbalanced_binary_classification</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">=</span> <span class="n">n_samples</span><span class="p">,</span> 
                                        <span class="n">n_features</span> <span class="o">=</span> <span class="n">n_features</span><span class="p">,</span>
                                        <span class="n">n_informative</span> <span class="o">=</span> <span class="n">n_informative</span><span class="p">,</span>
                                        <span class="n">imbalance</span> <span class="o">=</span> <span class="n">imbalance</span><span class="p">,</span>
                                        <span class="n">class_sep</span> <span class="o">=</span> <span class="n">class_sep</span><span class="p">)</span>
    <span class="c1"># Train test split</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">column_names</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
    
    <span class="c1"># Logistic Regression</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Log Regression ...&#39;</span><span class="p">)</span>
    <span class="n">lr_pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                            <span class="p">(</span><span class="s1">&#39;logreg&#39;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">())])</span>
    <span class="n">lr_pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_proba_lr</span> <span class="o">=</span> <span class="n">lr_pipeline</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="c1"># metrics[imbalance][&#39;logreg&#39;] = compute_metrics(y_true = y_test, y_proba = y_proba_lr[:,1])</span>
    <span class="n">metrics</span><span class="p">[</span><span class="n">imbalance</span><span class="p">][</span><span class="s1">&#39;logreg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_metrics_bootstrap</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_proba_lr</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="c1"># Decision Tree</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Decision Tree ...&#39;</span><span class="p">)</span>
    <span class="n">dt_clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
    <span class="n">dt_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_proba_dt</span> <span class="o">=</span> <span class="n">dt_clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="c1"># metrics[imbalance][&#39;dt&#39;] = compute_metrics(y_true = y_test, y_proba= y_proba_dt[:,1])</span>
    <span class="n">metrics</span><span class="p">[</span><span class="n">imbalance</span><span class="p">][</span><span class="s1">&#39;dt&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_metrics_bootstrap</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_proba_dt</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Random Forest</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Random Forest ...&#39;</span><span class="p">)</span>
    <span class="n">rf_clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">()</span>
    <span class="n">rf_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_proba_rf</span> <span class="o">=</span> <span class="n">rf_clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">metrics</span><span class="p">[</span><span class="n">imbalance</span><span class="p">][</span><span class="s1">&#39;rf&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_metrics_bootstrap</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_proba_rf</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Naive Bayes</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Naive Bayes ...&#39;</span><span class="p">)</span>
    <span class="n">nb_clf</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
    <span class="n">nb_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_proba_nb</span> <span class="o">=</span> <span class="n">nb_clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">metrics</span><span class="p">[</span><span class="n">imbalance</span><span class="p">][</span><span class="s1">&#39;nb&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_metrics_bootstrap</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_proba_nb</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Gradient Boosting Machine</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Gradient Boosting ...&#39;</span><span class="p">)</span>
    <span class="n">gb_clf</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">()</span>
    <span class="n">gb_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_proba_gb</span> <span class="o">=</span> <span class="n">gb_clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">metrics</span><span class="p">[</span><span class="n">imbalance</span><span class="p">][</span><span class="s1">&#39;gb&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_metrics_bootstrap</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_proba_gb</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># Dummy Classifier</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">Dummy Classifier ...&#39;</span><span class="p">)</span>
    <span class="n">dummy_clf</span> <span class="o">=</span> <span class="n">DummyClassifier</span><span class="p">()</span>
    <span class="n">dummy_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_proba_dummy</span> <span class="o">=</span> <span class="n">dummy_clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">metrics</span><span class="p">[</span><span class="n">imbalance</span><span class="p">][</span><span class="s1">&#39;dummy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">compute_metrics_bootstrap</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_proba_dummy</span><span class="p">[:,</span><span class="mi">1</span><span class="p">])</span>

    <span class="c1"># # SVM Classifier</span>
    <span class="c1"># print(&#39;Support Vector Machine ...&#39;)</span>
    <span class="c1"># svm_pipeline = Pipeline([(&#39;scaler&#39;, StandardScaler()),</span>
    <span class="c1">#                         (&#39;svc&#39;, SVC(gamma = &#39;auto&#39;,</span>
    <span class="c1">#                                     probability= True))])</span>
    <span class="c1"># svm_pipeline.fit(X_train, y_train)</span>
    <span class="c1"># y_proba_svm = svm_pipeline.predict_proba(X_test)</span>
    <span class="c1"># metrics[imbalance][&#39;svm&#39;] = compute_metrics_bootstrap(y_true= y_test, y_pred = y_proba_svm[:,1])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;end&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "ee12ba280a0e4067901bd856a90086f6", "version_major": 2, "version_minor": 0}
</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Minority class proportion: 0.5
	Log Regression ...
	Decision Tree ...
	Random Forest ...
	Naive Bayes ...
	Gradient Boosting ...
	Dummy Classifier ...
Minority class proportion: 0.2
	Log Regression ...
	Decision Tree ...
	Random Forest ...
	Naive Bayes ...
	Gradient Boosting ...
	Dummy Classifier ...
Minority class proportion: 0.1
	Log Regression ...
	Decision Tree ...
	Random Forest ...
	Naive Bayes ...
	Gradient Boosting ...
	Dummy Classifier ...
Minority class proportion: 0.05
	Log Regression ...
	Decision Tree ...
	Random Forest ...
	Naive Bayes ...
	Gradient Boosting ...
	Dummy Classifier ...
Minority class proportion: 0.01
	Log Regression ...
	Decision Tree ...
	Random Forest ...
	Naive Bayes ...
	Gradient Boosting ...
	Dummy Classifier ...
Minority class proportion: 0.005
	Log Regression ...
	Decision Tree ...
	Random Forest ...
	Naive Bayes ...
	Gradient Boosting ...
	Dummy Classifier ...
Minority class proportion: 0.001
	Log Regression ...
	Decision Tree ...
	Random Forest ...
	Naive Bayes ...
	Gradient Boosting ...
	Dummy Classifier ...
Minority class proportion: 0.0005
	Log Regression ...
	Decision Tree ...
	Random Forest ...
	Naive Bayes ...
	Gradient Boosting ...
	Dummy Classifier ...
end
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_metric_evolution</span><span class="p">(</span><span class="n">metrics</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> 
                          <span class="n">model_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
                          <span class="n">metric_4_display</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span>
                          <span class="n">color</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;purple&#39;</span><span class="p">,</span> 
                          <span class="n">ax</span><span class="p">:</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">axis</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Plots the imbalance-evolution of classification metrics</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;logreg&#39;</span><span class="p">:</span> <span class="s1">&#39;Logistic Regression&#39;</span><span class="p">,</span>
              <span class="s1">&#39;dt&#39;</span><span class="p">:</span> <span class="s1">&#39;Decision Tree&#39;</span><span class="p">,</span>
              <span class="s1">&#39;rf&#39;</span><span class="p">:</span> <span class="s1">&#39;Random Forest&#39;</span><span class="p">,</span>
              <span class="s1">&#39;nb&#39;</span><span class="p">:</span> <span class="s1">&#39;Naive Bayes&#39;</span><span class="p">,</span>
              <span class="s1">&#39;gb&#39;</span><span class="p">:</span> <span class="s1">&#39;Gradient Boosting&#39;</span><span class="p">,</span>
              <span class="s1">&#39;dummy&#39;</span><span class="p">:</span> <span class="s1">&#39;Dummy Classifier&#39;</span> <span class="p">}</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">dpi</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">metrics_mean</span> <span class="o">=</span> <span class="p">[</span><span class="n">metrics</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">model_name</span><span class="p">][</span><span class="s1">&#39;metrics_stats&#39;</span><span class="p">][</span><span class="n">metric_4_display</span><span class="p">][</span><span class="s1">&#39;mean&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>
    <span class="n">metrics_std</span> <span class="o">=</span> <span class="p">[</span><span class="n">metrics</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="n">model_name</span><span class="p">][</span><span class="s1">&#39;metrics_stats&#39;</span><span class="p">][</span><span class="n">metric_4_display</span><span class="p">][</span><span class="s1">&#39;std&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">keys</span><span class="p">()]</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">metrics_mean</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span>
            <span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">,</span> 
                    <span class="n">y1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="n">metrics_mean</span><span class="p">,</span> <span class="n">metrics_std</span><span class="p">),</span> 
                    <span class="n">y2</span> <span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">metrics_mean</span><span class="p">,</span> <span class="n">metrics_std</span><span class="p">),</span>
                    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">facecolor</span> <span class="o">=</span> <span class="n">color</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Minority class proportion&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ax</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">metric_4_display</span> <span class="o">=</span> <span class="s1">&#39;roc_auc&#39;</span>
<span class="n">colors</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;logreg&#39;</span><span class="p">:</span> <span class="s1">&#39;#2F58EB&#39;</span><span class="p">,</span>
          <span class="s1">&#39;dt&#39;</span><span class="p">:</span> <span class="s1">&#39;#773BEB&#39;</span><span class="p">,</span>
          <span class="s1">&#39;rf&#39;</span><span class="p">:</span> <span class="s1">&#39;#12B8EB&#39;</span><span class="p">,</span>
          <span class="s1">&#39;nb&#39;</span><span class="p">:</span> <span class="s1">&#39;#EB9846&#39;</span><span class="p">,</span>
          <span class="s1">&#39;gb&#39;</span><span class="p">:</span> <span class="s1">&#39;#FC5A50&#39;</span><span class="p">,</span>
          <span class="s1">&#39;dummy&#39;</span><span class="p">:</span> <span class="s1">&#39;#029386&#39;</span><span class="p">}</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">dpi</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">color</span> <span class="ow">in</span> <span class="n">colors</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">plot_metric_evolution</span><span class="p">(</span><span class="n">metrics</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">metric_4_display</span><span class="o">=</span> <span class="n">metric_4_display</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.4</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">metric_4_display</span><span class="o">.</span><span class="n">upper</span><span class="p">())</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Sample Size: </span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">clean_ax</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pablo-best-benchmark-model_17_0.png" src="../_images/pablo-best-benchmark-model_17_0.png" />
</div>
</div>
<p>Let us plot the evolution of ROC and PR curves. We will first fix the imbalance ratio and plot the curves for all models under comparison.
Since the metrics were computed for bootstrapped samples of predictions we will actually plot the mean ROC and PR curves. The computation of these mean curves must be taken with care since an intermediate interpolation step is carried out for the mean curve calculation. In particular, the interpolation between two points in PR space might not be linear, see section 4 in <a class="reference external" href="https://www.biostat.wisc.edu/~page/rocpr.pdf">The Relationship between Precision-Recall and ROC curves</a>. This interpolation procedure will be revisited in the near future.</p>
<p>Then we can vary the imbalance ratio <span class="math notranslate nohighlight">\(\nu\)</span> to see the evolution of the ROC and PR curves.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_roc_curve</span><span class="p">(</span><span class="n">df_metrics</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                    <span class="n">color</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;purple&#39;</span><span class="p">,</span>
                    <span class="n">label</span> <span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                    <span class="n">ax</span> <span class="p">:</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">axis</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;plots the roc curve for a bootstrapped confidence interval</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df_metrics: pd.DataFrame</span>
<span class="sd">        dataframe that holds the roc curves for each boostrap sample</span>
<span class="sd">    label: str, (default = None)</span>
<span class="sd">    axis: matplotlib.axis (default = None)</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ax: matplotlib.axis</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">df_metrics</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">),</span> <span class="s1">&#39;df_metrics has to be a dataframe object!&#39;</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">dpi</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;mean ROC curve&#39;</span>
    <span class="c1"># Define the mean false positive rate</span>
    <span class="n">mean_fpr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="c1"># Interpolate the roc curves using the mean fpr values</span>
    <span class="n">tprs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">df_metrics</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">tpr</span> <span class="o">=</span> <span class="n">df_metrics</span><span class="p">[</span><span class="s1">&#39;roc_curve&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;tpr&#39;</span><span class="p">]</span>
        <span class="n">fpr</span> <span class="o">=</span> <span class="n">df_metrics</span><span class="p">[</span><span class="s1">&#39;roc_curve&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;fpr&#39;</span><span class="p">]</span>
        <span class="n">inter_tpr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">mean_fpr</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
        <span class="n">inter_tpr</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">tprs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inter_tpr</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">inter_tpr</span>
    <span class="n">mean_tpr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tprs</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">std_tpr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">tprs</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_fpr</span><span class="p">,</span> <span class="n">mean_tpr</span><span class="p">,</span> 
        <span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span> 
        <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="p">,</span>
        <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">mean_fpr</span><span class="p">,</span> 
                <span class="n">y1</span> <span class="o">=</span> <span class="n">mean_tpr</span> <span class="o">-</span> <span class="n">std_tpr</span><span class="p">,</span>
                <span class="n">y2</span> <span class="o">=</span> <span class="n">mean_tpr</span> <span class="o">+</span> <span class="n">std_tpr</span><span class="p">,</span>
                <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.12</span><span class="p">,</span> <span class="n">facecolor</span> <span class="o">=</span> <span class="n">color</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">frameon</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ax</span>
    
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_pr_curve</span><span class="p">(</span><span class="n">df_metrics</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span>
                    <span class="n">color</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;purple&#39;</span><span class="p">,</span>
                    <span class="n">label</span> <span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                    <span class="n">ax</span> <span class="p">:</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">axis</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">axis</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;plots the precisio-recall curve for a bootstrapped PR calculation</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    df_metrics: pd.DataFrame</span>
<span class="sd">        dataframe that holds the pr curves for each boostrapped sample</span>
<span class="sd">    label: str, (default = None)</span>
<span class="sd">    axis: matplotlib.axis (default = None)</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    ax: matplotlib.axis</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">df_metrics</span><span class="p">,</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">),</span> <span class="s1">&#39;df_metrics has to be a dataframe object!&#39;</span>
    <span class="k">if</span> <span class="n">ax</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">dpi</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">label</span> <span class="o">==</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;mean PR curve&#39;</span>
    <span class="c1"># Define the mean recall</span>
    <span class="n">mean_recall</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">precisions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">aucs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># Interpolate the precision curves</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">df_metrics</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">precision</span> <span class="o">=</span> <span class="n">df_metrics</span><span class="p">[</span><span class="s1">&#39;pr_curve&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;precision&#39;</span><span class="p">]</span>
        <span class="n">recall</span> <span class="o">=</span> <span class="n">df_metrics</span><span class="p">[</span><span class="s1">&#39;pr_curve&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;recall&#39;</span><span class="p">]</span>
        <span class="n">interp_precision</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">mean_recall</span><span class="p">,</span> <span class="n">xp</span> <span class="o">=</span> <span class="n">recall</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">fp</span> <span class="o">=</span> <span class="n">precision</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">precisions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">interp_precision</span><span class="p">)</span>
        <span class="n">pr_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">recall</span><span class="p">,</span> <span class="n">precision</span><span class="p">)</span>
        <span class="n">aucs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pr_auc</span><span class="p">)</span>
    <span class="n">mean_precision</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">precisions</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">std_precision</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">precisions</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="c1"># Actual Plot</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_recall</span><span class="p">,</span> <span class="n">mean_precision</span><span class="p">,</span>
            <span class="n">color</span> <span class="o">=</span> <span class="n">color</span><span class="p">,</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="p">,</span>
            <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">mean_recall</span><span class="p">,</span> 
                    <span class="n">y1</span> <span class="o">=</span> <span class="n">mean_precision</span> <span class="o">-</span> <span class="n">std_precision</span><span class="p">,</span>
                    <span class="n">y2</span> <span class="o">=</span> <span class="n">mean_precision</span> <span class="o">+</span> <span class="n">std_precision</span><span class="p">,</span>
                    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.12</span><span class="p">,</span> <span class="n">facecolor</span> <span class="o">=</span> <span class="n">color</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">frameon</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Precision&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Recall&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ax</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">imbalance</span> <span class="o">=</span> <span class="mf">0.005</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;logreg&#39;</span>
<span class="c1"># Get metrics</span>
<span class="n">df_metrics</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="n">imbalance</span><span class="p">][</span><span class="n">model_name</span><span class="p">][</span><span class="s1">&#39;df_metrics&#39;</span><span class="p">]</span>
<span class="c1"># Plot</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">dpi</span> <span class="o">=</span> <span class="mi">120</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">plot_roc_curve</span><span class="p">(</span><span class="n">df_metrics</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span>
                <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">lw</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> 
            <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;random&#39;</span><span class="p">,</span>
             <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">frameon</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">plot_pr_curve</span><span class="p">(</span><span class="n">df_metrics</span><span class="o">=</span><span class="n">df_metrics</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">clean_ax</span><span class="p">(</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">f</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="n">model_name</span><span class="o">.</span><span class="n">upper</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 0.98, &#39;LOGREG&#39;)
</pre></div>
</div>
<img alt="../_images/pablo-best-benchmark-model_21_1.png" src="../_images/pablo-best-benchmark-model_21_1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">cm</span>

<span class="n">cmap</span> <span class="o">=</span> <span class="n">cm</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s1">&#39;winter&#39;</span><span class="p">)</span>
<span class="n">color_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">imbalances</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="n">cmap</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="n">i</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">imbalances</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">imbalances</span><span class="p">))}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;logreg&#39;</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">dpi</span> <span class="o">=</span> <span class="mi">120</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> 
                <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> 
                <span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span> 
                <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
                <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;random&#39;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">init</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;plot initialization function&quot;&quot;&quot;</span>
    <span class="n">line</span><span class="o">.</span><span class="n">set_data</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">line</span><span class="p">,</span> 
<span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;plot animation function&quot;&quot;&quot;</span>
    <span class="n">imbalance</span> <span class="o">=</span> <span class="n">imbalances</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="c1"># Get metrics</span>
    <span class="n">df_metrics</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">[</span><span class="n">imbalance</span><span class="p">][</span><span class="n">model_name</span><span class="p">][</span><span class="s1">&#39;df_metrics&#39;</span><span class="p">]</span>
    <span class="c1"># Define the mean false positive rate</span>
    <span class="n">mean_fpr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="c1"># Interpolate the roc curves using the mean fpr values</span>
    <span class="n">tprs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">df_metrics</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">tpr</span> <span class="o">=</span> <span class="n">df_metrics</span><span class="p">[</span><span class="s1">&#39;roc_curve&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;tpr&#39;</span><span class="p">]</span>
        <span class="n">fpr</span> <span class="o">=</span> <span class="n">df_metrics</span><span class="p">[</span><span class="s1">&#39;roc_curve&#39;</span><span class="p">][</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;fpr&#39;</span><span class="p">]</span>
        <span class="n">inter_tpr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">interp</span><span class="p">(</span><span class="n">mean_fpr</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
        <span class="n">inter_tpr</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">tprs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inter_tpr</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">inter_tpr</span>
    <span class="n">mean_tpr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tprs</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">std_tpr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">tprs</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_fpr</span><span class="p">,</span> <span class="n">mean_tpr</span><span class="p">,</span> 
        <span class="n">color</span> <span class="o">=</span> <span class="n">color_dict</span><span class="p">[</span><span class="n">imbalance</span><span class="p">],</span> 
        <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;Imbalance = </span><span class="si">{</span><span class="n">imbalance</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
        <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">mean_fpr</span><span class="p">,</span> 
                <span class="n">y1</span> <span class="o">=</span> <span class="n">mean_tpr</span> <span class="o">-</span> <span class="n">std_tpr</span><span class="p">,</span>
                <span class="n">y2</span> <span class="o">=</span> <span class="n">mean_tpr</span> <span class="o">+</span> <span class="n">std_tpr</span><span class="p">,</span>
                <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.12</span><span class="p">,</span> <span class="n">facecolor</span> <span class="o">=</span> <span class="n">color_dict</span><span class="p">[</span><span class="n">imbalance</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">frameon</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">8</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;FPR&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;TPR&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">model_name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="si">}</span><span class="s1">: Bootstrapped ROC curves&#39;</span><span class="p">)</span>
<span class="n">anim</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">init_func</span><span class="o">=</span> <span class="n">init</span><span class="p">,</span> <span class="n">frames</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">imbalances</span><span class="p">),</span> <span class="n">interval</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">)</span>
<span class="n">anim</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">model_name</span><span class="si">}</span><span class="s1">_ROC_animation.gif&#39;</span><span class="p">,</span> <span class="n">writer</span> <span class="o">=</span> <span class="s1">&#39;imagegick&#39;</span><span class="p">,</span> <span class="n">fps</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">dpi</span> <span class="o">=</span> <span class="mi">120</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MovieWriter imagegick unavailable; using Pillow instead.
</pre></div>
</div>
<img alt="../_images/pablo-best-benchmark-model_23_1.png" src="../_images/pablo-best-benchmark-model_23_1.png" />
</div>
</div>
</section>
<section id="next-steps">
<h2>Next steps<a class="headerlink" href="#next-steps" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>keep training set imbalance fixed while changing the imbalance in the test set</p></li>
<li><p>construct a big dataset and sample imbalanced datasets such that we alwasy have the same number of positive samples. So, the imbalance ratio depends only in the</p></li>
</ul>
<p>Now that we know how to plot the ROC and PR curves</p>
</section>
<section id="sample-size-effect">
<h2>Sample Size effect<a class="headerlink" href="#sample-size-effect" title="Permalink to this headline">#</a></h2>
<p>At first sight the Logistic Regression performance dependence on imbalance is very similar to the Gradient Boosting Machine, which makes the Logisitic Regression a good candidate for a benchmark model. What other quantities should we look at?</p>
<ul class="simple">
<li><p>Training time</p></li>
<li><p>model complexity</p></li>
</ul>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="model-assessment-and-selection">
<h1>Model Assessment and Selection<a class="headerlink" href="#model-assessment-and-selection" title="Permalink to this headline">#</a></h1>
<p>In this section we will discuss the problem of model selection inside the imbalanced learning context. To begin the discussion let us introduce some basic definitions for the binary classification problem.
In general terms, we are interested in extracting information about an <strong>unknown</strong> joint probability distribution <span class="math notranslate nohighlight">\(\text{Pr}(X,Y)\)</span> of random variables <span class="math notranslate nohighlight">\(X, Y\)</span> from which we got a sample <span class="math notranslate nohighlight">\(\mathcal{D} = \{(x_i, y_i)\}_{i = 1, \dots, N}\)</span> (the dataset), with the random variable <span class="math notranslate nohighlight">\(Y\)</span> taking values in <span class="math notranslate nohighlight">\(\{0,1\}\)</span>. In other words: <span class="math notranslate nohighlight">\(y|X = x \sim \text{Bernoulli}(x)\)</span>.
The binary classification problem can be summarized as the problem of finding a function <span class="math notranslate nohighlight">\(f: X \longrightarrow \{0,1\}\)</span> defined by:
$<span class="math notranslate nohighlight">\(f(x):= \mathbb{P}(Y=1 | X=x),\)</span><span class="math notranslate nohighlight">\(
often called **score**. In practice, solving the classification problem implies using an **estimator** function \)</span>\hat{f}:X \longrightarrow [0,1]<span class="math notranslate nohighlight">\(. It is worth mentioning that the output of this estimator function is not to be interpreted as a probability, in general. A more detailed discussion will be given in the model calibration section.
The classification problem is not entirely solved by finding the best estimator function \)</span>\hat{f}<span class="math notranslate nohighlight">\( since in general we could be actually interested in predicting the class to which a particular example belongs, this is, we are looking for an estimator:
\)</span><span class="math notranslate nohighlight">\(\hat{y}_{\lambda}(x) = \begin{cases}
                1,\;\; \text{if } \hat{f}(x) \geq \lambda, \\ 
                0, \;\; \text{if } \hat{f}(x) &lt; \lambda
            \end{cases}\)</span>$
Following these lines, we can state the problem of <strong>model selection</strong> as trying to answer the following question:</p>
<blockquote>
<div><p>Given a set of estimators <span class="math notranslate nohighlight">\(\{\hat{f}_1, \hat{f}_2, \dots\}\)</span>, <em>what is the best estimator function</em> <span class="math notranslate nohighlight">\(\hat{f}: X \longrightarrow [0,1]\)</span> among them?*</p>
</div></blockquote>
<p>What does it mean to be <em>the best</em> estimator anyway? We will begin the model selection discussion by trying to answer this question.
Note that there is a related question that we will try to solve (at the same time) related to the choice of the <span class="math notranslate nohighlight">\(\lambda\)</span> threshold parameter.</p>
<p>There are different criteria by which several <strong>estimators</strong> could be compared. In practice, however, we are mainly interested in measuring:</p>
<ul class="simple">
<li><p>missclassification error or classification metrics</p></li>
<li><p>model complexity and <em>bias-variance</em> tradeoff</p></li>
<li><p>training time</p></li>
<li><p>sensitivity to class imbalance</p></li>
<li><p>…</p></li>
</ul>
<p>Often, solving a binary classification problem involves optimizing the hyperparameters of inference models. This process can be both computationally intensive and time consuming. Thus, the gain in performance obtained by following this optimization procedure has to justify the effort, otherwise we would choose to go on with a simpler model. To this end, it is always a good practice to compare the results of complex model with those of a simpler and more interpretable model (usually logistic regression).</p>
<section id="error-estimation">
<h2>Error estimation<a class="headerlink" href="#error-estimation" title="Permalink to this headline">#</a></h2>
<p>There are different measures of missclassification error and choosing one might depend on the nature of the problem and the characteristics of the joint probability distribution <span class="math notranslate nohighlight">\(\text{Pr}(X,Y)\)</span>. A more detailed discussion will be given in the section about losses for binary classification.
Let us consider a classification model <span class="math notranslate nohighlight">\(\hat{f}: X \longrightarrow [0,1]\)</span> whose paremeters were estimated using a training set <span class="math notranslate nohighlight">\(\mathcal{T} = \{(\mathbb{x}_i, y_i)\}_{i = 1}^{N}\)</span>. Consider also an error (loss) function <span class="math notranslate nohighlight">\(L: \{0,1\} \times [0,1] \longrightarrow \mathbb{R}_{+}\)</span>, such that the missclassification error of a predicted observation <span class="math notranslate nohighlight">\(\hat{y} = \hat{f}(x)\)</span>, where <span class="math notranslate nohighlight">\(x \sim X\)</span> is denoted <span class="math notranslate nohighlight">\(L(y, \hat{y})\)</span>. One example of such a function and a common choice for binary classification problems is the so called <strong>Binary Cross-Entropy</strong> or <strong>Log Loss</strong>, given by:
\begin{align*}
L = -\dfrac{1}{N}\sum_{i = 1}^{N}\left[y_i \log(\hat{y}<em>i)+ (1 - y_i)\log(1 - \hat{y}<em>i)\right]
\end{align*}
where <span class="math notranslate nohighlight">\(N\)</span> is the size of the sample <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> used for the error estimation, <span class="math notranslate nohighlight">\(y_i\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th ground truth value an <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th estimated value (with estimator <span class="math notranslate nohighlight">\(\hat{f}\)</span>).
The <strong>Test error</strong> or <strong>Generalization error</strong> is the missclassification error calculated on an independent data sample, commonly called <em>test set</em>.
\begin{align}
\text{Err}</em>{\mathcal{T}} = \mathbb{E}\left[L(Y, \hat{f}(X))| \mathcal{T}\right],
\end{align}
note that the training set is fixed, this means that the error estimate is conditioned on the training set choice. We can also average over the training set generation process to get the <em>expected prediction error</em>:
\begin{align}
\text{Err} = \mathbb{E}\left[L(Y, \hat{f}(X))\right] = \mathbb{E}\left[\text{Err}</em>{\mathcal{T}}\right].
\end{align}
<strong>Training error</strong> is nothing else that the missclassification error calculated on the training dataset <span class="math notranslate nohighlight">\(\mathcal{T}\)</span>,
\begin{align*}
\overline{\text{err}} = \dfrac{1}{N}\sum_{i = 1}^{N}L(y_i, \hat{f}(x_i)).
\end{align*}</p>
<section id="training-error-is-a-bad-error-estimate">
<h3>Training error is a bad error estimate!<a class="headerlink" href="#training-error-is-a-bad-error-estimate" title="Permalink to this headline">#</a></h3>
<p>In order to illustrate how the training error is not a good estimate of the test error let us consider the loss dependance on the model complexity. Consider the case of a binary classification problem, which we are trying to solve using a <strong>logistic regression</strong> or another more complex model (such as a GBDT). We will analyze the behavior of the error estimates for different realizations of the test set <span class="math notranslate nohighlight">\(\mathcal{T}\)</span>. To do that, let us construct a big dataset (parent distribution) from which we will sample different realizations of the training set. Aditionally, a hold-out dataset (the test dataset) will be sampled a single time from the parent distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">log_loss</span><span class="p">,</span> <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">precision_recall_curve</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">4</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">data</span><span class="p">,</span> <span class="n">col_names</span> <span class="o">=</span> <span class="n">create_imbalanced_binary_classification</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">=</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">n_features</span><span class="p">,</span> 
                                                            <span class="n">n_informative</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_features</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">imbalance</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
                                                            <span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span><span class="p">,</span> <span class="n">class_sep</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.3018
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">col_names</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
<span class="n">X_train_0</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train_0</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
<span class="n">train_sample_size</span> <span class="o">=</span> <span class="mf">0.4</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">max_depths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">]</span>
<span class="n">results_train</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">)}</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">max_depths</span><span class="p">}</span>
<span class="n">results_test</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">)}</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">max_depths</span><span class="p">}</span>
<span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">product</span><span class="p">(</span><span class="n">max_depths</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">))):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">X_train_0</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span> <span class="o">=</span> <span class="n">train_sample_size</span><span class="p">)</span><span class="o">.</span><span class="n">index</span>
    <span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train_0</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train_0</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="c1"># Decision Tree</span>
    <span class="c1"># print(f&#39;\tDecision Tree | max_depth = {n}&#39;)</span>
    <span class="n">dt_clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">n</span><span class="p">)</span>
    <span class="n">dt_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_proba</span> <span class="o">=</span> <span class="n">dt_clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">y_proba_train</span> <span class="o">=</span> <span class="n">dt_clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
    <span class="n">results_train</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_loss</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_proba_train</span><span class="p">)</span> 
    <span class="n">results_test</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_loss</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_proba</span><span class="p">)</span> 
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"model_id": "2e9bd14c664141bfac47d53d0c415a26", "version_major": 2, "version_minor": 0}
</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results_stats_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="n">key</span> <span class="p">:</span> <span class="p">{</span><span class="s1">&#39;mean&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">results_train</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">())),</span>
                        <span class="s1">&#39;std&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">results_train</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()))}</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">max_depths</span><span class="p">})</span>
<span class="n">results_stats_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="n">key</span> <span class="p">:</span> <span class="p">{</span><span class="s1">&#39;mean&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">results_test</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">())),</span>
                        <span class="s1">&#39;std&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">results_test</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()))}</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">max_depths</span><span class="p">})</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">dpi</span> <span class="o">=</span> <span class="mi">120</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">max_depths</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_stats_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
            <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span>
            <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;train error&#39;</span><span class="p">,</span>
            <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> 
                <span class="n">y1</span> <span class="o">=</span> <span class="n">results_stats_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">results_stats_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;std&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                <span class="n">y2</span> <span class="o">=</span> <span class="n">results_stats_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span> <span class="o">+</span> <span class="n">results_stats_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;std&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
            <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_stats_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
            <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span>
            <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;test error&#39;</span><span class="p">,</span>
            <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;purple&#39;</span><span class="p">,</span>
            <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> 
                <span class="n">y1</span> <span class="o">=</span> <span class="n">results_stats_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">results_stats_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;std&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                <span class="n">y2</span> <span class="o">=</span> <span class="n">results_stats_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span> <span class="o">+</span> <span class="n">results_stats_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;std&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
            <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">max_depths</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Model Complexity (max depth)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Log Loss&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">frameon</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Generalization error $Err = \mathbb</span><span class="si">{E}</span><span class="s1">[Err_{\tau}]$&#39;</span><span class="p">)</span>
<span class="n">clean_ax</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pablo-best-benchmark-model_38_0.png" src="../_images/pablo-best-benchmark-model_38_0.png" />
</div>
</div>
<p>The amount by which the training error and the true error differ is often called <strong>optimism</strong>. There are several ways of seeing this. To construct the above figure we used a held-out dataset (not seen in the training process) to compute the <em>true error</em>. Another common estimate of the true <em>training</em> error is often called <strong>in-sample error</strong>:
$<span class="math notranslate nohighlight">\(\text{Err}_{\text{in}} = \dfrac{1}{N}\sum_{i=1}^{N}\mathbb{E}_{Y^0}[L(Y_{i}^0), \hat{f}(x_i)|\mathcal{T}],\)</span><span class="math notranslate nohighlight">\(
where \)</span>Y^0<span class="math notranslate nohighlight">\( stands for the distribution of all possible target values (in the training dataset). This is, for each observation \)</span>x_i<span class="math notranslate nohighlight">\( we compute the expected error over the distribution of all possible outcomes \)</span>Y^0_i<span class="math notranslate nohighlight">\(. The optimism is defined as:
\begin{align*}
\text{op} := \text{Err}_{\text{in}} - \overline{\text{err}}.
\end{align*}
Thus, a way of solving the error underestimation problem is to estimate the optimism \)</span>\text{op}<span class="math notranslate nohighlight">\( and add this value to the training error \)</span>\text{err}<span class="math notranslate nohighlight">\( so the underestimation gets &quot;corrected&quot;. There are several methods that treat the error underestimation problem this way such as the *Akaike information criterion* (AIC), the *Bayesian information criterion* and the \)</span>C_p$-<em>statistic</em>. We refer the reader to [Tibshirani] for a detailed discussion about these techniques. We will rather focus our discussion in the usual way this problem is tackled in practice, this is, Cross Validation and Bootstrap.</p>
<p>Let us see another example where we use an ensemble of decision trees, such as gradient boosted ensemble of decision trees. We are interested in the training-test error gap as the number of weak learners increases.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lightgbm</span> <span class="kn">import</span> <span class="n">LGBMClassifier</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># animation parameter</span>
<span class="n">n_samples_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">10</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="o">**</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="o">*</span><span class="mi">10</span><span class="o">**</span><span class="mi">4</span><span class="p">]</span> 

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">dpi</span> <span class="o">=</span> <span class="mi">120</span><span class="p">)</span>
<span class="n">line</span><span class="p">,</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([],[])</span>
<span class="k">def</span> <span class="nf">init</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;plot initialization function&quot;&quot;&quot;</span>
    <span class="n">line</span><span class="o">.</span><span class="n">set_data</span><span class="p">([],[])</span>
    <span class="k">return</span> <span class="n">line</span><span class="p">,</span> 
<span class="k">def</span> <span class="nf">animate</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Animates the training vs true error estimatives for different sample sizes and fixed imbalance&quot;&quot;&quot;</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
    <span class="c1"># n_samples_list = [100, 500, 10**3, 10**4, 10**5]</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">n_samples_list</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span>
    <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">data</span><span class="p">,</span> <span class="n">col_names</span> <span class="o">=</span> <span class="n">create_imbalanced_binary_classification</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">=</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">n_features</span><span class="p">,</span> 
                                                            <span class="n">n_informative</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_features</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">imbalance</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
                                                            <span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span><span class="p">,</span> <span class="n">class_sep</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
    <span class="n">parent_positive_rate</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">col_names</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
    <span class="n">X_train_0</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train_0</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">train_sample_size</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="n">n_trials</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">n_estimators</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">150</span><span class="p">]</span>
    <span class="n">results_train</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">)}</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">n_estimators</span><span class="p">}</span>
    <span class="n">results_test</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">)}</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">n_estimators</span><span class="p">}</span>
    <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_trials</span><span class="p">)):</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">X_train_0</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span> <span class="o">=</span> <span class="n">train_sample_size</span><span class="p">)</span><span class="o">.</span><span class="n">index</span>
        <span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train_0</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train_0</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="c1"># Model training</span>
        <span class="n">clf</span> <span class="o">=</span> <span class="n">LGBMClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="n">y_proba</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
        <span class="n">y_proba_train</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
        <span class="n">results_train</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_loss</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_proba_train</span><span class="p">)</span> 
        <span class="n">results_test</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_loss</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_proba</span><span class="p">)</span>
    <span class="c1"># Get results for plotting</span>
    <span class="n">results_stats_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="n">key</span> <span class="p">:</span> <span class="p">{</span><span class="s1">&#39;mean&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">results_train</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">())),</span>
                        <span class="s1">&#39;std&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">results_train</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()))}</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">n_estimators</span><span class="p">})</span>
    <span class="n">results_stats_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="n">key</span> <span class="p">:</span> <span class="p">{</span><span class="s1">&#39;mean&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">results_test</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">())),</span>    
                        <span class="s1">&#39;std&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">results_test</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()))}</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">n_estimators</span><span class="p">})</span>
    <span class="c1"># Plot the curves</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">n_estimators</span><span class="p">))</span>
    <span class="c1"># plt.xlim((0, 1)) </span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_stats_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span>
                <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;train error&#39;</span><span class="p">,</span>
                <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span>
                <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> 
                    <span class="n">y1</span> <span class="o">=</span> <span class="n">results_stats_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">results_stats_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;std&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                    <span class="n">y2</span> <span class="o">=</span> <span class="n">results_stats_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span> <span class="o">+</span> <span class="n">results_stats_train</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;std&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">results_stats_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                    <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span>
                    <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;test error&#39;</span><span class="p">,</span>
                    <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;purple&#39;</span><span class="p">,</span>
                    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.6</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> 
                        <span class="n">y1</span> <span class="o">=</span> <span class="n">results_stats_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span> <span class="o">-</span> <span class="n">results_stats_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;std&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                        <span class="n">y2</span> <span class="o">=</span> <span class="n">results_stats_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span> <span class="o">+</span> <span class="n">results_stats_test</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="s1">&#39;std&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>
                    <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;purple&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">n_estimators</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="s1">&#39;best&#39;</span><span class="p">,</span> <span class="n">frameon</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Model Complexity (n_estimators)&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Log Loss&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;Generalization error $Err = \mathbb</span><span class="si">{E}</span><span class="s1">[Err_{\tau}]$&#39;</span> <span class="o">+</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> Sample Size = </span><span class="si">{</span><span class="n">n_samples</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="c1"># Fun time!</span>

<span class="n">anim</span> <span class="o">=</span> <span class="n">FuncAnimation</span><span class="p">(</span><span class="n">fig</span><span class="p">,</span> <span class="n">animate</span><span class="p">,</span> <span class="n">init_func</span><span class="o">=</span> <span class="n">init</span><span class="p">,</span> <span class="n">frames</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">n_samples_list</span><span class="p">),</span> <span class="n">interval</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">5</span><span class="p">)</span>
<span class="n">anim</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;LGBM_error_animation.gif&#39;</span><span class="p">,</span> <span class="n">writer</span> <span class="o">=</span> <span class="s1">&#39;imagegick&#39;</span><span class="p">,</span> <span class="n">fps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dpi</span> <span class="o">=</span> <span class="mi">120</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MovieWriter imagegick unavailable; using Pillow instead.
</pre></div>
</div>
<img alt="../_images/pablo-best-benchmark-model_41_1.png" src="../_images/pablo-best-benchmark-model_41_1.png" />
</div>
</div>
<p>The above figure (see the animation) shows that the training error severely understimates the test (true) error value as the model becomes more complex, commonly known as <em>overfitting</em>. The effect is diminished as the training dataset size grows. The most widely used method to turnaround this error underestimation problem is known as <strong>Cross-Validation</strong> (CV). This method directly estimates the <em>extra-sample</em> error <span class="math notranslate nohighlight">\(\text{Err} = \mathbb{E}[L(Y, \hat{f}(X))]\)</span>, where <span class="math notranslate nohighlight">\((X,Y)\)</span> is a <em>test</em> sample from the joint distribution <span class="math notranslate nohighlight">\(p(X,Y)\)</span>. In k-fold CV the training dataset <span class="math notranslate nohighlight">\(\mathcal{T}\)</span> is split into k mutually-exclusive similar subsets <span class="math notranslate nohighlight">\(\{\mathcal{T}_1,\mathcal{T}_2, \dots, \mathcal{T}_k\}\)</span>. At each CV round, a different fold is left out of the training procedure and used as a testing set. The error (loss) is then averaged across all folds to obtain the generalization error estimate. To see this, let <span class="math notranslate nohighlight">\(\kappa: \{i\}_{i=1}^{N} \rightarrow \{k\}_{k=1}^{K}\)</span> be a mapping function that assigns a fold <span class="math notranslate nohighlight">\(k\)</span> to each observation <span class="math notranslate nohighlight">\(i\)</span>. Furthermore, let us write as <span class="math notranslate nohighlight">\(\mathcal{T}_k\)</span> the fold that is left out of the training procedure, the model obtained by fitting on the rest of the folds is then denoted as <span class="math notranslate nohighlight">\(\hat{f}^{-k}(x)\)</span>. Then we can write the cross-validation estimate of missclassification error as:
\begin{align*}
\widehat{\text{Err}}<em>{\text{CV}}(\hat{f}) = \dfrac{1}{N}\sum</em>{i = 1}^{N} L(y_i, \hat{f}^{-\kappa (i)}(x_i))
\end{align*}
The cross validation estimate is a random number that depends on the division into the k-folds. The extreme case where <span class="math notranslate nohighlight">\(k = n\)</span> is known as <strong>leave-one-out</strong> cross validation, since a single entity is left out of the training set for each fold and the fit is computed on all the rest observations. This can turn the method more computationally costly as the size of the training dataset grows. The choice for the number of splits is discussed in <a class="reference external" href="http://ai.stanford.edu/~ronnyk/accEst.pdf">kohavi</a>, stratified ten-fold cross-validation being the most common recommendation.</p>
<p>Often CV is also used as a method for evaluating different hyperparameters of the classification model. In this scenario, consider a set of different models all indexed by a (set of) parameter(s) <span class="math notranslate nohighlight">\(\{\xi\}\)</span> such as regularization weights or the number of estimators for ensemble methods. We denote the model whose fit was computed using all but the <span class="math notranslate nohighlight">\(k\)</span>-th fold as <span class="math notranslate nohighlight">\(\hat{f}^{-k}(x,\xi)\)</span> so we can write:
\begin{align*}
\widehat{\text{Err}}<em>{CV}(\hat{f}, \xi) = \dfrac{1}{N} \sum</em>{i = 1}^{N}L(y_i, \hat{f}^{-\kappa(i)}(x_i, \xi)),
\end{align*}
for the cross-validation error estimate. This error estimate can be used to select the parameter <span class="math notranslate nohighlight">\(\hat{\xi}\)</span> for which the above quantity is minimum. The model <span class="math notranslate nohighlight">\(\hat{f}(x, \hat{\xi})\)</span> is then chosen as the final model.</p>
<p>In the following experiment we will try to estimate the true error <a class="reference external" href="#fn1"><sup id="fn1">1</sup></a> using the cross validation strategy.</p>
<p><a class="reference external" href="#fn1"><sup id="fn1">1</sup></a> Computed on a held-out validation sample that is never seen in the training procedure.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate classification dataset</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">4</span>
<span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">data</span><span class="p">,</span> <span class="n">col_names</span> <span class="o">=</span> <span class="n">create_imbalanced_binary_classification</span><span class="p">(</span><span class="n">n_samples</span> <span class="o">=</span> <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">n_features</span><span class="p">,</span> 
                                                            <span class="n">n_informative</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">n_features</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">imbalance</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span>
                                                            <span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span><span class="p">,</span> <span class="n">class_sep</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.3018
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_validate</span><span class="p">,</span> <span class="n">KFold</span><span class="p">,</span> <span class="n">StratifiedKFold</span><span class="p">,</span> <span class="n">RepeatedStratifiedKFold</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">models</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;logreg&#39;</span><span class="p">:</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;scaler&#39;</span><span class="p">,</span> <span class="n">StandardScaler</span><span class="p">()),</span>
                        <span class="p">(</span><span class="s1">&#39;logreg&#39;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">())</span>
                        <span class="p">]),</span>
    <span class="s1">&#39;random_forest&#39;</span><span class="p">:</span> <span class="n">RandomForestClassifier</span><span class="p">(),</span>
    <span class="s1">&#39;lgbm&#39;</span><span class="p">:</span> <span class="n">LGBMClassifier</span><span class="p">(</span><span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">7</span><span class="p">),</span>
    <span class="s1">&#39;gaussian_nb&#39;</span><span class="p">:</span> <span class="n">GaussianNB</span><span class="p">()</span>
    
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">run_cross_validation_from_split</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">kfold</span><span class="p">,</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    trains and evaluates a model given the kfold generator</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># results storage:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">n_splits</span> <span class="o">=</span> <span class="n">kfold</span><span class="o">.</span><span class="n">n_splits</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
        <span class="n">n_splits</span> <span class="o">=</span> <span class="n">kfold</span><span class="o">.</span><span class="n">get_n_splits</span><span class="p">()</span>
    <span class="n">results_cv</span> <span class="o">=</span> <span class="p">{</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">:</span> <span class="p">{</span>
                            <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
                            <span class="s1">&#39;valid&#39;</span><span class="p">:</span> <span class="kc">None</span>
                            <span class="p">}</span> 
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">)</span>
                <span class="p">}</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">kfold</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">)):</span>
        <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X_dev</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">X_dev</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
        <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y_dev</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">train_index</span><span class="p">],</span> <span class="n">y_dev</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
        <span class="c1">### model training and evaluation</span>
        <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
        <span class="n">y_proba</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
        <span class="n">y_proba_valid</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_valid</span><span class="p">)</span>
        <span class="n">results_cv</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">][</span><span class="s1">&#39;test&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_loss</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_proba</span><span class="p">)</span>
        <span class="n">results_cv</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">][</span><span class="s1">&#39;valid&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">log_loss</span><span class="p">(</span><span class="n">y_valid</span><span class="p">,</span> <span class="n">y_proba_valid</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">results_cv</span>
</pre></div>
</div>
</div>
</div>
<p>10-fold Cross Validation</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;lgbm&#39;</span>
<span class="n">n_folds</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">col_names</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
<span class="c1"># Sample validation test set </span>
<span class="n">X_dev</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_dev</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">)</span>
<span class="c1"># KFold cross validation</span>
<span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_folds</span><span class="p">)</span>
<span class="n">results_cv</span> <span class="o">=</span> <span class="n">run_cross_validation_from_split</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span> 
                                                <span class="n">kfold</span> <span class="o">=</span> <span class="n">skf</span><span class="p">,</span>
                                                <span class="n">X_dev</span> <span class="o">=</span> <span class="n">X_dev</span><span class="p">,</span>
                                                <span class="n">y_dev</span> <span class="o">=</span> <span class="n">y_dev</span><span class="p">,</span> 
                                                <span class="n">X_valid</span> <span class="o">=</span> <span class="n">X_valid</span><span class="p">,</span> 
                                                <span class="n">y_valid</span> <span class="o">=</span> <span class="n">y_valid</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Repeated Cross Validation</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rskf</span> <span class="o">=</span> <span class="n">RepeatedStratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span> <span class="o">=</span> <span class="n">n_folds</span><span class="p">,</span> <span class="n">n_repeats</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">results_repeated_cv</span> <span class="o">=</span> <span class="n">run_cross_validation_from_split</span><span class="p">(</span><span class="n">models</span><span class="p">[</span><span class="n">model_name</span><span class="p">],</span> <span class="n">kfold</span> <span class="o">=</span> <span class="n">rskf</span><span class="p">,</span> 
                                                        <span class="n">X_dev</span> <span class="o">=</span> <span class="n">X_dev</span><span class="p">,</span> <span class="n">y_dev</span> <span class="o">=</span> <span class="n">y_dev</span><span class="p">,</span>
                                                        <span class="n">X_valid</span> <span class="o">=</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">y_valid</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&#39;RepeatedStratifiedKFold&#39; object has no attribute &#39;n_splits&#39;
</pre></div>
</div>
</div>
</div>
<p>Gather all results and plot:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df_results_cv</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results_cv</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">df_results_repeated_cv</span>  <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results_repeated_cv</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="c1"># For the repeated cross validation results, we need to average over each repetition</span>
<span class="n">df_results_repeated_cv</span><span class="p">[</span><span class="s1">&#39;rep&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_results_repeated_cv</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()[</span><span class="s1">&#39;index&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">10</span><span class="p">))</span><span class="o">.</span><span class="n">values</span>
<span class="n">df_results_rep_summarized</span> <span class="o">=</span> <span class="n">df_results_repeated_cv</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;rep&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">errors</span> <span class="o">=</span> <span class="p">[</span><span class="n">df_results_cv</span><span class="p">[</span><span class="s1">&#39;valid&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>\
         <span class="n">df_results_cv</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span>\
         <span class="n">df_results_rep_summarized</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">]]</span>
<span class="n">x_labels</span> <span class="o">=</span> <span class="p">[</span><span class="sa">r</span><span class="s1">&#39;$Err$&#39;</span><span class="p">,</span>\
            <span class="sa">r</span><span class="s1">&#39;$Err_</span><span class="si">{CV10}</span><span class="s1">$&#39;</span><span class="p">,</span>\
            <span class="sa">r</span><span class="s1">&#39;$Err_</span><span class="si">{RCV10}</span><span class="s1">$&#39;</span><span class="p">]</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">dpi</span> <span class="o">=</span> <span class="mi">120</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">errors</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">x_labels</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;log loss&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Error estimation comparative&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pablo-best-benchmark-model_53_0.png" src="../_images/pablo-best-benchmark-model_53_0.png" />
</div>
</div>
</section>
<section id="bootstrapp-error-estimation">
<h3>Bootstrapp Error estimation<a class="headerlink" href="#bootstrapp-error-estimation" title="Permalink to this headline">#</a></h3>
<p>The bootstrap [cite: Efron original paper, Efron’s book, Estimating the error rate of a prediction rule: improvements on crossvalidation] is a non-parametric procedure for estimating parameters, in general, and to estimate error rates in particular. The main idea behind estimating parameters (error rates) using the bootstrap is to construct different realizations of dataset by sampling from it with replacement. Then, the statistic of interest is calculated for each bootstrap sample to obtain a distribution from which we can extract an expected value and confidence intervals.</p>
<p>Naturally, the bootstrap can be used for missclassification error estimation in our binary classification context. To do that, we construct various realizations of the whole classification experiment and then estimate the error rate over the distribution of error values coming from the several bootstrap experiments.</p>
<p>To understand why we need to proceed with care when building a bootstrapped sample let us consider the main problem we are treating in this book: binary classification. So, let <span class="math notranslate nohighlight">\(\mathcal{D}=\{(x_i,y_i)\}_{i=1}^{N}\)</span>, where <span class="math notranslate nohighlight">\(x_i\)</span> is a feature vector for the i-th observation (subject) and <span class="math notranslate nohighlight">\(y_i \in \{0,1\} \)</span> its binary target value.</p>
<p>Suppose we want to estimate the expected error of a classification model trained on the dataset <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> via bootstrap. To do this, we could start by constructing <span class="math notranslate nohighlight">\(B\)</span> bootstrap samples <span class="math notranslate nohighlight">\(\{\mathcal{B}_b\}_{b = 1}^{B}\)</span> of the original dataset. A classification model is trained for each of the <span class="math notranslate nohighlight">\(B\)</span> bootstrap samples and the behavior (eg. performance) of the models is examined througout all bootstrap datasets. Let <span class="math notranslate nohighlight">\(\hat{f}^{*b}(x_i)\)</span> be the predicted value for observation <span class="math notranslate nohighlight">\(x_i\)</span> from the model trained using the bootstrap dataset <span class="math notranslate nohighlight">\(\mathcal{B}_b\)</span>. One naive estimate of the conditional error consists on evaluating the error of such models on the original training dataset:
$<span class="math notranslate nohighlight">\(\widehat{\text{Err}}_{boot} = \dfrac{1}{B}\dfrac{1}{N}\sum_{b = 1}^{B}\sum_{i = 1}^{N}L(y_i, \hat{f}^{*b}(x_i)).\)</span><span class="math notranslate nohighlight">\(
Note that by using the entire dataset to estimate the error, the train observations are also used in the error calculation which naturally introduces a bias to the error estimate. This is, the expression above actually underestimates the true error. Further discussion about this bias can be found at [TESL, ].
Even if we use extra-sample observations to estimate the conditional error, there is a bias in the estimation due to the number of unique observations on each bootstrap dataset being (on average) \)</span>0.632N<span class="math notranslate nohighlight">\(, where \)</span>N = |\mathcal{D}|<span class="math notranslate nohighlight">\( for big enough datasets. To see this, note that when constructing a bootstrap dataset \)</span>\mathcal{B}_b<span class="math notranslate nohighlight">\( some observations are actually left out of the sample. Moreover, the probability of the i-th observation \)</span>(x_i, y_i)<span class="math notranslate nohighlight">\( being part of the boostrap dataset \)</span>\mathcal{B}_b<span class="math notranslate nohighlight">\( is given by:
\)</span><span class="math notranslate nohighlight">\(\mathbb{P}((x_i,y_i)\in \mathcal{B}_B) = 1 - (1 - \dfrac{1}{N})^{N},\)</span><span class="math notranslate nohighlight">\(
this value converges to \)</span>0.632$ as the dataset size grows, as seen in the following figure:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.3e2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">p_boot</span> <span class="o">=</span>  <span class="p">(</span><span class="mi">1</span><span class="o">-</span> <span class="mi">1</span><span class="o">/</span><span class="n">n</span><span class="p">)</span><span class="o">**</span><span class="n">n</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dpi</span> <span class="o">=</span> <span class="mi">120</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p_boot</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;C4&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$e^{-1}$&#39;</span><span class="p">,</span> <span class="n">xy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.37</span><span class="p">),</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;C4&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$(1- \frac</span><span class="si">{1}{N}</span><span class="s1">)^</span><span class="si">{N}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="n">xy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.30</span><span class="p">),</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;C0&#39;</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$N$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mf">0.15</span><span class="p">,</span><span class="mf">0.45</span><span class="p">])</span>
<span class="n">clean_ax</span><span class="p">(</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">p_boot</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;C4&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.7</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$e^{-1} - (1- \frac</span><span class="si">{1}{N}</span><span class="s1">)^</span><span class="si">{N}</span><span class="s1">$&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;N&#39;</span><span class="p">)</span>
<span class="n">clean_ax</span><span class="p">(</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/pablo-best-benchmark-model_55_0.png" src="../_images/pablo-best-benchmark-model_55_0.png" />
</div>
</div>
<p>The usual approach to error estimation by bootstrap [Hastie, Efron] proceeds by constructing bootstrap datasets from the original sample as training samples and keeping the whole original dataset as the test sample. This leads to having common observations in both the training and testing samples and results in a underestimation of the actual error as we mentioned before. To surpass this problem a “cross-validated” version of the bootstrap method is often used. In this setup, for each observation <span class="math notranslate nohighlight">\(i\)</span>, we calculate the boostrap error considering only the bootstrap samples where the <span class="math notranslate nohighlight">\(i\)</span>-th observation is not present. The <em>leave-one-out</em> bootstrap error estimate is given by:
$<span class="math notranslate nohighlight">\(\widehat{\text{Err}}^{(1)} = \dfrac{1}{N}\sum_{i = 1}^{N}\dfrac{1}{|C_{-i}|}\sum_{b \in C_{-i}}L(y_i, \hat{f}^{*b}(x_i)),\)</span><span class="math notranslate nohighlight">\(
where \)</span>C_{-i}<span class="math notranslate nohighlight">\( is the set of all bootstrap samples that do not contain the \)</span>i<span class="math notranslate nohighlight">\(-th observation. The average number of unique observations in a bootstrap sample is approximately \)</span>0.632*N$, which makes this estimator to behave almost like a 2-fold cross validation, this is, the <em>leave-one-out</em> bootstrap error estimate will actually overestimate the true error rate.</p>
<p>To solve this, the <strong>.632</strong> bootstrap estimator was proposed by Efron and Tibshirani <a class="reference external" href="https://www.tandfonline.com/doi/abs/10.1080/01621459.1997.10474007">“Improvements on cross-validation: the 632+ bootstrap method.” Journal of the American Statistical Association 92.438 (1997): 548-560.</a>, defined as:
$<span class="math notranslate nohighlight">\(\widehat{\text{Err}}^{(.632)} = 0.368 \times \overline{\text{err}} + 0.632 \times \widehat{\text{Err}}^{(1)}.\)</span>$
Note that this estimator actually does not deal with overfitting very well, since one of the terms is actually the training error estimate. We refer the reader to [TESL] for further details about the <strong>.632</strong> bootstrap estimator.</p>
<p>Instead, we will construct another bootstrap estimator and compare it with the usual k-fold cross validation strategies.</p>
<p><strong>Bootstrapped error estimation</strong>: We need to implement a splitter that bootstraps the training samples only. Specifically, we want to implement a k-fold validation such that, at each fold split the trainin sample is bootstraped a certain number of times and a model is trained on each bootstraped sample. This Kfold object could be called <code class="docutils literal notranslate"><span class="pre">StratifiedBootstrapKFold</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection._split</span> <span class="kn">import</span> <span class="n">_BaseKFold</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span><span class="p">,</span> <span class="n">indexable</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="kn">import</span> <span class="n">column_or_1d</span><span class="p">,</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">_num_samples</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.multiclass</span> <span class="kn">import</span> <span class="n">type_of_target</span>

<span class="k">class</span> <span class="nc">StratifiedBootstrapKFold</span><span class="p">(</span><span class="n">_BaseKFold</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Stratified Bootstrap Kfolds cross-validator.</span>

<span class="sd">    Provides train/test indices to split date in train/test sets.</span>

<span class="sd">    This cross-validation object is a variation of KFold that returns </span>
<span class="sd">    bootstrapped stratified folds. The folds are made by preserving </span>
<span class="sd">    the percentage of samples for each class.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_splits : int, default=5</span>
<span class="sd">        Number of folds. Must be at least 2.</span>
<span class="sd">    shuffle : bool, default=False</span>
<span class="sd">        Whether to shuffle each class&#39;s samples before splitting into batches.</span>
<span class="sd">        Note that the samples within each split will not be shuffled.</span>
<span class="sd">    random_state : int, RandomState instance or None, default=None</span>
<span class="sd">        When `shuffle` is True, `random_state` affects the ordering of the</span>
<span class="sd">        indices, which controls the randomness of each fold for each class.</span>
<span class="sd">        Otherwise, leave `random_state` as `None`.</span>
<span class="sd">        Pass an int for reproducible output across multiple function calls.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_make_test_folds</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">type_of_target_y</span> <span class="o">=</span> <span class="n">type_of_target</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">allowed_target_types</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;multiclass&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">type_of_target_y</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">allowed_target_types</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Supported target types are: </span><span class="si">{}</span><span class="s2">. Got </span><span class="si">{!r}</span><span class="s2"> instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">allowed_target_types</span><span class="p">,</span> <span class="n">type_of_target_y</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">y_idx</span><span class="p">,</span> <span class="n">y_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">return_index</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># y_inv encodes y according to lexicographic order. We invert y_idx to</span>
        <span class="c1"># map the classes so that they are encoded by order of appearance:</span>
        <span class="c1"># 0 represents the first label appearing in y, 1 the second, etc.</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">class_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_idx</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">y_encoded</span> <span class="o">=</span> <span class="n">class_perm</span><span class="p">[</span><span class="n">y_inv</span><span class="p">]</span>

        <span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_idx</span><span class="p">)</span>
        <span class="n">y_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_encoded</span><span class="p">)</span>
        <span class="n">min_groups</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">y_counts</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span> <span class="o">&gt;</span> <span class="n">y_counts</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;n_splits=</span><span class="si">%d</span><span class="s2"> cannot be greater than the&quot;</span>
                <span class="s2">&quot; number of members in each class.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span> <span class="o">&gt;</span> <span class="n">min_groups</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The least populated class in y has only </span><span class="si">%d</span><span class="s2">&quot;</span>
                <span class="s2">&quot; members, which is less than n_splits=</span><span class="si">%d</span><span class="s2">.&quot;</span>
                <span class="o">%</span> <span class="p">(</span><span class="n">min_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span><span class="p">),</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Determine the optimal number of samples from each class in each fold,</span>
        <span class="c1"># using round robin over the sorted y. (This can be done direct from</span>
        <span class="c1"># counts, but that code is unreadable.)</span>
        <span class="n">y_order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">y_encoded</span><span class="p">)</span>
        <span class="n">allocation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_order</span><span class="p">[</span><span class="n">i</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span><span class="p">],</span> <span class="n">minlength</span><span class="o">=</span><span class="n">n_classes</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># To maintain the data order dependencies as best as possible within</span>
        <span class="c1"># the stratification constraint, we assign samples from each class in</span>
        <span class="c1"># blocks (and then mess that up when shuffle=True).</span>
        <span class="n">test_folds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">):</span>
            <span class="c1"># since the kth column of allocation stores the number of samples</span>
            <span class="c1"># of class k in each test set, this generates blocks of fold</span>
            <span class="c1"># indices corresponding to the allocation for class k.</span>
            <span class="n">folds_for_class</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">allocation</span><span class="p">[:,</span> <span class="n">k</span><span class="p">])</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">:</span>
                <span class="n">rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">folds_for_class</span><span class="p">)</span>
            <span class="n">test_folds</span><span class="p">[</span><span class="n">y_encoded</span> <span class="o">==</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">folds_for_class</span>
        <span class="k">return</span> <span class="n">test_folds</span>

    <span class="k">def</span> <span class="nf">_iter_test_masks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">test_folds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_test_folds</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">test_folds</span> <span class="o">==</span> <span class="n">i</span>
    
    <span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Generate indices to split data into training and test set.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            Training data, where `n_samples` is the number of samples</span>
<span class="sd">            and `n_features` is the number of features.</span>
<span class="sd">            Note that providing ``y`` is sufficient to generate the splits and</span>
<span class="sd">            hence ``np.zeros(n_samples)`` may be used as a placeholder for</span>
<span class="sd">            ``X`` instead of actual training data.</span>
<span class="sd">        y : array-like of shape (n_samples,)</span>
<span class="sd">            The target variable for supervised learning problems.</span>
<span class="sd">            Stratification is done based on the y labels.</span>
<span class="sd">        groups : object</span>
<span class="sd">            Always ignored, exists for compatibility.</span>
<span class="sd">        Yields</span>
<span class="sd">        ------</span>
<span class="sd">        train : ndarray</span>
<span class="sd">            The training set indices for that split.</span>
<span class="sd">        test : ndarray</span>
<span class="sd">            The testing set indices for that split.</span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Randomized CV splitters may return different results for each call of</span>
<span class="sd">        split. You can make the results identical by setting `random_state`</span>
<span class="sd">        to an integer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">input_name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Bootstrap the training samples only</span>
        
        <span class="c1"># return super().split(X, y, groups)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection._split</span> <span class="kn">import</span> <span class="n">_BaseKFold</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">check_random_state</span><span class="p">,</span> <span class="n">indexable</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.validation</span> <span class="kn">import</span> <span class="n">column_or_1d</span><span class="p">,</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">_num_samples</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.multiclass</span> <span class="kn">import</span> <span class="n">type_of_target</span>

<span class="k">class</span> <span class="nc">StratifiedBootstrapKFold</span><span class="p">(</span><span class="n">_BaseKFold</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Stratified Bootstrap Kfolds cross-validator.</span>

<span class="sd">    Provides train/test indices to split date in train/test sets.</span>

<span class="sd">    This cross-validation object is a variation of KFold that returns </span>
<span class="sd">    bootstrapped stratified folds. The folds are made by preserving </span>
<span class="sd">    the percentage of samples for each class.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n_splits : int, default=5</span>
<span class="sd">        Number of folds. Must be at least 2.</span>
<span class="sd">    shuffle : bool, default=False</span>
<span class="sd">        Whether to shuffle each class&#39;s samples before splitting into batches.</span>
<span class="sd">        Note that the samples within each split will not be shuffled.</span>
<span class="sd">    random_state : int, RandomState instance or None, default=None</span>
<span class="sd">        When `shuffle` is True, `random_state` affects the ordering of the</span>
<span class="sd">        indices, which controls the randomness of each fold for each class.</span>
<span class="sd">        Otherwise, leave `random_state` as `None`.</span>
<span class="sd">        Pass an int for reproducible output across multiple function calls.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">n_splits</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_make_test_folds</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">type_of_target_y</span> <span class="o">=</span> <span class="n">type_of_target</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">allowed_target_types</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;multiclass&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">type_of_target_y</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">allowed_target_types</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Supported target types are: </span><span class="si">{}</span><span class="s2">. Got </span><span class="si">{!r}</span><span class="s2"> instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">allowed_target_types</span><span class="p">,</span> <span class="n">type_of_target_y</span>
                <span class="p">)</span>
            <span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">y_idx</span><span class="p">,</span> <span class="n">y_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">return_index</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># y_inv encodes y according to lexicographic order. We invert y_idx to</span>
        <span class="c1"># map the classes so that they are encoded by order of appearance:</span>
        <span class="c1"># 0 represents the first label appearing in y, 1 the second, etc.</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">class_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_idx</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">y_encoded</span> <span class="o">=</span> <span class="n">class_perm</span><span class="p">[</span><span class="n">y_inv</span><span class="p">]</span>

        <span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_idx</span><span class="p">)</span>
        <span class="n">y_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_encoded</span><span class="p">)</span>
        <span class="n">min_groups</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">y_counts</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span> <span class="o">&gt;</span> <span class="n">y_counts</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;n_splits=</span><span class="si">%d</span><span class="s2"> cannot be greater than the&quot;</span>
                <span class="s2">&quot; number of members in each class.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span> <span class="o">&gt;</span> <span class="n">min_groups</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="s2">&quot;The least populated class in y has only </span><span class="si">%d</span><span class="s2">&quot;</span>
                <span class="s2">&quot; members, which is less than n_splits=</span><span class="si">%d</span><span class="s2">.&quot;</span>
                <span class="o">%</span> <span class="p">(</span><span class="n">min_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span><span class="p">),</span>
                <span class="ne">UserWarning</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Determine the optimal number of samples from each class in each fold,</span>
        <span class="c1"># using round robin over the sorted y. (This can be done direct from</span>
        <span class="c1"># counts, but that code is unreadable.)</span>
        <span class="n">y_order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">y_encoded</span><span class="p">)</span>
        <span class="n">allocation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_order</span><span class="p">[</span><span class="n">i</span> <span class="p">::</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span><span class="p">],</span> <span class="n">minlength</span><span class="o">=</span><span class="n">n_classes</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># To maintain the data order dependencies as best as possible within</span>
        <span class="c1"># the stratification constraint, we assign samples from each class in</span>
        <span class="c1"># blocks (and then mess that up when shuffle=True).</span>
        <span class="n">test_folds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i&quot;</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">):</span>
            <span class="c1"># since the kth column of allocation stores the number of samples</span>
            <span class="c1"># of class k in each test set, this generates blocks of fold</span>
            <span class="c1"># indices corresponding to the allocation for class k.</span>
            <span class="n">folds_for_class</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">allocation</span><span class="p">[:,</span> <span class="n">k</span><span class="p">])</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">:</span>
                <span class="n">rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">folds_for_class</span><span class="p">)</span>
            <span class="n">test_folds</span><span class="p">[</span><span class="n">y_encoded</span> <span class="o">==</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">folds_for_class</span>
        <span class="k">return</span> <span class="n">test_folds</span>

    <span class="k">def</span> <span class="nf">_iter_test_masks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">test_folds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_test_folds</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">test_folds</span> <span class="o">==</span> <span class="n">i</span>
    
    <span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Generate indices to split data into training and test set.</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like of shape (n_samples, n_features)</span>
<span class="sd">            Training data, where `n_samples` is the number of samples</span>
<span class="sd">            and `n_features` is the number of features.</span>
<span class="sd">            Note that providing ``y`` is sufficient to generate the splits and</span>
<span class="sd">            hence ``np.zeros(n_samples)`` may be used as a placeholder for</span>
<span class="sd">            ``X`` instead of actual training data.</span>
<span class="sd">        y : array-like of shape (n_samples,)</span>
<span class="sd">            The target variable for supervised learning problems.</span>
<span class="sd">            Stratification is done based on the y labels.</span>
<span class="sd">        groups : object</span>
<span class="sd">            Always ignored, exists for compatibility.</span>
<span class="sd">        Yields</span>
<span class="sd">        ------</span>
<span class="sd">        train : ndarray</span>
<span class="sd">            The training set indices for that split.</span>
<span class="sd">        test : ndarray</span>
<span class="sd">            The testing set indices for that split.</span>
<span class="sd">        Notes</span>
<span class="sd">        -----</span>
<span class="sd">        Randomized CV splitters may return different results for each call of</span>
<span class="sd">        split. You can make the results identical by setting `random_state`</span>
<span class="sd">        to an integer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">input_name</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

        <span class="c1"># Bootstrap the training samples only</span>
        
        <span class="c1"># return super().split(X, y, groups)</span>
</pre></div>
</div>
</div>
</div>
<p>Some comments about trying to stratify a boostrapped sample of the population. There might be a few ways of doing this and not much has been discussed in the literature about this approach to error estimation. To understand why we need to proceed with care when building a bootstrapped sample let us consider the main problem we are tackling in this paper, binary classification. So, let <span class="math notranslate nohighlight">\(\mathcal{D}=\{(x_i,y_i)\}_{i=1}^{N}\)</span>, where <span class="math notranslate nohighlight">\(x_i\)</span> is a feature vector for the i-th observation (subject) and <span class="math notranslate nohighlight">\(y_i \in \{0,1\} \)</span> its binary target value.  Denote the positive class ratio as:
$<span class="math notranslate nohighlight">\(p =\dfrac{N_1}{N},\)</span><span class="math notranslate nohighlight">\( 
where \)</span>n_1<span class="math notranslate nohighlight">\( is the number of positive class observations in the dataset. Suppose we want to estimate the expected error of a classification model trained on the dataset \)</span>\mathcal{D}<span class="math notranslate nohighlight">\( via bootstrap. To do this, we start by constructing \)</span>B<span class="math notranslate nohighlight">\( bootstrap samples \)</span>{\mathcal{B}<em>b}</em>{b = 1}^{B}<span class="math notranslate nohighlight">\( of the original dataset. A classification model is trained for each of the \)</span>B$ bootstrap samples and the behavior (eg. performance) of the models is examined througout all bootstrap datasets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random_state</span> <span class="o">=</span><span class="mi">42</span>
<span class="n">n_splits</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">shuffle</span> <span class="o">=</span> <span class="kc">False</span>
<span class="c1">#### </span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
<span class="n">y_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">type_of_target_y</span> <span class="o">=</span> <span class="n">type_of_target</span><span class="p">(</span><span class="n">y_</span><span class="p">)</span>
<span class="n">allowed_target_types</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span> <span class="s2">&quot;multiclass&quot;</span><span class="p">)</span>
<span class="k">if</span> <span class="n">type_of_target_y</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">allowed_target_types</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;Supported target types are: </span><span class="si">{}</span><span class="s2">. Got </span><span class="si">{!r}</span><span class="s2"> instead.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
            <span class="n">allowed_target_types</span><span class="p">,</span> <span class="n">type_of_target_y</span>
        <span class="p">)</span>
    <span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">column_or_1d</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="n">_</span><span class="p">,</span> <span class="n">y_idx</span><span class="p">,</span> <span class="n">y_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span> <span class="n">return_index</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># y_inv encodes y according to lexicographic order. We invert y_idx to</span>
<span class="c1"># map the classes so that they are encoded by order of appearance:</span>
<span class="c1"># 0 represents the first label appearing in y, 1 the second, etc.</span>
<span class="n">_</span><span class="p">,</span> <span class="n">class_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_idx</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y_encoded</span> <span class="o">=</span> <span class="n">class_perm</span><span class="p">[</span><span class="n">y_inv</span><span class="p">]</span>

<span class="n">n_classes</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_idx</span><span class="p">)</span>
<span class="n">y_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_encoded</span><span class="p">)</span>
<span class="n">min_groups</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">y_counts</span><span class="p">)</span>
<span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">n_splits</span> <span class="o">&gt;</span> <span class="n">y_counts</span><span class="p">):</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
        <span class="s2">&quot;n_splits=</span><span class="si">%d</span><span class="s2"> cannot be greater than the&quot;</span>
        <span class="s2">&quot; number of members in each class.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span><span class="p">)</span>
    <span class="p">)</span>
<span class="k">if</span> <span class="n">n_splits</span> <span class="o">&gt;</span> <span class="n">min_groups</span><span class="p">:</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;The least populated class in y has only </span><span class="si">%d</span><span class="s2">&quot;</span>
        <span class="s2">&quot; members, which is less than n_splits=</span><span class="si">%d</span><span class="s2">.&quot;</span>
        <span class="o">%</span> <span class="p">(</span><span class="n">min_groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_splits</span><span class="p">),</span>
        <span class="ne">UserWarning</span><span class="p">,</span>
    <span class="p">)</span>
<span class="c1"># Determine the optimal number of samples from each class in each fold,</span>
<span class="c1"># using round robin over the sorted y. (This can be done direct from</span>
<span class="c1"># counts, but that code is unreadable.)</span>
<span class="n">y_order</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">y_encoded</span><span class="p">)</span>
<span class="n">allocation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
    <span class="p">[</span>
        <span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_order</span><span class="p">[</span><span class="n">i</span> <span class="p">::</span> <span class="n">n_splits</span><span class="p">],</span> <span class="n">minlength</span><span class="o">=</span><span class="n">n_classes</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">)</span>
    <span class="p">]</span>
<span class="p">)</span>
<span class="n">test_folds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;i&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_classes</span><span class="p">):</span>
    <span class="c1"># since the kth column of allocation stores the number of samples</span>
    <span class="c1"># of class k in each test set, this generates blocks of fold</span>
    <span class="c1"># indices corresponding to the allocation for class k.</span>
    <span class="n">folds_for_class</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n_splits</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">allocation</span><span class="p">[:,</span> <span class="n">k</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">shuffle</span><span class="p">:</span>
        <span class="n">rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">folds_for_class</span><span class="p">)</span>
    <span class="n">test_folds</span><span class="p">[</span><span class="n">y_encoded</span> <span class="o">==</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">folds_for_class</span>

<span class="k">def</span> <span class="nf">_iter_test_masks</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># test_folds = _make_test_folds(X, y)</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_splits</span><span class="p">):</span>
            <span class="k">yield</span> <span class="n">test_folds</span> <span class="o">==</span> <span class="n">i</span>

<span class="k">def</span> <span class="nf">_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span> <span class="o">=</span> <span class="n">indexable</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
    <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">_num_samples</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">_iter_test_masks</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">):</span>
        <span class="n">train_index</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">test_index</span><span class="p">)]</span>
        <span class="n">train_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">train_index</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_index</span><span class="p">),</span> <span class="n">replace</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">test_index</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
        <span class="k">yield</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Train:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;n unique indices: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">train_index</span><span class="p">))</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">y</span><span class="p">[</span><span class="n">train_index</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test:&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;n unique indices: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">test_index</span><span class="p">))</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Train:
n unique indices: 5074
0.304125
Test:
n unique indices: 2000
Train:
n unique indices: 5101
0.302
Test:
n unique indices: 2000
Train:
n unique indices: 5051
0.301875
Test:
n unique indices: 2000
Train:
n unique indices: 5067
0.302375
Test:
n unique indices: 2000
Train:
n unique indices: 5108
0.2935
Test:
n unique indices: 2000
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">size</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([ 6, 10,  6,  3,  3,  4,  6,  4,  3,  3])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span> <span class="o">=</span> <span class="n">indexable</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">_num_samples</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="k">for</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_iter_test_masks</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">):</span>
    <span class="n">train_index</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">logical_not</span><span class="p">(</span><span class="n">test_index</span><span class="p">)]</span>
    <span class="n">test_index</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
    <span class="k">yield</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Experian LatAm DataLab<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>